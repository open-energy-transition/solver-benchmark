{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and Download Benchmark Sessions\n",
    "\n",
    "This notebook helps you:\n",
    "1. **Query running VMs** - List active benchmark VMs and their status\n",
    "2. **Download results** - Fetch benchmark results, logs, and startup scripts from running or completed sessions\n",
    "3. **View progress** - Observe the progress of the benchmarking session\n",
    "\n",
    "## Workflow\n",
    "1. Run `tofu apply -var-file benchmarks/{timestamp}_batch/run.tfvars` from `infrastructure/` to start benchmarks\n",
    "\n",
    "```\n",
    "...\n",
    "google_compute_instance.benchmark_instances[\"standard-00\"]: Creating...\n",
    "google_compute_instance.benchmark_instances[\"standard-01\"]: Creating...\n",
    "google_compute_instance.benchmark_instances[\"standard-01\"]: Still creating... [10s elapsed]\n",
    "google_compute_instance.benchmark_instances[\"standard-00\"]: Still creating... [10s elapsed]\n",
    "google_compute_instance.benchmark_instances[\"standard-01\"]: Still creating... [20s elapsed]\n",
    "google_compute_instance.benchmark_instances[\"standard-00\"]: Still creating... [20s elapsed]\n",
    "google_compute_instance.benchmark_instances[\"standard-00\"]: Creation complete after 21s [id=projects/compute-app-427709/zones/europe-west4-a/instances/benchmark-instance-standard-00]\n",
    "google_compute_instance.benchmark_instances[\"standard-01\"]: Creation complete after 21s [id=projects/compute-app-427709/zones/europe-west4-a/instances/benchmark-instance-standard-01]\n",
    "\n",
    "Apply complete! Resources: 2 added, 0 changed, 0 destroyed.\n",
    "\n",
    "Outputs:\n",
    "\n",
    "instance_ips = {\n",
    "  \"standard-00\" = \"34.90.225.213\"\n",
    "  \"standard-01\" = \"34.90.240.138\"\n",
    "}\n",
    "run_id = \"20251106_153156_batch\"\n",
    "```\n",
    "\n",
    "2. Use section *1. Configure variables for the Benchmarking Campaign* to point the notebook at your session\n",
    "3. Use section *2. Observe the Running Benchmark Session* to download result CSVs and logs to observe the progress of the Benchmarking Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configure variables for the Benchmarking Campaign\n",
    "\n",
    "- Set up Benchmarking Session configuration. (Set up using `notebooks/allocate-benchmarks-to-vms.ipynb`) \n",
    "- Set up results output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarks Dir: /home/madhukar/oet/solver-benchmark/infrastructure/benchmarks/sample_run\n",
      "run.tfvars  standard-00.yaml  standard-01.yaml\n",
      "\n",
      "Results Dir: /home/madhukar/oet/solver-benchmark/results/sample_run\n",
      "\n",
      "Session config:\n",
      "============================================================\n",
      "\n",
      "project_id = \"compute-app-427709\"\n",
      "# This will be overriden if a value is specified in the input metadata file\n",
      "zone = \"europe-west4-a\"\n",
      "# Optional\n",
      "enable_gcs_upload = false\n",
      "auto_destroy_vm = false\n",
      "benchmarks_dir = \"benchmarks/sample_run\""
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "BASE_RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "INFRASTRUCTURE_DIR = PROJECT_ROOT / \"infrastructure\"\n",
    "BASE_BENCHMARKS_DIR = INFRASTRUCTURE_DIR / \"benchmarks\"\n",
    "SESSION_ID = \"sample_run\"\n",
    "RUN_ID = \"20251106_153156_batch\"\n",
    "\n",
    "SESSION_RESULTS_DIR = BASE_RESULTS_DIR / SESSION_ID\n",
    "SESSION_BENCHMARKS_DIR = BASE_BENCHMARKS_DIR / SESSION_ID\n",
    "VM_SCRIPT = PROJECT_ROOT / \"vms_gcloud.py\"\n",
    "VM_NAME_PREFIX = \"benchmark-instance\"\n",
    "\n",
    "print(f\"Benchmarks Dir: {SESSION_BENCHMARKS_DIR}\")\n",
    "!ls {SESSION_BENCHMARKS_DIR}\n",
    "\n",
    "print(f\"\\nResults Dir: {SESSION_RESULTS_DIR}\")\n",
    "\n",
    "print(f\"\\nSession config:\\n{'=' * 60}\\n\")\n",
    "!cat $SESSION_BENCHMARKS_DIR/run.tfvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Observe the Running Benchmark Session\n",
    "\n",
    "All artifacts are plain text flat files that are accessed over ssh. The cells below download results and check them against the allocation configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Query Running VMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using current project: compute-app-427709\n",
      "Discovering zones across 1 projects...\n",
      "Querying VMs across 127 zones...\n",
      "Progress: 50/127 zones checked\n",
      "Progress: 100/127 zones checked\n",
      "\n",
      "Found 2 matching VMs\n",
      "\n",
      "Project                        Zone                 Name                           Status       Machine Type         Internal IP     External IP    \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "compute-app-427709             europe-west4-a       benchmark-instance-standard-00 RUNNING      c4-standard-2        10.164.0.42     34.90.225.213  \n",
      "compute-app-427709             europe-west4-a       benchmark-instance-standard-01 RUNNING      c4-standard-2        10.164.0.41     34.90.240.138  \n"
     ]
    }
   ],
   "source": [
    "# Query standard VMs\n",
    "!python $VM_SCRIPT $VM_NAME_PREFIX --output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using current project: compute-app-427709\n",
      "Discovering zones across 1 projects...\n",
      "Querying VMs across 127 zones...\n",
      "Progress: 50/127 zones checked\n",
      "Progress: 100/127 zones checked\n",
      "\n",
      "Found 2 matching VMs\n",
      "Executing command on 2 VMs: uptime\n",
      "\n",
      "============================================================\n",
      "✓ benchmark-instance-standard-00: Success\n",
      "STDOUT:\n",
      " 17:33:12 up  2:00,  1 user,  load average: 0.00, 0.00, 0.00\n",
      "\n",
      "\n",
      "============================================================\n",
      "✓ benchmark-instance-standard-01: Success\n",
      "STDOUT:\n",
      " 17:33:12 up  2:00,  2 users,  load average: 0.00, 0.00, 0.00\n",
      "\n",
      "\n",
      "============================================================\n",
      "Completed: 2/2 successful\n"
     ]
    }
   ],
   "source": [
    "# Run commands over ssh for running VMs\n",
    "!python $VM_SCRIPT $VM_NAME_PREFIX --ssh \"uptime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Download Results for the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using current project: compute-app-427709\n",
      "Discovering zones across 1 projects...\n",
      "Querying VMs across 127 zones...\n",
      "Progress: 50/127 zones checked\n",
      "Progress: 100/127 zones checked\n",
      "\n",
      "Found 2 matching VMs\n",
      "Copying from 2 VMs...\n",
      "✓ benchmark-instance-standard-00: Success\n",
      "✓ benchmark-instance-standard-01: Success\n",
      "\n",
      "Completed: 2/2 successful\n"
     ]
    }
   ],
   "source": [
    "# Download benchmark_results.csv\n",
    "!python $VM_SCRIPT $VM_NAME_PREFIX '--scp-source' 'vm:/solver-benchmark/results/benchmark_results.csv' '--scp-dest' $SESSION_RESULTS_DIR/scp/{{vm_name}}/benchmark_results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using current project: compute-app-427709\n",
      "Discovering zones across 1 projects...\n",
      "Querying VMs across 127 zones...\n",
      "Progress: 50/127 zones checked\n",
      "Progress: 100/127 zones checked\n",
      "\n",
      "Found 2 matching VMs\n",
      "Copying from 2 VMs...\n",
      "✓ benchmark-instance-standard-00: Success\n",
      "✓ benchmark-instance-standard-01: Success\n",
      "\n",
      "Completed: 2/2 successful\n"
     ]
    }
   ],
   "source": [
    "# Download startup-script.log\n",
    "!python $VM_SCRIPT $VM_NAME_PREFIX '--scp-source' 'vm:/var/log/startup-script.log' '--scp-dest' $SESSION_RESULTS_DIR/scp/{{vm_name}}/startup-script.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using current project: compute-app-427709\n",
      "Discovering zones across 1 projects...\n",
      "Querying VMs across 127 zones...\n",
      "Progress: 50/127 zones checked\n",
      "Progress: 100/127 zones checked\n",
      "\n",
      "Found 2 matching VMs\n",
      "Copying from 2 VMs...\n"
     ]
    }
   ],
   "source": [
    "# Download runner logs (recursive)\n",
    "!python $VM_SCRIPT $VM_NAME_PREFIX '--scp-source' 'vm:/solver-benchmark/runner/logs/' '--scp-dest' $SESSION_RESULTS_DIR/scp/{{vm_name}}/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download completed results from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GCS results found\n",
      "No GCS logs found\n"
     ]
    }
   ],
   "source": [
    "# Download results from GCS\n",
    "!gsutil -m cp -r gs://solver-benchmarks/results/$RUN_ID'*' $SESSION_RESULTS_DIR/gcs/results 2>/dev/null || echo \"No GCS results found\"\n",
    "!gsutil -m cp -r gs://solver-benchmarks/logs/$RUN_ID'*' $SESSION_RESULTS_DIR/gcs/logs 2>/dev/null || echo \"No GCS logs found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/madhukar/oet/solver-benchmark/results/sample_run/\u001b[0m\n",
      "└── \u001b[01;34mscp\u001b[0m\n",
      "    ├── \u001b[01;34mbenchmark-instance-standard-00\u001b[0m\n",
      "    │   ├── \u001b[00mbenchmark_results.csv\u001b[0m\n",
      "    │   ├── \u001b[01;34mlogs\u001b[0m\n",
      "    │   │   ├── \u001b[00mgenx-3_three_zones_w_co2_capture-no_uc-3-1h-highs-1.10.0.log\u001b[0m\n",
      "    │   │   ├── \u001b[00mgenx-3_three_zones_w_co2_capture-no_uc-3-1h-scip-9.2.2.log\u001b[0m\n",
      "    │   │   ├── \u001b[00mSienna_modified_RTS_GMLC_DA_sys_NetTransport_Horizon24_Day314-1-1h-highs-1.10.0.log\u001b[0m\n",
      "    │   │   └── \u001b[00mSienna_modified_RTS_GMLC_DA_sys_NetTransport_Horizon24_Day314-1-1h-scip-9.2.2.log\u001b[0m\n",
      "    │   └── \u001b[00mstartup-script.log\u001b[0m\n",
      "    └── \u001b[01;34mbenchmark-instance-standard-01\u001b[0m\n",
      "        ├── \u001b[00mbenchmark_results.csv\u001b[0m\n",
      "        ├── \u001b[01;34mlogs\u001b[0m\n",
      "        │   ├── \u001b[00mgenx-2_three_zones_w_electrolyzer-3-1h-highs-1.10.0.log\u001b[0m\n",
      "        │   ├── \u001b[00mgenx-2_three_zones_w_electrolyzer-3-1h-scip-9.2.2.log\u001b[0m\n",
      "        │   ├── \u001b[00mpypsa-power+ely+battery-ucgas-1-1h-highs-1.10.0.log\u001b[0m\n",
      "        │   └── \u001b[00mpypsa-power+ely+battery-ucgas-1-1h-scip-9.2.2.log\u001b[0m\n",
      "        └── \u001b[00mstartup-script.log\u001b[0m\n",
      "\n",
      "6 directories, 12 files\n"
     ]
    }
   ],
   "source": [
    "# List downloaded results\n",
    "!tree $SESSION_RESULTS_DIR/ 2>/dev/null || find $SESSION_RESULTS_DIR -type f | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Verify the Results against VM Allocations\n",
    "\n",
    "Check the results vs the allocation configs in the `infrastructure/benchmarks/<session-id>`\n",
    "to see how the benchmarks are being allocated and how the solvers are performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GCS results found\n",
      "  Loaded 5 rows from benchmark-instance-standard-00\n",
      "  Loaded 5 rows from benchmark-instance-standard-01\n",
      "Found 2 SCP CSV files\n",
      "\n",
      "Successfully combined 10 total rows (after deduplication)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_and_combine_results(session_dir):\n",
    "    \"\"\"\n",
    "    Load benchmark results from GCS and SCP, preferring GCS over SCP.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined results from all sources\n",
    "    \"\"\"\n",
    "    session_dir = Path(session_dir)\n",
    "    all_dfs = []\n",
    "\n",
    "    # Load GCS results first (preferred)\n",
    "    gcs_dir = session_dir / \"gcs\"\n",
    "    gcs_count = 0\n",
    "    if gcs_dir.exists():\n",
    "        for csv_file in sorted(gcs_dir.glob(\"**/benchmark_results.csv\")):\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                df[\"_source\"] = \"gcs\"\n",
    "                all_dfs.append(df)\n",
    "                gcs_count += 1\n",
    "                print(f\"  Loaded {len(df)} rows from {csv_file.parent.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {csv_file}: {e}\")\n",
    "\n",
    "    print(f\"Found {gcs_count} GCS CSV files\" if gcs_count else \"No GCS results found\")\n",
    "\n",
    "    # Load SCP results\n",
    "    scp_dir = session_dir / \"scp\"\n",
    "    scp_count = 0\n",
    "    if scp_dir.exists():\n",
    "        for csv_file in sorted(scp_dir.glob(\"**/benchmark_results.csv\")):\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                df[\"_source\"] = \"scp\"\n",
    "                all_dfs.append(df)\n",
    "                scp_count += 1\n",
    "                print(f\"  Loaded {len(df)} rows from {csv_file.parent.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {csv_file}: {e}\")\n",
    "\n",
    "    print(f\"Found {scp_count} SCP CSV files\" if scp_count else \"No SCP results found\")\n",
    "\n",
    "    # Combine all dataframes and deduplicate, preferring GCS\n",
    "    if all_dfs:\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "        # Deduplicate: keep GCS over SCP for same benchmark runs\n",
    "        # Sort so GCS comes first, then drop duplicates keeping first occurrence\n",
    "        combined_df = combined_df.sort_values(\n",
    "            \"_source\", key=lambda x: (x != \"gcs\")\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # Identify duplicates by benchmark data (all columns except _source)\n",
    "        cols_to_check = [c for c in combined_df.columns if c != \"_source\"]\n",
    "        combined_df = combined_df.drop_duplicates(subset=cols_to_check, keep=\"first\")\n",
    "\n",
    "        print(\n",
    "            f\"\\nSuccessfully combined {len(combined_df)} total rows (after deduplication)\"\n",
    "        )\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"\\nNo CSV files found\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load results\n",
    "results = load_and_combine_results(SESSION_RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timeout by machine type:\n",
      "  c4-standard-2: 3600.0s\n"
     ]
    }
   ],
   "source": [
    "def get_timeout_by_machine(runs: pd.DataFrame) -> dict:\n",
    "    \"\"\"Infer timeout values by machine type from actual results.\"\"\"\n",
    "    timeout_map = {}\n",
    "\n",
    "    # Find VM hostname column\n",
    "    vm_col = None\n",
    "    vm_candidates = {\n",
    "        \"hostname\",\n",
    "        \"host\",\n",
    "        \"vmhostname\",\n",
    "        \"vm\",\n",
    "        \"instancename\",\n",
    "        \"instance\",\n",
    "        \"_vm\",\n",
    "    }\n",
    "    for c in runs.columns:\n",
    "        normalized = c.lower().replace(\" \", \"\").replace(\"_\", \"\").replace(\"-\", \"\")\n",
    "        if normalized in vm_candidates:\n",
    "            vm_col = c\n",
    "            break\n",
    "\n",
    "    if vm_col and \"Timeout\" in runs.columns:\n",
    "        # Map hostname patterns to machine types\n",
    "        for _, row in runs.iterrows():\n",
    "            hostname = str(row.get(vm_col, \"\"))\n",
    "            timeout = row.get(\"Timeout\")\n",
    "\n",
    "            if pd.notna(timeout):\n",
    "                if \"highmem\" in hostname.lower():\n",
    "                    timeout_map[\"c4-highmem-8\"] = timeout\n",
    "                elif \"standard\" in hostname.lower():\n",
    "                    timeout_map[\"c4-standard-2\"] = timeout\n",
    "\n",
    "    return timeout_map\n",
    "\n",
    "\n",
    "if results is not None:\n",
    "    timeout_by_machine = get_timeout_by_machine(results)\n",
    "    print(\"\\nTimeout by machine type:\")\n",
    "    for machine_type, timeout in timeout_by_machine.items():\n",
    "        print(f\"  {machine_type}: {timeout}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "PER-VM EXPECTED VS OBSERVED SUMMARY\n",
      "====================================================================================================\n",
      "Looking for YAML files in: /home/madhukar/oet/solver-benchmark/infrastructure/benchmarks/sample_run\n",
      "Found 2 YAML files\n",
      "\n",
      "Hostnames in results: ['benchmark-instance-standard-00', 'benchmark-instance-standard-01']\n",
      "\n",
      "standard-00 (c4-standard-2): 4 expected runs, 1.0h, solvers=[highs scip]\n",
      "standard-01 (c4-standard-2): 4 expected runs, 1.0h, solvers=[highs scip]\n",
      "\n",
      "Matching 'standard-00' to hostnames: ['benchmark-instance-standard-00']\n",
      "Matching 'standard-01' to hostnames: ['benchmark-instance-standard-01']\n",
      "\n",
      "\n",
      "         VM       Machine    Solvers  Expected Runs  Expected Runtime (h)  Completed Runs  Failed Runs  Completed Runtime (h)  Remaining Runtime (h)  Expected Timeout  Expected Complete\n",
      "standard-00 c4-standard-2 highs scip              4                  1.00               4            0                   0.92                   0.08                 0                  4\n",
      "standard-01 c4-standard-2 highs scip              4                  0.99               4            0                   0.95                   0.04                 0                  4\n",
      "\n",
      "====================================================================================================\n",
      "SUMMARY\n",
      "====================================================================================================\n",
      "Total runs expected: 8\n",
      "Total expected runtime: 2.0h\n",
      "Total completed runs: 8\n",
      "Total completed runtime: 1.9h\n",
      "Total remaining runtime: 0.1h\n",
      "\n",
      "Expected to timeout: 0\n",
      "Expected to complete: 8\n",
      "\n",
      "====================================================================================================\n",
      "SOLVER ALLOCATION DETAIL\n",
      "====================================================================================================\n",
      "\n",
      "standard-00: highs, scip\n",
      "  Years: [2025]\n",
      "  Expected runs: 4\n",
      "    - Sienna_modified_RTS_GMLC_DA_sys_NetTransport_Horizon24_Day314-1-1h: 3422.8s (0.95h) with solvers: highs, scip\n",
      "    - genx-3_three_zones_w_co2_capture-no_uc-3-1h: 168.8s (0.05h) with solvers: highs, scip\n",
      "\n",
      "standard-01: highs, scip\n",
      "  Years: [2025]\n",
      "  Expected runs: 4\n",
      "    - genx-2_three_zones_w_electrolyzer-3-1h: 3291.7s (0.91h) with solvers: highs, scip\n",
      "    - pypsa-power+ely+battery-ucgas-1-1h: 282.0s (0.08h) with solvers: highs, scip\n",
      "\n",
      "====================================================================================================\n",
      "BENCHMARK COMPLETION DETAIL (Expected vs Actual)\n",
      "====================================================================================================\n",
      "\n",
      "standard-00:\n",
      "  Sienna_modified_RTS_GMLC_DA_sys_NetTransport_Horizon24_Day314-1-1h:\n",
      "    highs: 1102.7s ✓ (expected 1198.6s)\n",
      "    scip: 2045.5s ✓ (expected 2224.1s)\n",
      "  genx-3_three_zones_w_co2_capture-no_uc-3-1h:\n",
      "    highs: 71.6s ✓ (expected 75.7s)\n",
      "    scip: 88.2s ✓ (expected 93.1s)\n",
      "\n",
      "standard-01:\n",
      "  genx-2_three_zones_w_electrolyzer-3-1h:\n",
      "    highs: 2020.6s ✓ (expected 2132.8s)\n",
      "    scip: 1085.0s ✓ (expected 1158.9s)\n",
      "  pypsa-power+ely+battery-ucgas-1-1h:\n",
      "    highs: 126.2s ✓ (expected 106.8s)\n",
      "    scip: 206.1s ✓ (expected 175.2s)\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "if results is not None:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"PER-VM EXPECTED VS OBSERVED SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    # Load allocation from YAML files\n",
    "    expected_by_vm = {}\n",
    "\n",
    "    print(f\"Looking for YAML files in: {SESSION_BENCHMARKS_DIR}\")\n",
    "    yaml_files = sorted(SESSION_BENCHMARKS_DIR.glob(\"*.yaml\"))\n",
    "    print(f\"Found {len(yaml_files)} YAML files\\n\")\n",
    "\n",
    "    # Debug: show what hostnames are in results\n",
    "    print(f\"Hostnames in results: {results['Hostname'].unique().tolist()}\\n\")\n",
    "\n",
    "    for yaml_file in yaml_files:\n",
    "        if yaml_file.name == \"run.tfvars\":\n",
    "            continue\n",
    "\n",
    "        vm_name = yaml_file.stem  # e.g., \"standard-01\"\n",
    "\n",
    "        try:\n",
    "            with open(yaml_file) as f:\n",
    "                config = yaml.safe_load(f)\n",
    "\n",
    "            if not config:\n",
    "                continue\n",
    "\n",
    "            # Extract top-level fields\n",
    "            solvers_list = (\n",
    "                config.get(\"solver\", \"\").split() if config.get(\"solver\") else []\n",
    "            )\n",
    "            machine_type = config.get(\"machine-type\", \"unknown\")\n",
    "            years = config.get(\"years\", [])\n",
    "\n",
    "            expected_by_vm[vm_name] = {\n",
    "                \"expected_runs\": [],  # List of (benchmark, size, solver, runtime_s)\n",
    "                \"solvers\": solvers_list,\n",
    "                \"machine_type\": machine_type,\n",
    "                \"years\": years,\n",
    "            }\n",
    "\n",
    "            # Iterate through each benchmark\n",
    "            if \"benchmarks\" in config:\n",
    "                for bench_name, bench_data in config[\"benchmarks\"].items():\n",
    "                    if \"Sizes\" in bench_data:\n",
    "                        for size in bench_data[\"Sizes\"]:\n",
    "                            size_name = size.get(\"Name\", \"unknown\")\n",
    "                            size_solvers = size.get(\"_solvers\", [])\n",
    "                            solver_runtimes = size.get(\"_solver_runtimes_s\", {})\n",
    "\n",
    "                            # Create a (benchmark, size, solver) run for each solver\n",
    "                            for solver in size_solvers:\n",
    "                                solver_runtime_s = float(solver_runtimes.get(solver, 0))\n",
    "                                expected_by_vm[vm_name][\"expected_runs\"].append(\n",
    "                                    {\n",
    "                                        \"benchmark\": bench_name,\n",
    "                                        \"size\": size_name,\n",
    "                                        \"solver\": solver,\n",
    "                                        \"runtime_s\": solver_runtime_s,\n",
    "                                    }\n",
    "                                )\n",
    "\n",
    "            total_runtime_s = sum(\n",
    "                r[\"runtime_s\"] for r in expected_by_vm[vm_name][\"expected_runs\"]\n",
    "            )\n",
    "            total_h = total_runtime_s / 3600\n",
    "            solvers_str = \" \".join(solvers_list) if solvers_list else \"default\"\n",
    "            print(\n",
    "                f\"{vm_name} ({machine_type}): {len(expected_by_vm[vm_name]['expected_runs'])} expected runs, {total_h:.1f}h, solvers=[{solvers_str}]\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {yaml_file}: {e}\")\n",
    "\n",
    "    print()  # Blank line for readability\n",
    "\n",
    "    if not expected_by_vm:\n",
    "        print(\"\\nNo expected allocation found. Skipping per-VM comparison.\")\n",
    "    else:\n",
    "        # Build summary table\n",
    "        vm_summary = []\n",
    "\n",
    "        for vm_name in sorted(expected_by_vm.keys()):\n",
    "            expected = expected_by_vm[vm_name]\n",
    "\n",
    "            # Total expected runtime across all runs\n",
    "            total_expected_runtime_s = sum(\n",
    "                r[\"runtime_s\"] for r in expected[\"expected_runs\"]\n",
    "            )\n",
    "            expected_runtime_h = total_expected_runtime_s / 3600\n",
    "\n",
    "            # Get actual results for this VM - match by substring in Hostname\n",
    "            # e.g., vm_name=\"standard-00\" matches Hostname=\"benchmark-instance-standard-00\"\n",
    "            vm_results = results[\n",
    "                results[\"Hostname\"].str.contains(vm_name, case=False, na=False)\n",
    "            ]\n",
    "\n",
    "            # Filter to only benchmarks that are in the expected allocation (not reference benchmarks)\n",
    "            expected_benchmark_names = set(\n",
    "                r[\"benchmark\"] for r in expected[\"expected_runs\"]\n",
    "            )\n",
    "            vm_results_filtered = vm_results[\n",
    "                vm_results[\"Benchmark\"].isin(expected_benchmark_names)\n",
    "            ]\n",
    "\n",
    "            print(\n",
    "                f\"Matching '{vm_name}' to hostnames: {vm_results_filtered['Hostname'].unique().tolist() if len(vm_results_filtered) > 0 else 'no match'}\"\n",
    "            )\n",
    "\n",
    "            # Count completed runs (by benchmark-size-solver combination)\n",
    "            completed_runs = 0\n",
    "            completed_runtime_s = 0\n",
    "            completed_failed = 0\n",
    "\n",
    "            for run in expected[\"expected_runs\"]:\n",
    "                # Check if this specific run has a result\n",
    "                result = vm_results_filtered[\n",
    "                    (vm_results_filtered[\"Benchmark\"] == run[\"benchmark\"])\n",
    "                    & (vm_results_filtered[\"Size\"] == run[\"size\"])\n",
    "                    & (vm_results_filtered[\"Solver\"] == run[\"solver\"])\n",
    "                ]\n",
    "\n",
    "                if len(result) > 0:\n",
    "                    completed_runs += 1\n",
    "                    completed_runtime_s += result.iloc[0][\"Runtime (s)\"]\n",
    "                    # Check status (handle both 'ok' and 'OK')\n",
    "                    if result.iloc[0][\"Status\"].lower() != \"ok\":\n",
    "                        completed_failed += 1\n",
    "\n",
    "            completed_runtime_h = completed_runtime_s / 3600\n",
    "            completed_ok = completed_runs - completed_failed\n",
    "\n",
    "            # Calculate remaining (expected runs that haven't completed)\n",
    "            remaining_runtime_s = total_expected_runtime_s - completed_runtime_s\n",
    "            remaining_runtime_h = remaining_runtime_s / 3600\n",
    "\n",
    "            # Estimate timeouts\n",
    "            timeout_s = (\n",
    "                timeout_by_machine.get(\"c4-standard-2\", 3600)\n",
    "                if \"standard\" in vm_name\n",
    "                else timeout_by_machine.get(\"c4-highmem-8\", 3600)\n",
    "            )\n",
    "            timeout_h = timeout_s / 3600\n",
    "\n",
    "            expected_timeout = sum(\n",
    "                1\n",
    "                for r in expected[\"expected_runs\"]\n",
    "                if r[\"runtime_s\"] / 3600 >= timeout_h\n",
    "            )\n",
    "            expected_complete = len(expected[\"expected_runs\"]) - expected_timeout\n",
    "\n",
    "            vm_summary.append(\n",
    "                {\n",
    "                    \"VM\": vm_name,\n",
    "                    \"Machine\": expected[\"machine_type\"],\n",
    "                    \"Solvers\": \" \".join(expected[\"solvers\"])\n",
    "                    if expected[\"solvers\"]\n",
    "                    else \"default\",\n",
    "                    \"Expected Runs\": len(expected[\"expected_runs\"]),\n",
    "                    \"Expected Runtime (h)\": expected_runtime_h,\n",
    "                    \"Completed Runs\": completed_ok,\n",
    "                    \"Failed Runs\": completed_failed,\n",
    "                    \"Completed Runtime (h)\": completed_runtime_h,\n",
    "                    \"Remaining Runtime (h)\": max(0, remaining_runtime_h),\n",
    "                    \"Expected Timeout\": expected_timeout,\n",
    "                    \"Expected Complete\": expected_complete,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        vm_df = pd.DataFrame(vm_summary)\n",
    "\n",
    "        # Pretty print\n",
    "        print(\"\\n\")\n",
    "        with pd.option_context(\"display.max_columns\", None, \"display.width\", None):\n",
    "            with pd.option_context(\"display.float_format\", \"{:,.2f}\".format):\n",
    "                print(vm_df.to_string(index=False))\n",
    "\n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Total runs expected: {vm_df['Expected Runs'].sum()}\")\n",
    "        print(f\"Total expected runtime: {vm_df['Expected Runtime (h)'].sum():.1f}h\")\n",
    "        print(f\"Total completed runs: {vm_df['Completed Runs'].sum()}\")\n",
    "        print(f\"Total completed runtime: {vm_df['Completed Runtime (h)'].sum():.1f}h\")\n",
    "        print(f\"Total remaining runtime: {vm_df['Remaining Runtime (h)'].sum():.1f}h\")\n",
    "        print(f\"\\nExpected to timeout: {vm_df['Expected Timeout'].sum()}\")\n",
    "        print(f\"Expected to complete: {vm_df['Expected Complete'].sum()}\")\n",
    "\n",
    "        # Detailed solver breakdown\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"SOLVER ALLOCATION DETAIL\")\n",
    "        print(\"=\" * 100)\n",
    "        for vm_name in sorted(expected_by_vm.keys()):\n",
    "            expected = expected_by_vm[vm_name]\n",
    "            if expected[\"solvers\"]:\n",
    "                print(f\"\\n{vm_name}: {', '.join(expected['solvers'])}\")\n",
    "                print(f\"  Years: {expected['years']}\")\n",
    "                print(f\"  Expected runs: {len(expected['expected_runs'])}\")\n",
    "\n",
    "                # Group by benchmark-size to show summary\n",
    "                benchmarks_by_name = {}\n",
    "                for run in expected[\"expected_runs\"]:\n",
    "                    key = f\"{run['benchmark']}-{run['size']}\"\n",
    "                    if key not in benchmarks_by_name:\n",
    "                        benchmarks_by_name[key] = {\"solvers\": [], \"total_runtime_s\": 0}\n",
    "                    benchmarks_by_name[key][\"solvers\"].append(run[\"solver\"])\n",
    "                    benchmarks_by_name[key][\"total_runtime_s\"] += run[\"runtime_s\"]\n",
    "\n",
    "                for bench_name in sorted(benchmarks_by_name.keys())[:3]:\n",
    "                    b = benchmarks_by_name[bench_name]\n",
    "                    print(\n",
    "                        f\"    - {bench_name}: {b['total_runtime_s']:.1f}s ({b['total_runtime_s'] / 3600:.2f}h) with solvers: {', '.join(b['solvers'])}\"\n",
    "                    )\n",
    "\n",
    "                if len(benchmarks_by_name) > 3:\n",
    "                    print(f\"    ... and {len(benchmarks_by_name) - 3} more\")\n",
    "\n",
    "        # Detailed benchmark-by-benchmark-by-solver comparison\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"BENCHMARK COMPLETION DETAIL (Expected vs Actual)\")\n",
    "        print(\"=\" * 100)\n",
    "        for vm_name in sorted(expected_by_vm.keys()):\n",
    "            expected = expected_by_vm[vm_name]\n",
    "            vm_results = results[\n",
    "                results[\"Hostname\"].str.contains(vm_name, case=False, na=False)\n",
    "            ]\n",
    "\n",
    "            # Only show allocated benchmarks, exclude reference benchmarks\n",
    "            expected_benchmark_names = set(\n",
    "                r[\"benchmark\"] for r in expected[\"expected_runs\"]\n",
    "            )\n",
    "            vm_results_filtered = vm_results[\n",
    "                vm_results[\"Benchmark\"].isin(expected_benchmark_names)\n",
    "            ]\n",
    "\n",
    "            print(f\"\\n{vm_name}:\")\n",
    "\n",
    "            # Group expected runs by benchmark-size for cleaner output\n",
    "            benchmarks_by_key = {}\n",
    "            for run in expected[\"expected_runs\"]:\n",
    "                key = f\"{run['benchmark']}-{run['size']}\"\n",
    "                if key not in benchmarks_by_key:\n",
    "                    benchmarks_by_key[key] = []\n",
    "                benchmarks_by_key[key].append(run)\n",
    "\n",
    "            for bench_key in sorted(benchmarks_by_key.keys()):\n",
    "                runs = benchmarks_by_key[bench_key]\n",
    "                print(f\"  {bench_key}:\")\n",
    "\n",
    "                for run in runs:\n",
    "                    # Check if this run has a result\n",
    "                    result = vm_results_filtered[\n",
    "                        (vm_results_filtered[\"Benchmark\"] == run[\"benchmark\"])\n",
    "                        & (vm_results_filtered[\"Size\"] == run[\"size\"])\n",
    "                        & (vm_results_filtered[\"Solver\"] == run[\"solver\"])\n",
    "                    ]\n",
    "\n",
    "                    if len(result) > 0:\n",
    "                        status = result.iloc[0][\"Status\"]\n",
    "                        runtime = result.iloc[0][\"Runtime (s)\"]\n",
    "                        status_symbol = (\n",
    "                            \"✓\" if status.lower() == \"ok\" else f\"✗({status})\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"    {run['solver']}: {runtime:.1f}s {status_symbol} (expected {run['runtime_s']:.1f}s)\"\n",
    "                        )\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"    {run['solver']}: NOT STARTED (expected {run['runtime_s']:.1f}s)\"\n",
    "                        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
