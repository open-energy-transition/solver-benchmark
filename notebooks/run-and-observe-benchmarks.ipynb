{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and Download Benchmark Sessions\n",
    "\n",
    "This notebook helps you:\n",
    "1. **Query running VMs** - List active benchmark VMs and their status\n",
    "2. **Download results** - Fetch benchmark results, logs, and startup scripts from running or completed sessions\n",
    "3. **Download from GCS** - Retrieve completed sessions from Google Cloud Storage\n",
    "4. **Combine results** - Merge results from GCS and SCP sources\n",
    "\n",
    "## Workflow\n",
    "1. Run `tofu apply -var-file benchmarks/{timestamp}_batch/run.tfvars` from `infrastructure/` to start benchmarks\n",
    "2. Use **Cell 2** to query VM status while running\n",
    "3. Use **Cell 3** to download results from running VMs\n",
    "4. Use **Cell 4** to download from GCS\n",
    "5. Use **Cell 5+** to combine and analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarks Dir: /home/madhukar/oet/solver-benchmark/infrastructure/benchmarks/sample_run\n",
      "run.tfvars  standard-01.yaml  standard-02.yaml\n",
      "\n",
      "Results Dir: /home/madhukar/oet/solver-benchmark/results/sample_run\n",
      "\n",
      "Session config:\n",
      "============================================================\n",
      "\n",
      "project_id = \"compute-app-427709\"\n",
      "# This will be overriden if a value is specified in the input metadata file\n",
      "zone = \"europe-west4-a\"\n",
      "# Optional\n",
      "enable_gcs_upload = true\n",
      "auto_destroy_vm = false\n",
      "benchmarks_dir = \"benchmarks/sample_run\""
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "BASE_RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "INFRASTRUCTURE_DIR = PROJECT_ROOT / \"infrastructure\"\n",
    "BASE_BENCHMARKS_DIR = INFRASTRUCTURE_DIR / \"benchmarks\"\n",
    "SESSION_ID = \"sample_run\"\n",
    "\n",
    "SESSION_RESULTS_DIR = BASE_RESULTS_DIR / SESSION_ID\n",
    "SESSION_BENCHMARKS_DIR = BASE_BENCHMARKS_DIR / SESSION_ID\n",
    "VM_SCRIPT = PROJECT_ROOT / \"vms_gcloud.py\"\n",
    "VM_NAME_PREFIX = \"benchmark-instance\"\n",
    "\n",
    "print(f\"Benchmarks Dir: {SESSION_BENCHMARKS_DIR}\")\n",
    "!ls {SESSION_BENCHMARKS_DIR}\n",
    "\n",
    "print(f\"\\nResults Dir: {SESSION_RESULTS_DIR}\")\n",
    "\n",
    "print(f\"\\nSession config:\\n{'='*60}\\n\")\n",
    "!cat $SESSION_BENCHMARKS_DIR/run.tfvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Running VMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using current project: compute-app-427709\n",
      "Discovering zones across 1 projects...\n",
      "Querying VMs across 127 zones...\n",
      "Progress: 50/127 zones checked\n",
      "Progress: 100/127 zones checked\n",
      "\n",
      "Found 2 matching VMs\n",
      "\n",
      "Project                        Zone                 Name                           Status       Machine Type         Internal IP     External IP    \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "compute-app-427709             europe-west4-a       benchmark-instance-standard-01 RUNNING      c4-standard-2        10.164.0.2      34.90.240.138  \n",
      "compute-app-427709             europe-west4-a       benchmark-instance-standard-02 RUNNING      c4-standard-2        10.164.0.3      34.90.225.213  \n"
     ]
    }
   ],
   "source": [
    "# Query standard VMs\n",
    "!python $VM_SCRIPT $VM_NAME_PREFIX --output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using current project: compute-app-427709\n",
      "Discovering zones across 1 projects...\n",
      "Querying VMs across 127 zones...\n",
      "Progress: 50/127 zones checked\n",
      "Progress: 100/127 zones checked\n",
      "\n",
      "Found 2 matching VMs\n",
      "Executing command on 2 VMs: uptime\n",
      "\n",
      "============================================================\n",
      "✓ benchmark-instance-standard-01: Success\n",
      "STDOUT:\n",
      " 14:17:53 up  1:14,  1 user,  load average: 0.00, 0.00, 0.00\n",
      "\n",
      "\n",
      "============================================================\n",
      "✓ benchmark-instance-standard-02: Success\n",
      "STDOUT:\n",
      " 14:17:53 up  1:14,  1 user,  load average: 0.00, 0.00, 0.00\n",
      "\n",
      "\n",
      "============================================================\n",
      "Completed: 2/2 successful\n"
     ]
    }
   ],
   "source": [
    "# Run commands over ssh for running VMs\n",
    "!python $VM_SCRIPT $VM_NAME_PREFIX --ssh \"uptime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results from Running VMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using current project: compute-app-427709\n",
      "Discovering zones across 1 projects...\n",
      "Querying VMs across 127 zones...\n",
      "Progress: 50/127 zones checked\n",
      "Progress: 100/127 zones checked\n",
      "\n",
      "Found 2 matching VMs\n",
      "Copying from 2 VMs...\n",
      "✓ benchmark-instance-standard-01: Success\n",
      "✓ benchmark-instance-standard-02: Success\n",
      "\n",
      "Completed: 2/2 successful\n"
     ]
    }
   ],
   "source": [
    "# Download benchmark_results.csv\n",
    "!python $VM_SCRIPT $VM_NAME_PREFIX '--scp-source' 'vm:/solver-benchmark/results/benchmark_results.csv' '--scp-dest' $SESSION_RESULTS_DIR/scp/{{vm_name}}/benchmark_results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using current project: compute-app-427709\n",
      "Discovering zones across 1 projects...\n",
      "Querying VMs across 127 zones...\n",
      "Progress: 50/127 zones checked\n",
      "Progress: 100/127 zones checked\n",
      "\n",
      "Found 2 matching VMs\n",
      "Copying from 2 VMs...\n",
      "✓ benchmark-instance-standard-01: Success\n",
      "✓ benchmark-instance-standard-02: Success\n",
      "\n",
      "Completed: 2/2 successful\n"
     ]
    }
   ],
   "source": [
    "# Download startup-script.log\n",
    "!python $VM_SCRIPT $VM_NAME_PREFIX '--scp-source' 'vm:/var/log/startup-script.log' '--scp-dest' $SESSION_RESULTS_DIR/scp/{{vm_name}}/startup-script.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using current project: compute-app-427709\n",
      "Discovering zones across 1 projects...\n",
      "Querying VMs across 127 zones...\n",
      "Progress: 50/127 zones checked\n",
      "Progress: 100/127 zones checked\n",
      "\n",
      "Found 2 matching VMs\n",
      "Copying from 2 VMs...\n",
      "✓ benchmark-instance-standard-01: Success\n",
      "✓ benchmark-instance-standard-02: Success\n",
      "\n",
      "Completed: 2/2 successful\n"
     ]
    }
   ],
   "source": [
    "# Download runner logs (recursive)\n",
    "!python $VM_SCRIPT $VM_NAME_PREFIX '--scp-source' 'vm:/solver-benchmark/runner/logs/' '--scp-dest' $SESSION_RESULTS_DIR/scp/{{vm_name}}/logs/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GCS results found\n",
      "No GCS logs found\n"
     ]
    }
   ],
   "source": [
    "# Download results from GCS\n",
    "!gsutil -m cp -r gs://solver-benchmarks/results/$SESSION_ID'*' $SESSION_RESULTS_DIR/gcs/results 2>/dev/null || echo \"No GCS results found\"\n",
    "!gsutil -m cp -r gs://solver-benchmarks/logs/$SESSION_ID'*' $SESSION_RESULTS_DIR/gcs/logs 2>/dev/null || echo \"No GCS logs found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/madhukar/oet/solver-benchmark/results/sample_run/\u001b[0m\n",
      "└── \u001b[01;34mscp\u001b[0m\n",
      "    ├── \u001b[01;34mbenchmark-instance-standard-01\u001b[0m\n",
      "    │   ├── \u001b[00mbenchmark_results.csv\u001b[0m\n",
      "    │   ├── \u001b[01;34mlogs\u001b[0m\n",
      "    │   │   └── \u001b[01;34mlogs\u001b[0m\n",
      "    │   │       └── \u001b[00mpypsa-eur-elec-op-ucconv-2-3h-highs-1.10.0.log\u001b[0m\n",
      "    │   └── \u001b[00mstartup-script.log\u001b[0m\n",
      "    └── \u001b[01;34mbenchmark-instance-standard-02\u001b[0m\n",
      "        ├── \u001b[00mbenchmark_results.csv\u001b[0m\n",
      "        ├── \u001b[01;34mlogs\u001b[0m\n",
      "        │   └── \u001b[01;34mlogs\u001b[0m\n",
      "        │       └── \u001b[00mSienna_modified_RTS_GMLC_DA_sys_NetDC_Horizon24_Day332-1-1h-highs-1.10.0.log\u001b[0m\n",
      "        └── \u001b[00mstartup-script.log\u001b[0m\n",
      "\n",
      "8 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "# List downloaded results\n",
    "!tree $SESSION_RESULTS_DIR/ 2>/dev/null || find $SESSION_RESULTS_DIR -type f | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GCS results found\n",
      "  Loaded 1 rows from benchmark-instance-standard-01\n",
      "  Loaded 1 rows from benchmark-instance-standard-02\n",
      "Found 2 SCP CSV files\n",
      "\n",
      "Successfully combined 2 total rows (after deduplication)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_combine_results(session_dir):\n",
    "    \"\"\"\n",
    "    Load benchmark results from GCS and SCP, preferring GCS over SCP.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined results from all sources\n",
    "    \"\"\"\n",
    "    session_dir = Path(session_dir)\n",
    "    all_dfs = []\n",
    "\n",
    "    # Load GCS results first (preferred)\n",
    "    gcs_dir = session_dir / \"gcs\"\n",
    "    gcs_count = 0\n",
    "    if gcs_dir.exists():\n",
    "        for csv_file in sorted(gcs_dir.glob(\"**/benchmark_results.csv\")):\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                df['_source'] = 'gcs'\n",
    "                all_dfs.append(df)\n",
    "                gcs_count += 1\n",
    "                print(f\"  Loaded {len(df)} rows from {csv_file.parent.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {csv_file}: {e}\")\n",
    "\n",
    "    print(f\"Found {gcs_count} GCS CSV files\" if gcs_count else \"No GCS results found\")\n",
    "\n",
    "    # Load SCP results\n",
    "    scp_dir = session_dir / \"scp\"\n",
    "    scp_count = 0\n",
    "    if scp_dir.exists():\n",
    "        for csv_file in sorted(scp_dir.glob(\"**/benchmark_results.csv\")):\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                df['_source'] = 'scp'\n",
    "                all_dfs.append(df)\n",
    "                scp_count += 1\n",
    "                print(f\"  Loaded {len(df)} rows from {csv_file.parent.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {csv_file}: {e}\")\n",
    "\n",
    "    print(f\"Found {scp_count} SCP CSV files\" if scp_count else \"No SCP results found\")\n",
    "\n",
    "    # Combine all dataframes and deduplicate, preferring GCS\n",
    "    if all_dfs:\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "        # Deduplicate: keep GCS over SCP for same benchmark runs\n",
    "        # Sort so GCS comes first, then drop duplicates keeping first occurrence\n",
    "        combined_df = combined_df.sort_values('_source', key=lambda x: (x != 'gcs')).reset_index(drop=True)\n",
    "\n",
    "        # Identify duplicates by benchmark data (all columns except _source)\n",
    "        cols_to_check = [c for c in combined_df.columns if c != '_source']\n",
    "        combined_df = combined_df.drop_duplicates(subset=cols_to_check, keep='first')\n",
    "\n",
    "        print(f\"\\nSuccessfully combined {len(combined_df)} total rows (after deduplication)\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"\\nNo CSV files found\")\n",
    "        return None\n",
    "\n",
    "# Load results\n",
    "results = load_and_combine_results(SESSION_RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timeout by machine type:\n",
      "  c4-standard-2: 600s\n"
     ]
    }
   ],
   "source": [
    "def get_timeout_by_machine(runs: pd.DataFrame) -> dict:\n",
    "    \"\"\"Infer timeout values by machine type from actual results.\"\"\"\n",
    "    timeout_map = {}\n",
    "\n",
    "    # Find VM hostname column\n",
    "    vm_col = None\n",
    "    vm_candidates = {\"hostname\", \"host\", \"vmhostname\", \"vm\", \"instancename\", \"instance\", \"_vm\"}\n",
    "    for c in runs.columns:\n",
    "        normalized = c.lower().replace(\" \", \"\").replace(\"_\", \"\").replace(\"-\", \"\")\n",
    "        if normalized in vm_candidates:\n",
    "            vm_col = c\n",
    "            break\n",
    "\n",
    "    if vm_col and \"Timeout\" in runs.columns:\n",
    "        # Map hostname patterns to machine types\n",
    "        for _, row in runs.iterrows():\n",
    "            hostname = str(row.get(vm_col, \"\"))\n",
    "            timeout = row.get(\"Timeout\")\n",
    "\n",
    "            if pd.notna(timeout):\n",
    "                if \"highmem\" in hostname.lower():\n",
    "                    timeout_map[\"c4-highmem-8\"] = timeout\n",
    "                elif \"standard\" in hostname.lower():\n",
    "                    timeout_map[\"c4-standard-2\"] = timeout\n",
    "\n",
    "    return timeout_map\n",
    "\n",
    "if results is not None:\n",
    "    timeout_by_machine = get_timeout_by_machine(results)\n",
    "    print(\"\\nTimeout by machine type:\")\n",
    "    for machine_type, timeout in timeout_by_machine.items():\n",
    "        print(f\"  {machine_type}: {timeout}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMBINED RESULTS SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "Total rows: 2\n",
      "Columns: ['Benchmark', 'Size', 'Solver', 'Solver Version', 'Solver Release Year', 'Status', 'Termination Condition', 'Runtime (s)', 'Memory Usage (MB)', 'Objective Value', 'Max Integrality Violation', 'Duality Gap', 'Reported Runtime (s)', 'Timeout', 'Hostname', 'Run ID', 'Timestamp', '_source']\n",
      "\n",
      "VM breakdown:\n",
      "Hostname\n",
      "benchmark-instance-standard-01    1\n",
      "benchmark-instance-standard-02    1\n",
      "\n",
      "Source breakdown:\n",
      "_source\n",
      "scp    2\n",
      "\n",
      "Solver breakdown:\n",
      "Solver\n",
      "highs    2\n",
      "\n",
      "Status breakdown:\n",
      "Status\n",
      "TO    2\n",
      "\n",
      "Unique benchmarks: 2\n",
      "Unique sizes: 2\n"
     ]
    }
   ],
   "source": [
    "if results is not None:\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"COMBINED RESULTS SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    print(f\"\\nTotal rows: {len(results)}\")\n",
    "    print(f\"Columns: {list(results.columns)}\")\n",
    "\n",
    "    if 'Hostname' in results.columns:\n",
    "        print(f\"\\nVM breakdown:\")\n",
    "        print(results['Hostname'].value_counts().to_string())\n",
    "\n",
    "    print(f\"\\nSource breakdown:\")\n",
    "    print(results['_source'].value_counts().to_string())\n",
    "\n",
    "    if 'Solver' in results.columns:\n",
    "        print(f\"\\nSolver breakdown:\")\n",
    "        print(results['Solver'].value_counts().to_string())\n",
    "\n",
    "    if 'Status' in results.columns:\n",
    "        print(f\"\\nStatus breakdown:\")\n",
    "        print(results['Status'].value_counts().to_string())\n",
    "\n",
    "    if 'Benchmark' in results.columns and 'Size' in results.columns:\n",
    "        print(f\"\\nUnique benchmarks: {results['Benchmark'].nunique()}\")\n",
    "        print(f\"Unique sizes: {results['Size'].nunique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
