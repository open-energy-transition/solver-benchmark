{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime-Optimized VM Allocation for Benchmarks\n",
    "\n",
    "This notebook implements a bin packing algorithm to allocate benchmarks to VMs based on actual HiGHS v1.10 runtime data, aiming to minimize total runtime variance across VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "# CONFIGURATION\n",
    "MAX_RUNTIME_PER_VM_SECONDS = 3600  # Set to a number (e.g., 3600 for 1 hour) to cap VM runtime, or None for no limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Runtime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 successful HiGHS v1.10/hipo benchmark runs\n",
      "Runtime range: 900s (15 min) to 3600s (1 hour)\n",
      "Solvers included: ['highs']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Benchmark</th>\n",
       "      <th>Size</th>\n",
       "      <th>Solver</th>\n",
       "      <th>Solver Version</th>\n",
       "      <th>Solver Release Year</th>\n",
       "      <th>Status</th>\n",
       "      <th>Termination Condition</th>\n",
       "      <th>Runtime (s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Objective Value</th>\n",
       "      <th>Max Integrality Violation</th>\n",
       "      <th>Duality Gap</th>\n",
       "      <th>Reported Runtime (s)</th>\n",
       "      <th>Timeout</th>\n",
       "      <th>Hostname</th>\n",
       "      <th>Run ID</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>DCOPF-Carolinas_uc_2M</td>\n",
       "      <td>1-997</td>\n",
       "      <td>highs</td>\n",
       "      <td>1.10.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>ok</td>\n",
       "      <td>optimal</td>\n",
       "      <td>2080.905538397994</td>\n",
       "      <td>2878.12</td>\n",
       "      <td>4463695.700557045</td>\n",
       "      <td>3.502851030676985e-11</td>\n",
       "      <td>9.478984509261144e-05</td>\n",
       "      <td>2079.6019673347478</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>benchmark-instance-z2-m41</td>\n",
       "      <td>20250503_040156_benchmark-instance-z2-m41</td>\n",
       "      <td>2025-05-03 21:11:07.318952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>pypsa-eur-elec-op-ucconv</td>\n",
       "      <td>10-24h</td>\n",
       "      <td>highs</td>\n",
       "      <td>1.10.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>ok</td>\n",
       "      <td>optimal</td>\n",
       "      <td>2060.496564678004</td>\n",
       "      <td>2751.096</td>\n",
       "      <td>10527862989.467287</td>\n",
       "      <td>3.219646771412954e-15</td>\n",
       "      <td>9.998511252765709e-05</td>\n",
       "      <td>2059.3151636123657</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>benchmark-instance-z2-m38</td>\n",
       "      <td>20250501_144136_benchmark-instance-z2-m38</td>\n",
       "      <td>2025-05-02 00:26:36.498948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Sienna_modified_RTS_GMLC_DA_sys_NetTransport_H...</td>\n",
       "      <td>1-1h</td>\n",
       "      <td>highs</td>\n",
       "      <td>1.10.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>ok</td>\n",
       "      <td>optimal</td>\n",
       "      <td>1198.634356795999</td>\n",
       "      <td>838.736</td>\n",
       "      <td>796622.7803292888</td>\n",
       "      <td>3.998330308359003e-15</td>\n",
       "      <td>9.978093355103796e-05</td>\n",
       "      <td>1198.434877872467</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>benchmark-instance-z-m35</td>\n",
       "      <td>20250429_090601_benchmark-instance-z-m35</td>\n",
       "      <td>2025-04-30 05:59:11.473448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>pypsa-eur-sec</td>\n",
       "      <td>5-12h</td>\n",
       "      <td>highs</td>\n",
       "      <td>1.10.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>ok</td>\n",
       "      <td>optimal</td>\n",
       "      <td>1472.7624695099948</td>\n",
       "      <td>1671.124</td>\n",
       "      <td>15716534864.16614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1468.8594851493835</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>benchmark-instance-z-m24</td>\n",
       "      <td>20250429_090644_benchmark-instance-z-m24</td>\n",
       "      <td>2025-04-30 09:42:31.254238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Sienna_modified_RTS_GMLC_DA_sys_NetCopperPlate...</td>\n",
       "      <td>1-1h</td>\n",
       "      <td>highs</td>\n",
       "      <td>1.10.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>ok</td>\n",
       "      <td>optimal</td>\n",
       "      <td>1272.9416091339954</td>\n",
       "      <td>965.796</td>\n",
       "      <td>796885.0277238319</td>\n",
       "      <td>1.5987211554602254e-14</td>\n",
       "      <td>9.849357480446288e-05</td>\n",
       "      <td>1272.7875311374664</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>benchmark-instance-z-m23</td>\n",
       "      <td>20250429_090640_benchmark-instance-z-m23</td>\n",
       "      <td>2025-04-30 09:36:56.834400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Benchmark    Size Solver  \\\n",
       "86                               DCOPF-Carolinas_uc_2M   1-997  highs   \n",
       "196                           pypsa-eur-elec-op-ucconv  10-24h  highs   \n",
       "261  Sienna_modified_RTS_GMLC_DA_sys_NetTransport_H...    1-1h  highs   \n",
       "317                                      pypsa-eur-sec   5-12h  highs   \n",
       "365  Sienna_modified_RTS_GMLC_DA_sys_NetCopperPlate...    1-1h  highs   \n",
       "\n",
       "    Solver Version Solver Release Year Status Termination Condition  \\\n",
       "86          1.10.0                2025     ok               optimal   \n",
       "196         1.10.0                2025     ok               optimal   \n",
       "261         1.10.0                2025     ok               optimal   \n",
       "317         1.10.0                2025     ok               optimal   \n",
       "365         1.10.0                2025     ok               optimal   \n",
       "\n",
       "            Runtime (s) Memory Usage (MB)     Objective Value  \\\n",
       "86    2080.905538397994           2878.12   4463695.700557045   \n",
       "196   2060.496564678004          2751.096  10527862989.467287   \n",
       "261   1198.634356795999           838.736   796622.7803292888   \n",
       "317  1472.7624695099948          1671.124   15716534864.16614   \n",
       "365  1272.9416091339954           965.796   796885.0277238319   \n",
       "\n",
       "    Max Integrality Violation            Duality Gap Reported Runtime (s)  \\\n",
       "86      3.502851030676985e-11  9.478984509261144e-05   2079.6019673347478   \n",
       "196     3.219646771412954e-15  9.998511252765709e-05   2059.3151636123657   \n",
       "261     3.998330308359003e-15  9.978093355103796e-05    1198.434877872467   \n",
       "317                       NaN                    NaN   1468.8594851493835   \n",
       "365    1.5987211554602254e-14  9.849357480446288e-05   1272.7875311374664   \n",
       "\n",
       "    Timeout                   Hostname  \\\n",
       "86   3600.0  benchmark-instance-z2-m41   \n",
       "196  3600.0  benchmark-instance-z2-m38   \n",
       "261  3600.0   benchmark-instance-z-m35   \n",
       "317  3600.0   benchmark-instance-z-m24   \n",
       "365  3600.0   benchmark-instance-z-m23   \n",
       "\n",
       "                                        Run ID                   Timestamp  \n",
       "86   20250503_040156_benchmark-instance-z2-m41  2025-05-03 21:11:07.318952  \n",
       "196  20250501_144136_benchmark-instance-z2-m38  2025-05-02 00:26:36.498948  \n",
       "261   20250429_090601_benchmark-instance-z-m35  2025-04-30 05:59:11.473448  \n",
       "317   20250429_090644_benchmark-instance-z-m24  2025-04-30 09:42:31.254238  \n",
       "365   20250429_090640_benchmark-instance-z-m23  2025-04-30 09:36:56.834400  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load HiGHS runtime data (including HiGHS-hipo if available)\n",
    "highs_data = pd.read_csv('main_results.csv', header=None, names=[\n",
    "    'Benchmark', 'Size', 'Solver', 'Solver Version', 'Solver Release Year',\n",
    "    'Status', 'Termination Condition', 'Runtime (s)', 'Memory Usage (MB)',\n",
    "    'Objective Value', 'Max Integrality Violation', 'Duality Gap',\n",
    "    'Reported Runtime (s)', 'Timeout', 'Hostname', 'Run ID', 'Timestamp'\n",
    "])\n",
    "\n",
    "# Filter for HiGHS 1.10.0 (2025) and HiGHS-hipo successful runs\n",
    "highs_v110 = highs_data[\n",
    "    ((highs_data['Solver Version'] == '1.10.0') |\n",
    "     (highs_data['Solver'] == 'highs-hipo')) &\n",
    "    (highs_data['Status'] == 'ok')\n",
    "]\n",
    "\n",
    "# Filter runs with runtime between 15 minutes (900s) and 1 hour (3600s)\n",
    "MIN_RUNTIME_SECONDS = 900   # 15 minutes\n",
    "MAX_RUNTIME_SECONDS = 3600  # 1 hour\n",
    "\n",
    "highs_v110 = highs_v110[\n",
    "    (highs_v110['Runtime (s)'].astype(float) >= MIN_RUNTIME_SECONDS) &\n",
    "    (highs_v110['Runtime (s)'].astype(float) <= MAX_RUNTIME_SECONDS)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(highs_v110)} successful HiGHS v1.10/hipo benchmark runs\")\n",
    "print(f\"Runtime range: {MIN_RUNTIME_SECONDS}s ({MIN_RUNTIME_SECONDS/60:.0f} min) to {MAX_RUNTIME_SECONDS}s ({MAX_RUNTIME_SECONDS/3600:.0f} hour)\")\n",
    "print(f\"Solvers included: {highs_v110['Solver'].unique()}\")\n",
    "highs_v110.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime data available for 13 benchmarks\n",
      "Total runtime: 25216.991877514996 seconds (7.0 hours)\n"
     ]
    }
   ],
   "source": [
    "# Create benchmark runtime mapping\n",
    "benchmark_runtimes = {}\n",
    "for _, row in highs_v110.iterrows():\n",
    "    benchmark_key = f\"{row['Benchmark']}-{row['Size']}\"\n",
    "    try:\n",
    "        benchmark_runtimes[benchmark_key] = float(row['Runtime (s)'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"Runtime data available for {len(benchmark_runtimes)} benchmarks\")\n",
    "print(f\"Total runtime: {sum(benchmark_runtimes.values())} seconds ({sum(benchmark_runtimes.values())/3600:.1f} hours)\")\n",
    "# print(sum(benchmark_runtimes.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Benchmark Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total benchmark instances (from filtered dataset): 13\n",
      "  S: 0\n",
      "  M: 13\n",
      "  L: 0\n",
      "All instances have runtime data from highs_v110\n"
     ]
    }
   ],
   "source": [
    "# Load benchmark metadata to get size categories and URLs\n",
    "meta = yaml.safe_load(open('results/metadata.yaml'))\n",
    "\n",
    "# Create a lookup for metadata\n",
    "metadata_lookup = {}\n",
    "for name, benchmark in meta['benchmarks'].items():\n",
    "    for size_info in benchmark['Sizes']:\n",
    "        instance_key = f\"{name}-{size_info['Name']}\"\n",
    "        metadata_lookup[instance_key] = size_info\n",
    "\n",
    "# Categorize benchmarks by size and create instances list\n",
    "# Use ONLY the filtered highs_v110 dataset\n",
    "benchmarks_by_size = {'S': [], 'M': [], 'L': []}\n",
    "all_benchmark_instances = []\n",
    "\n",
    "for _, row in highs_v110.iterrows():\n",
    "    instance_key = f\"{row['Benchmark']}-{row['Size']}\"\n",
    "\n",
    "    # Get metadata for this instance\n",
    "    size_info = metadata_lookup.get(instance_key)\n",
    "    if size_info is None:\n",
    "        print(f\"Warning: No metadata found for {instance_key}\")\n",
    "        continue\n",
    "\n",
    "    instance = {\n",
    "        'name': row['Benchmark'],\n",
    "        'size_name': row['Size'],\n",
    "        'size_category': size_info['Size'],\n",
    "        'instance_key': instance_key,\n",
    "        'runtime': float(row['Runtime (s)']),\n",
    "        'num_variables': size_info.get('Num. variables', 0),\n",
    "        'num_constraints': size_info.get('Num. constraints', 0),\n",
    "        'url': size_info['URL']\n",
    "    }\n",
    "\n",
    "    benchmarks_by_size[size_info['Size']].append(instance)\n",
    "    all_benchmark_instances.append(instance)\n",
    "\n",
    "print(f\"Total benchmark instances (from filtered dataset): {len(all_benchmark_instances)}\")\n",
    "for size, instances in benchmarks_by_size.items():\n",
    "    print(f\"  {size}: {len(instances)}\")\n",
    "print(f\"All instances have runtime data from highs_v110\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin Packing Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMAllocation:\n",
    "    def __init__(self, vm_id: int):\n",
    "        self.vm_id = vm_id\n",
    "        self.benchmarks = []\n",
    "        self.total_runtime = 0.0\n",
    "\n",
    "    def add_benchmark(self, benchmark: dict):\n",
    "        \"\"\"Add benchmark with real runtime data only\"\"\"\n",
    "        if benchmark['runtime'] is None:\n",
    "            raise ValueError(f\"Benchmark {benchmark['instance_key']} has no runtime data!\")\n",
    "\n",
    "        self.benchmarks.append(benchmark)\n",
    "        self.total_runtime += benchmark['runtime']\n",
    "\n",
    "    def get_total_runtime(self):\n",
    "        return self.total_runtime\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        # For heap operations - compare by total runtime\n",
    "        return self.total_runtime < other.total_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_fit_decreasing(benchmarks: List[dict], num_vms: int) -> List[VMAllocation]:\n",
    "    \"\"\"\n",
    "    First Fit Decreasing bin packing algorithm.\n",
    "    Uses ONLY benchmarks with real runtime data.\n",
    "    \"\"\"\n",
    "    # Filter to only benchmarks with real runtime data\n",
    "    runtime_benchmarks = [b for b in benchmarks if b['runtime'] is not None]\n",
    "    print(f\"Using {len(runtime_benchmarks)} benchmarks with real runtime data (filtered from {len(benchmarks)} total)\")\n",
    "\n",
    "    # Create VMs\n",
    "    vms = [VMAllocation(i) for i in range(num_vms)]\n",
    "\n",
    "    # Sort benchmarks by runtime (descending)\n",
    "    sorted_benchmarks = sorted(runtime_benchmarks, key=lambda x: x['runtime'], reverse=True)\n",
    "\n",
    "    # Assign benchmarks to VMs\n",
    "    for benchmark in sorted_benchmarks:\n",
    "        # Find VM with minimum current runtime\n",
    "        min_vm = min(vms, key=lambda vm: vm.total_runtime)\n",
    "        min_vm.add_benchmark(benchmark)\n",
    "\n",
    "    return vms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_processing_time_first(benchmarks: List[dict], num_vms: int) -> List[VMAllocation]:\n",
    "    \"\"\"\n",
    "    Longest Processing Time First algorithm using a min-heap.\n",
    "    Uses ONLY benchmarks with real runtime data.\n",
    "    \"\"\"\n",
    "    # Filter to only benchmarks with real runtime data\n",
    "    runtime_benchmarks = [b for b in benchmarks if b['runtime'] is not None]\n",
    "    print(f\"Using {len(runtime_benchmarks)} benchmarks with real runtime data (filtered from {len(benchmarks)} total)\")\n",
    "\n",
    "    # Create VMs and initialize heap\n",
    "    vms = [VMAllocation(i) for i in range(num_vms)]\n",
    "    vm_heap = list(vms)  # Min-heap based on total runtime\n",
    "    heapq.heapify(vm_heap)\n",
    "\n",
    "    # Sort benchmarks by runtime (descending)\n",
    "    sorted_benchmarks = sorted(runtime_benchmarks, key=lambda x: x['runtime'], reverse=True)\n",
    "\n",
    "    # Assign benchmarks\n",
    "    for benchmark in sorted_benchmarks:\n",
    "        # Get VM with minimum load\n",
    "        min_vm = heapq.heappop(vm_heap)\n",
    "        min_vm.add_benchmark(benchmark)\n",
    "        # Re-insert VM into heap\n",
    "        heapq.heappush(vm_heap, min_vm)\n",
    "\n",
    "    return vms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_partition(benchmarks: List[dict], num_vms: int, max_runtime_per_vm: float = None) -> List[VMAllocation]:\n",
    "    \"\"\"\n",
    "    Balanced partition algorithm that tries to achieve equal total runtime per VM.\n",
    "    Uses ONLY benchmarks with real runtime data.\n",
    "\n",
    "    Args:\n",
    "        benchmarks: List of benchmark dictionaries with runtime data\n",
    "        num_vms: Initial number of VMs to create\n",
    "        max_runtime_per_vm: Maximum runtime allowed per VM (in seconds). If None, no limit.\n",
    "    \"\"\"\n",
    "    # Filter to only benchmarks with real runtime data\n",
    "    runtime_benchmarks = [b for b in benchmarks if b['runtime'] is not None]\n",
    "    print(f\"Using {len(runtime_benchmarks)} benchmarks with real runtime data (filtered from {len(benchmarks)} total)\")\n",
    "\n",
    "    # Calculate total runtime and target per VM\n",
    "    total_runtime = sum(b['runtime'] for b in runtime_benchmarks)\n",
    "    target_runtime_per_vm = total_runtime / num_vms\n",
    "\n",
    "    # If max_runtime_per_vm is set and target exceeds it, increase num_vms\n",
    "    if max_runtime_per_vm is not None and target_runtime_per_vm > max_runtime_per_vm:\n",
    "        original_num_vms = num_vms\n",
    "        num_vms = int(np.ceil(total_runtime / max_runtime_per_vm))\n",
    "        target_runtime_per_vm = total_runtime / num_vms\n",
    "        print(f\"⚠️  Target runtime {target_runtime_per_vm/3600:.1f}h exceeds max {max_runtime_per_vm/3600:.1f}h\")\n",
    "        print(f\"   Increasing VMs from {original_num_vms} to {num_vms} to respect runtime cap\")\n",
    "\n",
    "    print(f\"Target runtime per VM: {target_runtime_per_vm:.1f} seconds ({target_runtime_per_vm/3600:.1f} hours)\")\n",
    "    if max_runtime_per_vm is not None:\n",
    "        print(f\"Max runtime per VM: {max_runtime_per_vm:.1f} seconds ({max_runtime_per_vm/3600:.1f} hours)\")\n",
    "\n",
    "    # Create VMs\n",
    "    vms = [VMAllocation(i) for i in range(num_vms)]\n",
    "\n",
    "    # Sort benchmarks by runtime (descending)\n",
    "    sorted_benchmarks = sorted(runtime_benchmarks, key=lambda x: x['runtime'], reverse=True)\n",
    "\n",
    "    # Assign benchmarks with balance consideration\n",
    "    for benchmark in sorted_benchmarks:\n",
    "        benchmark_runtime = benchmark['runtime']\n",
    "\n",
    "        # Find VM that would be closest to target after adding this benchmark\n",
    "        best_vm = None\n",
    "        best_score = float('inf')\n",
    "\n",
    "        for vm in vms:\n",
    "            current_runtime = vm.total_runtime\n",
    "            after_runtime = current_runtime + benchmark_runtime\n",
    "\n",
    "            # Skip if this would exceed max runtime (only if max is set)\n",
    "            if max_runtime_per_vm is not None and after_runtime > max_runtime_per_vm:\n",
    "                # Check if any VM can still fit this benchmark\n",
    "                if current_runtime + benchmark_runtime <= max_runtime_per_vm * 1.05:  # Allow 5% overflow\n",
    "                    pass  # Continue to consider this VM\n",
    "                else:\n",
    "                    continue  # Skip this VM\n",
    "\n",
    "            # Score based on deviation from target\n",
    "            score = abs(after_runtime - target_runtime_per_vm)\n",
    "\n",
    "            # Prefer VMs that are under-loaded\n",
    "            if current_runtime < target_runtime_per_vm:\n",
    "                score *= 0.8  # Bonus for under-loaded VMs\n",
    "\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_vm = vm\n",
    "\n",
    "        # Add benchmark to best VM (or create new VM if needed)\n",
    "        if best_vm is None and max_runtime_per_vm is not None:\n",
    "            # All VMs are at capacity, create a new one\n",
    "            print(f\"⚠️  All VMs at capacity, creating additional VM for benchmark {benchmark['instance_key']}\")\n",
    "            best_vm = VMAllocation(len(vms))\n",
    "            vms.append(best_vm)\n",
    "\n",
    "        if best_vm is not None:\n",
    "            best_vm.add_benchmark(benchmark)\n",
    "        else:\n",
    "            print(f\"❌ Could not allocate benchmark {benchmark['instance_key']} (runtime: {benchmark_runtime:.1f}s)\")\n",
    "\n",
    "    return vms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_allocation(vms: List[VMAllocation], algorithm_name: str):\n",
    "    \"\"\"\n",
    "    Analyze and print statistics for a VM allocation.\n",
    "    \"\"\"\n",
    "    runtimes = [vm.total_runtime for vm in vms]\n",
    "\n",
    "    # Filter out empty VMs (should not happen with real runtime data only)\n",
    "    active_vms = [vm for vm in vms if vm.total_runtime > 0]\n",
    "    active_runtimes = [vm.total_runtime for vm in active_vms]\n",
    "\n",
    "    print(f\"\\n=== {algorithm_name} ===\")\n",
    "    print(f\"Total VMs created: {len(vms)}\")\n",
    "    print(f\"Active VMs (with benchmarks): {len(active_vms)}\")\n",
    "    print(f\"Empty VMs: {len(vms) - len(active_vms)}\")\n",
    "\n",
    "    if len(active_vms) > 0:\n",
    "        print(f\"Total runtime: {sum(active_runtimes):.1f} seconds ({sum(active_runtimes)/3600:.1f} hours)\")\n",
    "        print(f\"Average runtime per active VM: {np.mean(active_runtimes):.1f} seconds ({np.mean(active_runtimes)/3600:.1f} hours)\")\n",
    "        print(f\"Runtime standard deviation: {np.std(active_runtimes):.1f} seconds ({np.std(active_runtimes)/3600:.1f} hours)\")\n",
    "        print(f\"Min runtime: {min(active_runtimes):.1f} seconds ({min(active_runtimes)/3600:.1f} hours)\")\n",
    "        print(f\"Max runtime: {max(active_runtimes):.1f} seconds ({max(active_runtimes)/3600:.1f} hours)\")\n",
    "        print(f\"Runtime ratio (max/min): {max(active_runtimes)/min(active_runtimes):.2f}\")\n",
    "\n",
    "        # Efficiency (how balanced the allocation is)\n",
    "        efficiency = 1 - (np.std(active_runtimes) / np.mean(active_runtimes))\n",
    "        print(f\"Load balance efficiency: {efficiency:.3f} (1.0 = perfect balance)\")\n",
    "    else:\n",
    "        print(\"No active VMs found!\")\n",
    "        efficiency = 0\n",
    "\n",
    "    return {\n",
    "        'algorithm': algorithm_name,\n",
    "        'total_runtime': sum(active_runtimes) if active_vms else 0,\n",
    "        'std_runtime': np.std(active_runtimes) if active_vms else 0,\n",
    "        'max_runtime': max(active_runtimes) if active_vms else 0,\n",
    "        'min_runtime': min(active_runtimes) if active_vms else 0,\n",
    "        'efficiency': efficiency,\n",
    "        'runtimes': runtimes,\n",
    "        'active_vms': len(active_vms),\n",
    "        'num_vms': len(vms)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 benchmarks with real HiGHS runtime data\n",
      "Excluded 0 benchmarks without runtime data\n",
      "Total runtime of included benchmarks: 7.0 hours\n",
      "\n",
      "⚙️  Runtime cap enabled: 3600 seconds (1.0 hours) per VM\n",
      "\n",
      "Benchmark separation by size category:\n",
      "  L-size (highmem): 0 benchmarks, 0.0 hours\n",
      "  S/M-size (standard): 13 benchmarks, 7.0 hours\n",
      "\n",
      "==================================================\n",
      "TESTING L-SIZE BENCHMARKS (HIGHMEM MACHINES)\n",
      "==================================================\n",
      "No L-size benchmarks with runtime data\n",
      "\n",
      "==================================================\n",
      "TESTING S/M-SIZE BENCHMARKS (STANDARD MACHINES)\n",
      "==================================================\n",
      "\n",
      "Testing 8 standard VMs for S/M-size benchmarks:\n",
      "Using 13 benchmarks with real runtime data (filtered from 13 total)\n",
      "Target runtime per VM: 3152.1 seconds (0.9 hours)\n",
      "Max runtime per VM: 3600.0 seconds (1.0 hours)\n",
      "\n",
      "=== S/M-size Balanced Partition (8 VMs) ===\n",
      "Total VMs created: 8\n",
      "Active VMs (with benchmarks): 8\n",
      "Empty VMs: 0\n",
      "Total runtime: 25217.0 seconds (7.0 hours)\n",
      "Average runtime per active VM: 3152.1 seconds (0.9 hours)\n",
      "Runtime standard deviation: 276.7 seconds (0.1 hours)\n",
      "Min runtime: 2706.9 seconds (0.8 hours)\n",
      "Max runtime: 3570.5 seconds (1.0 hours)\n",
      "Runtime ratio (max/min): 1.32\n",
      "Load balance efficiency: 0.912 (1.0 = perfect balance)\n",
      "\n",
      "Testing 10 standard VMs for S/M-size benchmarks:\n",
      "Using 13 benchmarks with real runtime data (filtered from 13 total)\n",
      "Target runtime per VM: 2521.7 seconds (0.7 hours)\n",
      "Max runtime per VM: 3600.0 seconds (1.0 hours)\n",
      "\n",
      "=== S/M-size Balanced Partition (10 VMs) ===\n",
      "Total VMs created: 10\n",
      "Active VMs (with benchmarks): 9\n",
      "Empty VMs: 1\n",
      "Total runtime: 25217.0 seconds (7.0 hours)\n",
      "Average runtime per active VM: 2801.9 seconds (0.8 hours)\n",
      "Runtime standard deviation: 397.0 seconds (0.1 hours)\n",
      "Min runtime: 2080.9 seconds (0.6 hours)\n",
      "Max runtime: 3195.5 seconds (0.9 hours)\n",
      "Runtime ratio (max/min): 1.54\n",
      "Load balance efficiency: 0.858 (1.0 = perfect balance)\n",
      "\n",
      "Testing 12 standard VMs for S/M-size benchmarks:\n",
      "Using 13 benchmarks with real runtime data (filtered from 13 total)\n",
      "Target runtime per VM: 2101.4 seconds (0.6 hours)\n",
      "Max runtime per VM: 3600.0 seconds (1.0 hours)\n",
      "\n",
      "=== S/M-size Balanced Partition (12 VMs) ===\n",
      "Total VMs created: 12\n",
      "Active VMs (with benchmarks): 10\n",
      "Empty VMs: 2\n",
      "Total runtime: 25217.0 seconds (7.0 hours)\n",
      "Average runtime per active VM: 2521.7 seconds (0.7 hours)\n",
      "Runtime standard deviation: 388.4 seconds (0.1 hours)\n",
      "Min runtime: 1947.3 seconds (0.5 hours)\n",
      "Max runtime: 2945.7 seconds (0.8 hours)\n",
      "Runtime ratio (max/min): 1.51\n",
      "Load balance efficiency: 0.846 (1.0 = perfect balance)\n",
      "\n",
      "Testing 15 standard VMs for S/M-size benchmarks:\n",
      "Using 13 benchmarks with real runtime data (filtered from 13 total)\n",
      "Target runtime per VM: 1681.1 seconds (0.5 hours)\n",
      "Max runtime per VM: 3600.0 seconds (1.0 hours)\n",
      "\n",
      "=== S/M-size Balanced Partition (15 VMs) ===\n",
      "Total VMs created: 15\n",
      "Active VMs (with benchmarks): 12\n",
      "Empty VMs: 3\n",
      "Total runtime: 25217.0 seconds (7.0 hours)\n",
      "Average runtime per active VM: 2101.4 seconds (0.6 hours)\n",
      "Runtime standard deviation: 503.9 seconds (0.1 hours)\n",
      "Min runtime: 1272.9 seconds (0.4 hours)\n",
      "Max runtime: 2945.7 seconds (0.8 hours)\n",
      "Runtime ratio (max/min): 2.31\n",
      "Load balance efficiency: 0.760 (1.0 = perfect balance)\n"
     ]
    }
   ],
   "source": [
    "# Use ONLY benchmarks that have real runtime data - no estimation!\n",
    "benchmarks_with_runtime = [b for b in all_benchmark_instances if b['runtime'] is not None]\n",
    "print(f\"Using {len(benchmarks_with_runtime)} benchmarks with real HiGHS runtime data\")\n",
    "print(f\"Excluded {len(all_benchmark_instances) - len(benchmarks_with_runtime)} benchmarks without runtime data\")\n",
    "print(f\"Total runtime of included benchmarks: {sum(b['runtime'] for b in benchmarks_with_runtime)/3600:.1f} hours\")\n",
    "\n",
    "if MAX_RUNTIME_PER_VM_SECONDS is not None:\n",
    "    print(f\"\\n⚙️  Runtime cap enabled: {MAX_RUNTIME_PER_VM_SECONDS} seconds ({MAX_RUNTIME_PER_VM_SECONDS/3600:.1f} hours) per VM\")\n",
    "else:\n",
    "    print(f\"\\n⚙️  No runtime cap configured (unlimited)\")\n",
    "\n",
    "# Separate L-size benchmarks for highmem machines\n",
    "l_size_benchmarks = [b for b in benchmarks_with_runtime if b['size_category'] == 'L']\n",
    "non_l_benchmarks = [b for b in benchmarks_with_runtime if b['size_category'] != 'L']\n",
    "\n",
    "print(f\"\\nBenchmark separation by size category:\")\n",
    "print(f\"  L-size (highmem): {len(l_size_benchmarks)} benchmarks, {sum(b['runtime'] for b in l_size_benchmarks)/3600:.1f} hours\")\n",
    "print(f\"  S/M-size (standard): {len(non_l_benchmarks)} benchmarks, {sum(b['runtime'] for b in non_l_benchmarks)/3600:.1f} hours\")\n",
    "\n",
    "# Test different numbers of VMs for each category\n",
    "results = []\n",
    "\n",
    "# L-size benchmarks (fewer VMs since they need highmem)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TESTING L-SIZE BENCHMARKS (HIGHMEM MACHINES)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "l_vm_options = [2, 3, 4, 5] if len(l_size_benchmarks) > 0 else [1]\n",
    "for num_vms in l_vm_options:\n",
    "    if len(l_size_benchmarks) == 0:\n",
    "        print(\"No L-size benchmarks with runtime data\")\n",
    "        break\n",
    "\n",
    "    print(f\"\\nTesting {num_vms} highmem VMs for L-size benchmarks:\")\n",
    "\n",
    "    bp_vms = balanced_partition(l_size_benchmarks, num_vms, MAX_RUNTIME_PER_VM_SECONDS)\n",
    "    bp_result = analyze_allocation(bp_vms, f\"L-size Balanced Partition ({num_vms} VMs)\")\n",
    "    bp_result['num_vms'] = num_vms\n",
    "    bp_result['size_category'] = 'L'\n",
    "    results.append(bp_result)\n",
    "\n",
    "# S/M-size benchmarks (more VMs with standard machines)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TESTING S/M-SIZE BENCHMARKS (STANDARD MACHINES)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "sm_vm_options = [8, 10, 12, 15]\n",
    "for num_vms in sm_vm_options:\n",
    "    if len(non_l_benchmarks) == 0:\n",
    "        print(\"No S/M-size benchmarks with runtime data\")\n",
    "        break\n",
    "\n",
    "    print(f\"\\nTesting {num_vms} standard VMs for S/M-size benchmarks:\")\n",
    "\n",
    "    bp_vms = balanced_partition(non_l_benchmarks, num_vms, MAX_RUNTIME_PER_VM_SECONDS)\n",
    "    bp_result = analyze_allocation(bp_vms, f\"S/M-size Balanced Partition ({num_vms} VMs)\")\n",
    "    bp_result['num_vms'] = num_vms\n",
    "    bp_result['size_category'] = 'S/M'\n",
    "    results.append(bp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "ALGORITHM COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\\nSize   VM Count  Algorithm                 Efficiency   Max Runtime (h) Std Dev (h) \n",
      "-------------------------------------------------------------------------------------\n",
      "S/M    8         S/M-size Balanced Partition 0.912         1.0             0.1\n",
      "S/M    10        S/M-size Balanced Partition 0.858         0.9             0.1\n",
      "S/M    12        S/M-size Balanced Partition 0.846         0.8             0.1\n",
      "S/M    15        S/M-size Balanced Partition 0.760         0.8             0.1\n",
      "\\n================================================================================\n",
      "BEST CONFIGURATIONS:\n",
      "================================================================================\n",
      "Best S/M-size (standard): 8 VMs (efficiency: 0.912)\n",
      "\\nTotal deployment: 8 standard VMs only\n",
      "Efficiency: 0.912\n"
     ]
    }
   ],
   "source": [
    "# Print summary comparison table\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ALGORITHM COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Separate results by size category\n",
    "l_results = df_results[df_results['size_category'] == 'L'] if 'size_category' in df_results.columns else pd.DataFrame()\n",
    "sm_results = df_results[df_results['size_category'] == 'S/M'] if 'size_category' in df_results.columns else df_results\n",
    "\n",
    "print(f\"\\\\n{'Size':<6} {'VM Count':<9} {'Algorithm':<25} {'Efficiency':<12} {'Max Runtime (h)':<15} {'Std Dev (h)':<12}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    size_cat = row.get('size_category', 'Mixed')\n",
    "    alg_name = row['algorithm'].split('(')[0].strip()\n",
    "    print(f\"{size_cat:<6} {row['num_vms']:<9} {alg_name:<25} \"\n",
    "          f\"{row['efficiency']:.3f}{'':8} {row['max_runtime']/3600:.1f}{'':12} \"\n",
    "          f\"{row['std_runtime']/3600:.1f}\")\n",
    "\n",
    "# Find best configurations for each size category\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(\"BEST CONFIGURATIONS:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if len(l_results) > 0:\n",
    "    best_l = l_results.loc[l_results['efficiency'].idxmax()]\n",
    "    print(f\"Best L-size (highmem): {best_l['num_vms']} VMs (efficiency: {best_l['efficiency']:.3f})\")\n",
    "\n",
    "if len(sm_results) > 0:\n",
    "    best_sm = sm_results.loc[sm_results['efficiency'].idxmax()]\n",
    "    print(f\"Best S/M-size (standard): {best_sm['num_vms']} VMs (efficiency: {best_sm['efficiency']:.3f})\")\n",
    "\n",
    "# Calculate total deployment\n",
    "if len(l_results) > 0 and len(sm_results) > 0:\n",
    "    total_vms = best_l['num_vms'] + best_sm['num_vms']\n",
    "    total_efficiency = (best_l['efficiency'] + best_sm['efficiency']) / 2\n",
    "    print(f\"\\\\nTotal deployment: {total_vms} VMs ({best_l['num_vms']} highmem + {best_sm['num_vms']} standard)\")\n",
    "    print(f\"Average efficiency: {total_efficiency:.3f}\")\n",
    "elif len(sm_results) > 0:\n",
    "    print(f\"\\\\nTotal deployment: {best_sm['num_vms']} standard VMs only\")\n",
    "    print(f\"Efficiency: {best_sm['efficiency']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Optimal Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating optimal allocations with size-based machine separation...\n",
      "Runtime cap: 3600s (1.0h) per VM\n",
      "\n",
      "S/M-size benchmarks: 8 standard VMs\n",
      "Efficiency: 0.912\n",
      "Max VM runtime: 1.0 hours\n",
      "Using 13 benchmarks with real runtime data (filtered from 13 total)\n",
      "Target runtime per VM: 3152.1 seconds (0.9 hours)\n",
      "Max runtime per VM: 3600.0 seconds (1.0 hours)\n",
      "\n",
      "=== Final S/M-size Allocation - Standard ===\n",
      "Total VMs created: 8\n",
      "Active VMs (with benchmarks): 8\n",
      "Empty VMs: 0\n",
      "Total runtime: 25217.0 seconds (7.0 hours)\n",
      "Average runtime per active VM: 3152.1 seconds (0.9 hours)\n",
      "Runtime standard deviation: 276.7 seconds (0.1 hours)\n",
      "Min runtime: 2706.9 seconds (0.8 hours)\n",
      "Max runtime: 3570.5 seconds (1.0 hours)\n",
      "Runtime ratio (max/min): 1.32\n",
      "Load balance efficiency: 0.912 (1.0 = perfect balance)\n",
      "\n",
      "============================================================\n",
      "FINAL ALLOCATION SUMMARY\n",
      "============================================================\n",
      "Total VMs: 8\n",
      "  - Highmem VMs (L-size): 0\n",
      "  - Standard VMs (S/M-size): 8\n",
      "Total allocated runtime: 7.0 hours\n",
      "Machine separation ensures optimal resource utilization\n"
     ]
    }
   ],
   "source": [
    "# Generate optimal allocations for both size categories\n",
    "print(f\"\\n\\nGenerating optimal allocations with size-based machine separation...\")\n",
    "\n",
    "if MAX_RUNTIME_PER_VM_SECONDS is not None:\n",
    "    print(f\"Runtime cap: {MAX_RUNTIME_PER_VM_SECONDS}s ({MAX_RUNTIME_PER_VM_SECONDS/3600:.1f}h) per VM\")\n",
    "\n",
    "optimal_l_vms = []\n",
    "optimal_sm_vms = []\n",
    "best_l_result = None\n",
    "best_sm_result = None\n",
    "\n",
    "# Generate L-size allocation (highmem machines)\n",
    "if len(l_results) > 0:\n",
    "    best_l_result = l_results.loc[l_results['efficiency'].idxmax()]\n",
    "    optimal_l_num_vms = best_l_result['num_vms']\n",
    "\n",
    "    print(f\"\\nL-size benchmarks: {optimal_l_num_vms} highmem VMs\")\n",
    "    print(f\"Efficiency: {best_l_result['efficiency']:.3f}\")\n",
    "    print(f\"Max VM runtime: {best_l_result['max_runtime']/3600:.1f} hours\")\n",
    "\n",
    "    optimal_l_vms = balanced_partition(l_size_benchmarks, optimal_l_num_vms, MAX_RUNTIME_PER_VM_SECONDS)\n",
    "    l_final_result = analyze_allocation(optimal_l_vms, f\"Final L-size Allocation - Highmem\")\n",
    "\n",
    "# Generate S/M-size allocation (standard machines)\n",
    "if len(sm_results) > 0:\n",
    "    best_sm_result = sm_results.loc[sm_results['efficiency'].idxmax()]\n",
    "    optimal_sm_num_vms = best_sm_result['num_vms']\n",
    "\n",
    "    print(f\"\\nS/M-size benchmarks: {optimal_sm_num_vms} standard VMs\")\n",
    "    print(f\"Efficiency: {best_sm_result['efficiency']:.3f}\")\n",
    "    print(f\"Max VM runtime: {best_sm_result['max_runtime']/3600:.1f} hours\")\n",
    "\n",
    "    optimal_sm_vms = balanced_partition(non_l_benchmarks, optimal_sm_num_vms, MAX_RUNTIME_PER_VM_SECONDS)\n",
    "    sm_final_result = analyze_allocation(optimal_sm_vms, f\"Final S/M-size Allocation - Standard\")\n",
    "\n",
    "# Combined summary\n",
    "total_vms = len(optimal_l_vms) + len(optimal_sm_vms)\n",
    "total_runtime = sum(vm.total_runtime for vm in optimal_l_vms + optimal_sm_vms)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL ALLOCATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total VMs: {total_vms}\")\n",
    "print(f\"  - Highmem VMs (L-size): {len(optimal_l_vms)}\")\n",
    "print(f\"  - Standard VMs (S/M-size): {len(optimal_sm_vms)}\")\n",
    "print(f\"Total allocated runtime: {total_runtime/3600:.1f} hours\")\n",
    "print(f\"Machine separation ensures optimal resource utilization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 0 highmem VMs and 8 standard VMs (skipping empty VMs)\n",
      "\n",
      "Exported highs_prelim_test_00.yaml: 1 S/M-size benchmarks, 0.8h runtime\n",
      "Exported highs_prelim_test_01.yaml: 1 S/M-size benchmarks, 0.8h runtime\n",
      "Exported highs_prelim_test_02.yaml: 1 S/M-size benchmarks, 0.8h runtime\n",
      "Exported highs_prelim_test_03.yaml: 2 S/M-size benchmarks, 0.9h runtime\n",
      "Exported highs_prelim_test_04.yaml: 2 S/M-size benchmarks, 0.9h runtime\n",
      "Exported highs_prelim_test_05.yaml: 2 S/M-size benchmarks, 0.9h runtime\n",
      "Exported highs_prelim_test_06.yaml: 2 S/M-size benchmarks, 1.0h runtime\n",
      "Exported highs_prelim_test_07.yaml: 2 S/M-size benchmarks, 1.0h runtime\n",
      "\n",
      "======================================================================\n",
      "Configuration files written to infrastructure/benchmarks/runtime_optimized/\n",
      "Total VMs exported: 8 (skipped 0 empty VMs)\n",
      "  - Highmem VMs: 0\n",
      "  - Standard VMs: 8\n",
      "Total benchmarks exported: 13\n",
      "Total runtime allocated: 7.0 hours\n",
      "\n",
      "MACHINE SEPARATION POLICY:\n",
      "  - L-size benchmarks → c4-highmem-8 (high memory for large problems)\n",
      "  - S/M-size benchmarks → c4-standard-2 (cost-effective for smaller problems)\n",
      "\n",
      "NOTE: Only benchmarks with real HiGHS runtime data were included.\n",
      "      Runtime metadata added with '_runtime_s' and '_total_runtime_s' keys for cross-checking.\n"
     ]
    }
   ],
   "source": [
    "# Export the allocation to YAML files for infrastructure\n",
    "# NOTE: This exports ONLY benchmarks with real runtime data, separated by size category\n",
    "# ONLY exports VMs that have benchmarks assigned (skips empty VMs)\n",
    "output_dir = Path('infrastructure/benchmarks/runtime_optimized')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Clear existing files\n",
    "for file in output_dir.glob('*.yaml'):\n",
    "    file.unlink()\n",
    "\n",
    "exported_vms = 0\n",
    "total_benchmarks_exported = 0\n",
    "\n",
    "# Filter to only VMs with benchmarks\n",
    "active_l_vms = [vm for vm in optimal_l_vms if vm.benchmarks]\n",
    "active_sm_vms = [vm for vm in optimal_sm_vms if vm.benchmarks]\n",
    "\n",
    "print(f\"Exporting {len(active_l_vms)} highmem VMs and {len(active_sm_vms)} standard VMs (skipping empty VMs)\\n\")\n",
    "\n",
    "# Export L-size VMs (highmem machines)\n",
    "for vm_idx, vm in enumerate(active_l_vms):\n",
    "    # L-size benchmarks always get highmem machines\n",
    "    machine_type = 'c4-highmem-8'\n",
    "    years = [2025]  # Include highs-hipo for L benchmarks\n",
    "\n",
    "    # Create benchmark structure with runtime metadata\n",
    "    benchmarks_dict = {}\n",
    "    for benchmark in vm.benchmarks:\n",
    "        benchmark_name = benchmark['name']\n",
    "        if benchmark_name not in benchmarks_dict:\n",
    "            benchmarks_dict[benchmark_name] = {'Sizes': []}\n",
    "\n",
    "        size_entry = {\n",
    "            'Name': benchmark['size_name'],\n",
    "            'Size': benchmark['size_category'],\n",
    "            'URL': benchmark['url'],\n",
    "            '_runtime_s': round(benchmark['runtime'], 2)  # Add runtime for cross-checking\n",
    "        }\n",
    "        benchmarks_dict[benchmark_name]['Sizes'].append(size_entry)\n",
    "\n",
    "    # Create YAML content with total runtime metadata\n",
    "    yaml_content = {\n",
    "        'machine-type': machine_type,\n",
    "        'years': years,\n",
    "        '_total_runtime_s': round(vm.total_runtime, 2),  # Total runtime for this VM\n",
    "        '_total_runtime_h': round(vm.total_runtime / 3600, 2),  # In hours for readability\n",
    "        '_num_benchmarks': len(vm.benchmarks),\n",
    "        'benchmarks': benchmarks_dict\n",
    "    }\n",
    "\n",
    "    # Write to file\n",
    "    filename = f'highmem_vm_{vm_idx:02d}.yaml'\n",
    "    with open(output_dir / filename, 'w') as f:\n",
    "        yaml.safe_dump(yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "    print(f\"Exported {filename}: {len(vm.benchmarks)} L-size benchmarks, \"\n",
    "          f\"{vm.total_runtime/3600:.1f}h runtime\")\n",
    "\n",
    "    total_benchmarks_exported += len(vm.benchmarks)\n",
    "\n",
    "# Export S/M-size VMs (standard machines)\n",
    "for vm_idx, vm in enumerate(active_sm_vms):\n",
    "    # S/M-size benchmarks get standard machines\n",
    "    machine_type = 'c4-standard-2'\n",
    "    # years = [2020, 2022, 2023, 2024, 2025]\n",
    "    years = [2025]\n",
    "\n",
    "    # Create benchmark structure with runtime metadata\n",
    "    benchmarks_dict = {}\n",
    "    for benchmark in vm.benchmarks:\n",
    "        benchmark_name = benchmark['name']\n",
    "        if benchmark_name not in benchmarks_dict:\n",
    "            benchmarks_dict[benchmark_name] = {'Sizes': []}\n",
    "\n",
    "        size_entry = {\n",
    "            'Name': benchmark['size_name'],\n",
    "            'Size': benchmark['size_category'],\n",
    "            'URL': benchmark['url'],\n",
    "            '_runtime_s': round(benchmark['runtime'], 2)  # Add runtime for cross-checking\n",
    "        }\n",
    "        benchmarks_dict[benchmark_name]['Sizes'].append(size_entry)\n",
    "\n",
    "    # Create YAML content with total runtime metadata\n",
    "    yaml_content = {\n",
    "        'machine-type': machine_type,\n",
    "        'years': years,\n",
    "        '_total_runtime_s': round(vm.total_runtime, 2),  # Total runtime for this VM\n",
    "        '_total_runtime_h': round(vm.total_runtime / 3600, 2),  # In hours for readability\n",
    "        '_num_benchmarks': len(vm.benchmarks),\n",
    "        'benchmarks': benchmarks_dict\n",
    "    }\n",
    "\n",
    "    # Write to file\n",
    "    filename = f'highs_prelim_test_{vm_idx:02d}.yaml'\n",
    "    with open(output_dir / filename, 'w') as f:\n",
    "        yaml.safe_dump(yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "    print(f\"Exported {filename}: {len(vm.benchmarks)} S/M-size benchmarks, \"\n",
    "          f\"{vm.total_runtime/3600:.1f}h runtime\")\n",
    "\n",
    "    total_benchmarks_exported += len(vm.benchmarks)\n",
    "\n",
    "total_exported_vms = len(active_l_vms) + len(active_sm_vms)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Configuration files written to {output_dir}/\")\n",
    "print(f\"Total VMs exported: {total_exported_vms} (skipped {len(optimal_l_vms) + len(optimal_sm_vms) - total_exported_vms} empty VMs)\")\n",
    "print(f\"  - Highmem VMs: {len(active_l_vms)}\")\n",
    "print(f\"  - Standard VMs: {len(active_sm_vms)}\")\n",
    "print(f\"Total benchmarks exported: {total_benchmarks_exported}\")\n",
    "print(f\"Total runtime allocated: {sum(vm.total_runtime for vm in active_l_vms + active_sm_vms)/3600:.1f} hours\")\n",
    "print(f\"\\nMACHINE SEPARATION POLICY:\")\n",
    "print(f\"  - L-size benchmarks → c4-highmem-8 (high memory for large problems)\")\n",
    "print(f\"  - S/M-size benchmarks → c4-standard-2 (cost-effective for smaller problems)\")\n",
    "print(f\"\\nNOTE: Only benchmarks with real HiGHS runtime data were included.\")\n",
    "print(f\"      Runtime metadata added with '_runtime_s' and '_total_runtime_s' keys for cross-checking.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
