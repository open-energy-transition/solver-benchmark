{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime-Optimized VM Allocation for Benchmarks\n",
    "\n",
    "This notebook implements a bin packing algorithm to allocate benchmarks to VMs based on actual HiGHS v1.10 runtime data, aiming to minimize total runtime variance across VMs.\n",
    "\n",
    "## Important: HiGHS Variant Multiplier\n",
    "\n",
    "**We run 5 HiGHS solver variants for each benchmark:**\n",
    "1. `highs` (v1.10.0 standard)\n",
    "2. `highs-hipo-ipm` (v1.11.0 with IPM solver)\n",
    "3. `highs-hipo` (v1.11.0 with HiPO solver + 128 block size)\n",
    "4. `highs-hipo-32` (v1.11.0 with HiPO + 32 block size)\n",
    "5. `highs-hipo-64` (v1.11.0 with HiPO + 64 block size)\n",
    "\n",
    "Therefore, **actual VM runtime = base HiGHS runtime × 5**\n",
    "\n",
    "This multiplier is applied to all runtime calculations and VM allocations to accurately reflect the actual wall-clock time each VM will take to complete all benchmark variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "# CONFIGURATION\n",
    "# MAX_RUNTIME_PER_VM_SECONDS = 3600  # Set to a number (e.g., 3600 for 1 hour) to cap VM runtime, or None for no limit\n",
    "MAX_RUNTIME_PER_VM_SECONDS = None  # Set to a number (e.g., 3600 for 1 hour) to cap VM runtime, or None for no limit\n",
    "hipo_variants = ['highs-hipo-ipm', 'highs-hipo-128', 'highs-hipo-32', 'highs-hipo-64']\n",
    "NUM_HIGHS_VARIANTS = len(hipo_variants) + 1  # +1 for standard 'highs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Runtime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 successful HiGHS v1.10/hipo benchmark runs\n",
      "Solvers included: ['highs']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Benchmark</th>\n",
       "      <th>Size</th>\n",
       "      <th>Solver</th>\n",
       "      <th>Solver Version</th>\n",
       "      <th>Solver Release Year</th>\n",
       "      <th>Status</th>\n",
       "      <th>Termination Condition</th>\n",
       "      <th>Runtime (s)</th>\n",
       "      <th>Memory Usage (MB)</th>\n",
       "      <th>Objective Value</th>\n",
       "      <th>Max Integrality Violation</th>\n",
       "      <th>Duality Gap</th>\n",
       "      <th>Reported Runtime (s)</th>\n",
       "      <th>Timeout</th>\n",
       "      <th>Hostname</th>\n",
       "      <th>Run ID</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>genx-4_three_zones_w_policies_slack</td>\n",
       "      <td>3-1h</td>\n",
       "      <td>highs</td>\n",
       "      <td>1.10.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>TO</td>\n",
       "      <td>Timeout</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3812.844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>benchmark-instance-z-m37</td>\n",
       "      <td>20250429_090606_benchmark-instance-z-m37</td>\n",
       "      <td>2025-04-29 22:40:55.143181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>genx-6_three_zones_w_multistage-no_uc</td>\n",
       "      <td>3-1h</td>\n",
       "      <td>highs</td>\n",
       "      <td>1.10.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>ok</td>\n",
       "      <td>optimal</td>\n",
       "      <td>170.6828514480003</td>\n",
       "      <td>524.544</td>\n",
       "      <td>16995.001810747806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169.56626176834106</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>benchmark-instance-z-m37</td>\n",
       "      <td>20250429_090606_benchmark-instance-z-m37</td>\n",
       "      <td>2025-04-30 00:48:26.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>pypsa-eur-elec-trex</td>\n",
       "      <td>6-12h</td>\n",
       "      <td>highs</td>\n",
       "      <td>1.10.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>ok</td>\n",
       "      <td>optimal</td>\n",
       "      <td>226.40647792699747</td>\n",
       "      <td>692.944</td>\n",
       "      <td>7243765719.563413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224.6772541999817</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>benchmark-instance-z-m37</td>\n",
       "      <td>20250429_090606_benchmark-instance-z-m37</td>\n",
       "      <td>2025-04-30 01:05:00.905452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>times-etimeseu-france-elec+heat-co2-multi_stage</td>\n",
       "      <td>1-64ts</td>\n",
       "      <td>highs</td>\n",
       "      <td>1.10.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>ok</td>\n",
       "      <td>optimal</td>\n",
       "      <td>22.216355682998252</td>\n",
       "      <td>365.092</td>\n",
       "      <td>427842.1201974797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.157464027404785</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>benchmark-instance-z-m37</td>\n",
       "      <td>20250429_090606_benchmark-instance-z-m37</td>\n",
       "      <td>2025-04-30 01:13:25.552901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>DCOPF-Carolinas_uc_2M</td>\n",
       "      <td>1-997</td>\n",
       "      <td>highs</td>\n",
       "      <td>1.10.0</td>\n",
       "      <td>2025</td>\n",
       "      <td>ok</td>\n",
       "      <td>optimal</td>\n",
       "      <td>2080.905538397994</td>\n",
       "      <td>2878.12</td>\n",
       "      <td>4463695.700557045</td>\n",
       "      <td>3.502851030676985e-11</td>\n",
       "      <td>9.478984509261144e-05</td>\n",
       "      <td>2079.6019673347478</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>benchmark-instance-z2-m41</td>\n",
       "      <td>20250503_040156_benchmark-instance-z2-m41</td>\n",
       "      <td>2025-05-03 21:11:07.318952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Benchmark    Size Solver  \\\n",
       "45              genx-4_three_zones_w_policies_slack    3-1h  highs   \n",
       "47            genx-6_three_zones_w_multistage-no_uc    3-1h  highs   \n",
       "49                              pypsa-eur-elec-trex   6-12h  highs   \n",
       "51  times-etimeseu-france-elec+heat-co2-multi_stage  1-64ts  highs   \n",
       "86                            DCOPF-Carolinas_uc_2M   1-997  highs   \n",
       "\n",
       "   Solver Version Solver Release Year Status Termination Condition  \\\n",
       "45         1.10.0                2025     TO               Timeout   \n",
       "47         1.10.0                2025     ok               optimal   \n",
       "49         1.10.0                2025     ok               optimal   \n",
       "51         1.10.0                2025     ok               optimal   \n",
       "86         1.10.0                2025     ok               optimal   \n",
       "\n",
       "           Runtime (s) Memory Usage (MB)     Objective Value  \\\n",
       "45              3600.0          3812.844                 NaN   \n",
       "47   170.6828514480003           524.544  16995.001810747806   \n",
       "49  226.40647792699747           692.944   7243765719.563413   \n",
       "51  22.216355682998252           365.092   427842.1201974797   \n",
       "86   2080.905538397994           2878.12   4463695.700557045   \n",
       "\n",
       "   Max Integrality Violation            Duality Gap Reported Runtime (s)  \\\n",
       "45                       NaN                    NaN               3600.0   \n",
       "47                       NaN                    NaN   169.56626176834106   \n",
       "49                       NaN                    NaN    224.6772541999817   \n",
       "51                       NaN                    NaN   21.157464027404785   \n",
       "86     3.502851030676985e-11  9.478984509261144e-05   2079.6019673347478   \n",
       "\n",
       "   Timeout                   Hostname  \\\n",
       "45  3600.0   benchmark-instance-z-m37   \n",
       "47  3600.0   benchmark-instance-z-m37   \n",
       "49  3600.0   benchmark-instance-z-m37   \n",
       "51  3600.0   benchmark-instance-z-m37   \n",
       "86  3600.0  benchmark-instance-z2-m41   \n",
       "\n",
       "                                       Run ID                   Timestamp  \n",
       "45   20250429_090606_benchmark-instance-z-m37  2025-04-29 22:40:55.143181  \n",
       "47   20250429_090606_benchmark-instance-z-m37  2025-04-30 00:48:26.168300  \n",
       "49   20250429_090606_benchmark-instance-z-m37  2025-04-30 01:05:00.905452  \n",
       "51   20250429_090606_benchmark-instance-z-m37  2025-04-30 01:13:25.552901  \n",
       "86  20250503_040156_benchmark-instance-z2-m41  2025-05-03 21:11:07.318952  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load HiGHS runtime data (including HiGHS-hipo if available)\n",
    "highs_data = pd.read_csv('../main_results.csv', header=None, names=[\n",
    "    'Benchmark', 'Size', 'Solver', 'Solver Version', 'Solver Release Year',\n",
    "    'Status', 'Termination Condition', 'Runtime (s)', 'Memory Usage (MB)',\n",
    "    'Objective Value', 'Max Integrality Violation', 'Duality Gap',\n",
    "    'Reported Runtime (s)', 'Timeout', 'Hostname', 'Run ID', 'Timestamp'\n",
    "])\n",
    "\n",
    "# Filter for HiGHS 1.10.0 (2025) and HiGHS-hipo successful runs\n",
    "highs_v110 = highs_data[\n",
    "    ((highs_data['Solver Version'] == '1.10.0') &\n",
    "     (highs_data['Solver'] == 'highs'))\n",
    "]\n",
    "\n",
    "print(f\"Found {len(highs_v110)} successful HiGHS v1.10/hipo benchmark runs\")\n",
    "print(f\"Solvers included: {highs_v110['Solver'].unique()}\")\n",
    "highs_v110.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime data available for 120 benchmarks\n",
      "Base total runtime (single variant): 525601.61061178 seconds (146.0 hours)\n",
      "Actual total runtime (5 variants): 2628008.0530589 seconds (730.0 hours)\n",
      "Multiplier applied: 5x (running 5 HiGHS variants per benchmark)\n"
     ]
    }
   ],
   "source": [
    "# Create benchmark runtime mapping\n",
    "# IMPORTANT: We multiply runtime by NUM_HIGHS_VARIANTS since each benchmark\n",
    "# is run with 5 different HiGHS variants (highs, highs-hipo-128, highs-hipo-ipm, highs-hipo-32, highs-hipo-64)\n",
    "benchmark_runtimes = {}\n",
    "for _, row in highs_v110.iterrows():\n",
    "    benchmark_key = f\"{row['Benchmark']}-{row['Size']}\"\n",
    "    try:\n",
    "        base_runtime = float(row['Runtime (s)'])\n",
    "        # Multiply by number of variants since we run all variants for each benchmark\n",
    "        if pd.isna(base_runtime):\n",
    "            base_runtime = float(row['Timeout'])\n",
    "        actual_vm_runtime = base_runtime * NUM_HIGHS_VARIANTS\n",
    "        benchmark_runtimes[benchmark_key] = actual_vm_runtime\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {row}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"Runtime data available for {len(benchmark_runtimes)} benchmarks\")\n",
    "print(f\"Base total runtime (single variant): {sum(benchmark_runtimes.values())/NUM_HIGHS_VARIANTS} seconds ({sum(benchmark_runtimes.values())/NUM_HIGHS_VARIANTS/3600:.1f} hours)\")\n",
    "print(f\"Actual total runtime ({NUM_HIGHS_VARIANTS} variants): {sum(benchmark_runtimes.values())} seconds ({sum(benchmark_runtimes.values())/3600:.1f} hours)\")\n",
    "print(f\"Multiplier applied: {NUM_HIGHS_VARIANTS}x (running {NUM_HIGHS_VARIANTS} HiGHS variants per benchmark)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Benchmark Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total benchmark instances (from filtered dataset): 120\n",
      "  S: 18\n",
      "  M: 87\n",
      "  L: 15\n",
      "All instances have runtime data from highs_v110\n",
      "Runtime multiplier: 5x (to account for running all HiGHS variants)\n"
     ]
    }
   ],
   "source": [
    "# Load benchmark metadata to get size categories and URLs\n",
    "meta = yaml.safe_load(open('../results/metadata.yaml'))\n",
    "\n",
    "# Create a lookup for metadata\n",
    "metadata_lookup = {}\n",
    "for name, benchmark in meta['benchmarks'].items():\n",
    "    for size_info in benchmark['Sizes']:\n",
    "        instance_key = f\"{name}-{size_info['Name']}\"\n",
    "        metadata_lookup[instance_key] = size_info\n",
    "\n",
    "benchmarks_by_size = {'S': [], 'M': [], 'L': []}\n",
    "all_benchmark_instances = []\n",
    "\n",
    "for _, row in highs_v110.iterrows():\n",
    "    instance_key = f\"{row['Benchmark']}-{row['Size']}\"\n",
    "\n",
    "    # Get metadata for this instance\n",
    "    size_info = metadata_lookup.get(instance_key)\n",
    "    if size_info is None:\n",
    "        print(f\"Warning: No metadata found for {instance_key}\")\n",
    "        continue\n",
    "\n",
    "    # Apply variant multiplier to runtime\n",
    "    if pd.isna(row['Runtime (s)']):\n",
    "        base_runtime = float(row['Timeout'])\n",
    "    else:\n",
    "        base_runtime = float(row['Runtime (s)'])\n",
    "\n",
    "    actual_runtime: float = base_runtime * NUM_HIGHS_VARIANTS\n",
    "\n",
    "    instance = {\n",
    "        'name': row['Benchmark'],\n",
    "        'size_name': row['Size'],\n",
    "        'size_category': size_info['Size'],\n",
    "        'instance_key': instance_key,\n",
    "        'runtime': actual_runtime,  # Runtime multiplied by number of variants\n",
    "        'base_runtime': base_runtime,  # Store original single-variant runtime for reference\n",
    "        'num_variables': size_info.get('Num. variables', 0),\n",
    "        'num_constraints': size_info.get('Num. constraints', 0),\n",
    "        'url': size_info['URL']\n",
    "    }\n",
    "\n",
    "    benchmarks_by_size[size_info['Size']].append(instance)\n",
    "    all_benchmark_instances.append(instance)\n",
    "\n",
    "print(f\"Total benchmark instances (from filtered dataset): {len(all_benchmark_instances)}\")\n",
    "for size, instances in benchmarks_by_size.items():\n",
    "    print(f\"  {size}: {len(instances)}\")\n",
    "print(f\"All instances have runtime data from highs_v110\")\n",
    "print(f\"Runtime multiplier: {NUM_HIGHS_VARIANTS}x (to account for running all HiGHS variants)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin Packing Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMAllocation:\n",
    "    def __init__(self, vm_id: int):\n",
    "        self.vm_id = vm_id\n",
    "        self.benchmarks = []\n",
    "        self.total_runtime = 0.0\n",
    "\n",
    "    def add_benchmark(self, benchmark: dict):\n",
    "        \"\"\"Add benchmark with real runtime data only\"\"\"\n",
    "        if benchmark['runtime'] is None:\n",
    "            raise ValueError(f\"Benchmark {benchmark['instance_key']} has no runtime data!\")\n",
    "\n",
    "        self.benchmarks.append(benchmark)\n",
    "        self.total_runtime += benchmark['runtime']\n",
    "\n",
    "    def get_total_runtime(self):\n",
    "        return self.total_runtime\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        # For heap operations - compare by total runtime\n",
    "        return self.total_runtime < other.total_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_fit_decreasing(benchmarks: list[dict], num_vms: int) -> list[VMAllocation]:\n",
    "    \"\"\"\n",
    "    First Fit Decreasing bin packing algorithm.\n",
    "    Uses ONLY benchmarks with real runtime data.\n",
    "    \"\"\"\n",
    "    # Filter to only benchmarks with real runtime data\n",
    "    runtime_benchmarks = [b for b in benchmarks if b['runtime'] is not None]\n",
    "    print(f\"Using {len(runtime_benchmarks)} benchmarks with real runtime data (filtered from {len(benchmarks)} total)\")\n",
    "\n",
    "    # Create VMs\n",
    "    vms = [VMAllocation(i) for i in range(num_vms)]\n",
    "\n",
    "    # Sort benchmarks by runtime (descending)\n",
    "    sorted_benchmarks = sorted(runtime_benchmarks, key=lambda x: x['runtime'], reverse=True)\n",
    "\n",
    "    # Assign benchmarks to VMs\n",
    "    for benchmark in sorted_benchmarks:\n",
    "        # Find VM with minimum current runtime\n",
    "        min_vm = min(vms, key=lambda vm: vm.total_runtime)\n",
    "        min_vm.add_benchmark(benchmark)\n",
    "\n",
    "    return vms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_processing_time_first(benchmarks: list[dict], num_vms: int) -> list[VMAllocation]:\n",
    "    \"\"\"\n",
    "    Longest Processing Time First algorithm using a min-heap.\n",
    "    Uses ONLY benchmarks with real runtime data.\n",
    "    \"\"\"\n",
    "    # Filter to only benchmarks with real runtime data\n",
    "    runtime_benchmarks = [b for b in benchmarks if b['runtime'] is not None]\n",
    "    print(f\"Using {len(runtime_benchmarks)} benchmarks with real runtime data (filtered from {len(benchmarks)} total)\")\n",
    "\n",
    "    # Create VMs and initialize heap\n",
    "    vms = [VMAllocation(i) for i in range(num_vms)]\n",
    "    vm_heap = list(vms)  # Min-heap based on total runtime\n",
    "    heapq.heapify(vm_heap)\n",
    "\n",
    "    # Sort benchmarks by runtime (descending)\n",
    "    sorted_benchmarks = sorted(runtime_benchmarks, key=lambda x: x['runtime'], reverse=True)\n",
    "\n",
    "    # Assign benchmarks\n",
    "    for benchmark in sorted_benchmarks:\n",
    "        # Get VM with minimum load\n",
    "        min_vm = heapq.heappop(vm_heap)\n",
    "        min_vm.add_benchmark(benchmark)\n",
    "        # Re-insert VM into heap\n",
    "        heapq.heappush(vm_heap, min_vm)\n",
    "\n",
    "    return vms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_partition(benchmarks: list[dict], num_vms: int, max_runtime_per_vm: float = None) -> list[VMAllocation]:\n",
    "    \"\"\"\n",
    "    Balanced partition algorithm that tries to achieve equal total runtime per VM.\n",
    "    Uses ONLY benchmarks with real runtime data.\n",
    "\n",
    "    Args:\n",
    "        benchmarks: List of benchmark dictionaries with runtime data\n",
    "        num_vms: Initial number of VMs to create\n",
    "        max_runtime_per_vm: Maximum runtime allowed per VM (in seconds). If None, no limit.\n",
    "    \"\"\"\n",
    "    # Filter to only benchmarks with real runtime data\n",
    "    runtime_benchmarks = [b for b in benchmarks if b['runtime'] is not None]\n",
    "    print(f\"Using {len(runtime_benchmarks)} benchmarks with real runtime data (filtered from {len(benchmarks)} total)\")\n",
    "\n",
    "    # Calculate total runtime and target per VM\n",
    "    total_runtime = sum(b['runtime'] for b in runtime_benchmarks)\n",
    "    target_runtime_per_vm = total_runtime / num_vms\n",
    "\n",
    "    # If max_runtime_per_vm is set and target exceeds it, increase num_vms\n",
    "    if max_runtime_per_vm is not None and target_runtime_per_vm > max_runtime_per_vm:\n",
    "        original_num_vms = num_vms\n",
    "        num_vms = int(np.ceil(total_runtime / max_runtime_per_vm))\n",
    "        target_runtime_per_vm = total_runtime / num_vms\n",
    "        print(f\"⚠️  Target runtime {target_runtime_per_vm/3600:.1f}h exceeds max {max_runtime_per_vm/3600:.1f}h\")\n",
    "        print(f\"   Increasing VMs from {original_num_vms} to {num_vms} to respect runtime cap\")\n",
    "\n",
    "    print(f\"Target runtime per VM: {target_runtime_per_vm:.1f} seconds ({target_runtime_per_vm/3600:.1f} hours)\")\n",
    "    if max_runtime_per_vm is not None:\n",
    "        print(f\"Max runtime per VM: {max_runtime_per_vm:.1f} seconds ({max_runtime_per_vm/3600:.1f} hours)\")\n",
    "\n",
    "    # Create VMs\n",
    "    vms = [VMAllocation(i) for i in range(num_vms)]\n",
    "\n",
    "    # Sort benchmarks by runtime (descending)\n",
    "    sorted_benchmarks = sorted(runtime_benchmarks, key=lambda x: x['runtime'], reverse=True)\n",
    "\n",
    "    # Assign benchmarks with balance consideration\n",
    "    for benchmark in sorted_benchmarks:\n",
    "        benchmark_runtime = benchmark['runtime']\n",
    "\n",
    "        # Find VM that would be closest to target after adding this benchmark\n",
    "        best_vm = None\n",
    "        best_score = float('inf')\n",
    "\n",
    "        for vm in vms:\n",
    "            current_runtime = vm.total_runtime\n",
    "            after_runtime = current_runtime + benchmark_runtime\n",
    "\n",
    "            # Skip if this would exceed max runtime (only if max is set)\n",
    "            if max_runtime_per_vm is not None and after_runtime > max_runtime_per_vm:\n",
    "                # Check if any VM can still fit this benchmark\n",
    "                if current_runtime + benchmark_runtime <= max_runtime_per_vm * 1.05:  # Allow 5% overflow\n",
    "                    pass  # Continue to consider this VM\n",
    "                else:\n",
    "                    continue  # Skip this VM\n",
    "\n",
    "            # Score based on deviation from target\n",
    "            score = abs(after_runtime - target_runtime_per_vm)\n",
    "\n",
    "            # Prefer VMs that are under-loaded\n",
    "            if current_runtime < target_runtime_per_vm:\n",
    "                score *= 0.8  # Bonus for under-loaded VMs\n",
    "\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_vm = vm\n",
    "\n",
    "        # Add benchmark to best VM (or create new VM if needed)\n",
    "        if best_vm is None and max_runtime_per_vm is not None:\n",
    "            # All VMs are at capacity, create a new one\n",
    "            print(f\"⚠️  All VMs at capacity, creating additional VM for benchmark {benchmark['instance_key']}\")\n",
    "            best_vm = VMAllocation(len(vms))\n",
    "            vms.append(best_vm)\n",
    "\n",
    "        if best_vm is not None:\n",
    "            best_vm.add_benchmark(benchmark)\n",
    "        else:\n",
    "            print(f\"❌ Could not allocate benchmark {benchmark['instance_key']} (runtime: {benchmark_runtime:.1f}s)\")\n",
    "\n",
    "    return vms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_allocation(vms: list[VMAllocation], algorithm_name: str):\n",
    "    \"\"\"\n",
    "    Analyze and print statistics for a VM allocation.\n",
    "    \"\"\"\n",
    "    runtimes = [vm.total_runtime for vm in vms]\n",
    "\n",
    "    # Filter out empty VMs (should not happen with real runtime data only)\n",
    "    active_vms = [vm for vm in vms if vm.total_runtime > 0]\n",
    "    active_runtimes = [vm.total_runtime for vm in active_vms]\n",
    "\n",
    "    print(f\"\\n=== {algorithm_name} ===\")\n",
    "    print(f\"Total VMs created: {len(vms)}\")\n",
    "    print(f\"Active VMs (with benchmarks): {len(active_vms)}\")\n",
    "    print(f\"Empty VMs: {len(vms) - len(active_vms)}\")\n",
    "\n",
    "    if len(active_vms) > 0:\n",
    "        print(f\"Total runtime: {sum(active_runtimes):.1f} seconds ({sum(active_runtimes)/3600:.1f} hours)\")\n",
    "        print(f\"Average runtime per active VM: {np.mean(active_runtimes):.1f} seconds ({np.mean(active_runtimes)/3600:.1f} hours)\")\n",
    "        print(f\"Runtime standard deviation: {np.std(active_runtimes):.1f} seconds ({np.std(active_runtimes)/3600:.1f} hours)\")\n",
    "        print(f\"Min runtime: {min(active_runtimes):.1f} seconds ({min(active_runtimes)/3600:.1f} hours)\")\n",
    "        print(f\"Max runtime: {max(active_runtimes):.1f} seconds ({max(active_runtimes)/3600:.1f} hours)\")\n",
    "        print(f\"Runtime ratio (max/min): {max(active_runtimes)/min(active_runtimes):.2f}\")\n",
    "\n",
    "        # Efficiency (how balanced the allocation is)\n",
    "        efficiency = 1 - (np.std(active_runtimes) / np.mean(active_runtimes))\n",
    "        print(f\"Load balance efficiency: {efficiency:.3f} (1.0 = perfect balance)\")\n",
    "    else:\n",
    "        print(\"No active VMs found!\")\n",
    "        efficiency = 0\n",
    "\n",
    "    return {\n",
    "        'algorithm': algorithm_name,\n",
    "        'total_runtime': sum(active_runtimes) if active_vms else 0,\n",
    "        'std_runtime': np.std(active_runtimes) if active_vms else 0,\n",
    "        'max_runtime': max(active_runtimes) if active_vms else 0,\n",
    "        'min_runtime': min(active_runtimes) if active_vms else 0,\n",
    "        'efficiency': efficiency,\n",
    "        'runtimes': runtimes,\n",
    "        'active_vms': len(active_vms),\n",
    "        'num_vms': len(vms)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 120 benchmarks with real HiGHS runtime data\n",
      "Excluded 0 benchmarks without runtime data\n",
      "Base total runtime (single HiGHS variant): 146.0 hours\n",
      "Actual total runtime (5 HiGHS variants): 730.0 hours\n",
      "Runtime multiplier: 5x\n",
      "\n",
      "⚙️  No runtime cap configured (unlimited)\n",
      "\n",
      "Benchmark separation by size category:\n",
      "  L-size (highmem): 15 benchmarks, 507.9 hours (with 5 variants)\n",
      "  S/M-size (standard): 105 benchmarks, 222.1 hours (with 5 variants)\n",
      "\n",
      "==================================================\n",
      "TESTING L-SIZE BENCHMARKS (HIGHMEM MACHINES)\n",
      "==================================================\n",
      "\n",
      "Testing 2 highmem VMs for L-size benchmarks:\n",
      "Using 15 benchmarks with real runtime data (filtered from 15 total)\n",
      "Target runtime per VM: 914259.4 seconds (254.0 hours)\n",
      "\n",
      "=== L-size Balanced Partition (2 VMs) ===\n",
      "Total VMs created: 2\n",
      "Active VMs (with benchmarks): 2\n",
      "Empty VMs: 0\n",
      "Total runtime: 1828518.8 seconds (507.9 hours)\n",
      "Average runtime per active VM: 914259.4 seconds (254.0 hours)\n",
      "Runtime standard deviation: 525740.6 seconds (146.0 hours)\n",
      "Min runtime: 388518.8 seconds (107.9 hours)\n",
      "Max runtime: 1440000.0 seconds (400.0 hours)\n",
      "Runtime ratio (max/min): 3.71\n",
      "Load balance efficiency: 0.425 (1.0 = perfect balance)\n",
      "\n",
      "Testing 3 highmem VMs for L-size benchmarks:\n",
      "Using 15 benchmarks with real runtime data (filtered from 15 total)\n",
      "Target runtime per VM: 609506.3 seconds (169.3 hours)\n",
      "\n",
      "=== L-size Balanced Partition (3 VMs) ===\n",
      "Total VMs created: 3\n",
      "Active VMs (with benchmarks): 2\n",
      "Empty VMs: 1\n",
      "Total runtime: 1828518.8 seconds (507.9 hours)\n",
      "Average runtime per active VM: 914259.4 seconds (254.0 hours)\n",
      "Runtime standard deviation: 8123.0 seconds (2.3 hours)\n",
      "Min runtime: 906136.4 seconds (251.7 hours)\n",
      "Max runtime: 922382.4 seconds (256.2 hours)\n",
      "Runtime ratio (max/min): 1.02\n",
      "Load balance efficiency: 0.991 (1.0 = perfect balance)\n",
      "\n",
      "Testing 4 highmem VMs for L-size benchmarks:\n",
      "Using 15 benchmarks with real runtime data (filtered from 15 total)\n",
      "Target runtime per VM: 457129.7 seconds (127.0 hours)\n",
      "\n",
      "=== L-size Balanced Partition (4 VMs) ===\n",
      "Total VMs created: 4\n",
      "Active VMs (with benchmarks): 3\n",
      "Empty VMs: 1\n",
      "Total runtime: 1828518.8 seconds (507.9 hours)\n",
      "Average runtime per active VM: 609506.3 seconds (169.3 hours)\n",
      "Runtime standard deviation: 7829.3 seconds (2.2 hours)\n",
      "Min runtime: 602830.2 seconds (167.5 hours)\n",
      "Max runtime: 620494.1 seconds (172.4 hours)\n",
      "Runtime ratio (max/min): 1.03\n",
      "Load balance efficiency: 0.987 (1.0 = perfect balance)\n",
      "\n",
      "Testing 5 highmem VMs for L-size benchmarks:\n",
      "Using 15 benchmarks with real runtime data (filtered from 15 total)\n",
      "Target runtime per VM: 365703.8 seconds (101.6 hours)\n",
      "\n",
      "=== L-size Balanced Partition (5 VMs) ===\n",
      "Total VMs created: 5\n",
      "Active VMs (with benchmarks): 4\n",
      "Empty VMs: 1\n",
      "Total runtime: 1828518.8 seconds (507.9 hours)\n",
      "Average runtime per active VM: 457129.7 seconds (127.0 hours)\n",
      "Runtime standard deviation: 117864.1 seconds (32.7 hours)\n",
      "Min runtime: 255731.2 seconds (71.0 hours)\n",
      "Max runtime: 540000.0 seconds (150.0 hours)\n",
      "Runtime ratio (max/min): 2.11\n",
      "Load balance efficiency: 0.742 (1.0 = perfect balance)\n",
      "\n",
      "==================================================\n",
      "TESTING S/M-SIZE BENCHMARKS (STANDARD MACHINES)\n",
      "==================================================\n",
      "\n",
      "Testing 8 standard VMs for S/M-size benchmarks:\n",
      "Using 105 benchmarks with real runtime data (filtered from 105 total)\n",
      "Target runtime per VM: 99936.2 seconds (27.8 hours)\n",
      "\n",
      "=== S/M-size Balanced Partition (8 VMs) ===\n",
      "Total VMs created: 8\n",
      "Active VMs (with benchmarks): 5\n",
      "Empty VMs: 3\n",
      "Total runtime: 799489.3 seconds (222.1 hours)\n",
      "Average runtime per active VM: 159897.9 seconds (44.4 hours)\n",
      "Runtime standard deviation: 2864.0 seconds (0.8 hours)\n",
      "Min runtime: 154760.9 seconds (43.0 hours)\n",
      "Max runtime: 162000.0 seconds (45.0 hours)\n",
      "Runtime ratio (max/min): 1.05\n",
      "Load balance efficiency: 0.982 (1.0 = perfect balance)\n",
      "\n",
      "Testing 10 standard VMs for S/M-size benchmarks:\n",
      "Using 105 benchmarks with real runtime data (filtered from 105 total)\n",
      "Target runtime per VM: 79948.9 seconds (22.2 hours)\n",
      "\n",
      "=== S/M-size Balanced Partition (10 VMs) ===\n",
      "Total VMs created: 10\n",
      "Active VMs (with benchmarks): 6\n",
      "Empty VMs: 4\n",
      "Total runtime: 799489.3 seconds (222.1 hours)\n",
      "Average runtime per active VM: 133248.2 seconds (37.0 hours)\n",
      "Runtime standard deviation: 10.2 seconds (0.0 hours)\n",
      "Min runtime: 133238.4 seconds (37.0 hours)\n",
      "Max runtime: 133267.2 seconds (37.0 hours)\n",
      "Runtime ratio (max/min): 1.00\n",
      "Load balance efficiency: 1.000 (1.0 = perfect balance)\n",
      "\n",
      "Testing 12 standard VMs for S/M-size benchmarks:\n",
      "Using 105 benchmarks with real runtime data (filtered from 105 total)\n",
      "Target runtime per VM: 66624.1 seconds (18.5 hours)\n",
      "\n",
      "=== S/M-size Balanced Partition (12 VMs) ===\n",
      "Total VMs created: 12\n",
      "Active VMs (with benchmarks): 7\n",
      "Empty VMs: 5\n",
      "Total runtime: 799489.3 seconds (222.1 hours)\n",
      "Average runtime per active VM: 114212.8 seconds (31.7 hours)\n",
      "Runtime standard deviation: 6.4 seconds (0.0 hours)\n",
      "Min runtime: 114205.9 seconds (31.7 hours)\n",
      "Max runtime: 114222.8 seconds (31.7 hours)\n",
      "Runtime ratio (max/min): 1.00\n",
      "Load balance efficiency: 1.000 (1.0 = perfect balance)\n",
      "\n",
      "Testing 15 standard VMs for S/M-size benchmarks:\n",
      "Using 105 benchmarks with real runtime data (filtered from 105 total)\n",
      "Target runtime per VM: 53299.3 seconds (14.8 hours)\n",
      "\n",
      "=== S/M-size Balanced Partition (15 VMs) ===\n",
      "Total VMs created: 15\n",
      "Active VMs (with benchmarks): 10\n",
      "Empty VMs: 5\n",
      "Total runtime: 799489.3 seconds (222.1 hours)\n",
      "Average runtime per active VM: 79948.9 seconds (22.2 hours)\n",
      "Runtime standard deviation: 1074.9 seconds (0.3 hours)\n",
      "Min runtime: 79567.1 seconds (22.1 hours)\n",
      "Max runtime: 83173.4 seconds (23.1 hours)\n",
      "Runtime ratio (max/min): 1.05\n",
      "Load balance efficiency: 0.987 (1.0 = perfect balance)\n"
     ]
    }
   ],
   "source": [
    "# Use ONLY benchmarks that have real runtime data - no estimation!\n",
    "benchmarks_with_runtime = [b for b in all_benchmark_instances if b['runtime'] is not None]\n",
    "print(f\"Using {len(benchmarks_with_runtime)} benchmarks with real HiGHS runtime data\")\n",
    "print(f\"Excluded {len(all_benchmark_instances) - len(benchmarks_with_runtime)} benchmarks without runtime data\")\n",
    "print(f\"Base total runtime (single HiGHS variant): {sum(b['base_runtime'] for b in benchmarks_with_runtime)/3600:.1f} hours\")\n",
    "print(f\"Actual total runtime ({NUM_HIGHS_VARIANTS} HiGHS variants): {sum(b['runtime'] for b in benchmarks_with_runtime)/3600:.1f} hours\")\n",
    "print(f\"Runtime multiplier: {NUM_HIGHS_VARIANTS}x\")\n",
    "\n",
    "if MAX_RUNTIME_PER_VM_SECONDS is not None:\n",
    "    print(f\"\\n⚙️  Runtime cap enabled: {MAX_RUNTIME_PER_VM_SECONDS} seconds ({MAX_RUNTIME_PER_VM_SECONDS/3600:.1f} hours) per VM\")\n",
    "else:\n",
    "    print(f\"\\n⚙️  No runtime cap configured (unlimited)\")\n",
    "\n",
    "# Separate L-size benchmarks for highmem machines\n",
    "l_size_benchmarks = [b for b in benchmarks_with_runtime if b['size_category'] == 'L']\n",
    "non_l_benchmarks = [b for b in benchmarks_with_runtime if b['size_category'] != 'L']\n",
    "\n",
    "print(f\"\\nBenchmark separation by size category:\")\n",
    "print(f\"  L-size (highmem): {len(l_size_benchmarks)} benchmarks, {sum(b['runtime'] for b in l_size_benchmarks)/3600:.1f} hours (with {NUM_HIGHS_VARIANTS} variants)\")\n",
    "print(f\"  S/M-size (standard): {len(non_l_benchmarks)} benchmarks, {sum(b['runtime'] for b in non_l_benchmarks)/3600:.1f} hours (with {NUM_HIGHS_VARIANTS} variants)\")\n",
    "\n",
    "# Test different numbers of VMs for each category\n",
    "results = []\n",
    "\n",
    "# L-size benchmarks (fewer VMs since they need highmem)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TESTING L-SIZE BENCHMARKS (HIGHMEM MACHINES)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "l_vm_options = [2, 3, 4, 5] if len(l_size_benchmarks) > 0 else [1]\n",
    "for num_vms in l_vm_options:\n",
    "    if len(l_size_benchmarks) == 0:\n",
    "        print(\"No L-size benchmarks with runtime data\")\n",
    "        break\n",
    "\n",
    "    print(f\"\\nTesting {num_vms} highmem VMs for L-size benchmarks:\")\n",
    "\n",
    "    bp_vms = balanced_partition(l_size_benchmarks, num_vms, MAX_RUNTIME_PER_VM_SECONDS)\n",
    "    bp_result = analyze_allocation(bp_vms, f\"L-size Balanced Partition ({num_vms} VMs)\")\n",
    "    bp_result['num_vms'] = num_vms\n",
    "    bp_result['size_category'] = 'L'\n",
    "    results.append(bp_result)\n",
    "\n",
    "# S/M-size benchmarks (more VMs with standard machines)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TESTING S/M-SIZE BENCHMARKS (STANDARD MACHINES)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "sm_vm_options = [8, 10, 12, 15]\n",
    "for num_vms in sm_vm_options:\n",
    "    if len(non_l_benchmarks) == 0:\n",
    "        print(\"No S/M-size benchmarks with runtime data\")\n",
    "        break\n",
    "\n",
    "    print(f\"\\nTesting {num_vms} standard VMs for S/M-size benchmarks:\")\n",
    "\n",
    "    bp_vms = balanced_partition(non_l_benchmarks, num_vms, MAX_RUNTIME_PER_VM_SECONDS)\n",
    "    bp_result = analyze_allocation(bp_vms, f\"S/M-size Balanced Partition ({num_vms} VMs)\")\n",
    "    bp_result['num_vms'] = num_vms\n",
    "    bp_result['size_category'] = 'S/M'\n",
    "    results.append(bp_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "ALGORITHM COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\\nSize   VM Count  Algorithm                 Efficiency   Max Runtime (h) Std Dev (h) \n",
      "-------------------------------------------------------------------------------------\n",
      "L      2         L-size Balanced Partition 0.425         400.0             146.0\n",
      "L      3         L-size Balanced Partition 0.991         256.2             2.3\n",
      "L      4         L-size Balanced Partition 0.987         172.4             2.2\n",
      "L      5         L-size Balanced Partition 0.742         150.0             32.7\n",
      "S/M    8         S/M-size Balanced Partition 0.982         45.0             0.8\n",
      "S/M    10        S/M-size Balanced Partition 1.000         37.0             0.0\n",
      "S/M    12        S/M-size Balanced Partition 1.000         31.7             0.0\n",
      "S/M    15        S/M-size Balanced Partition 0.987         23.1             0.3\n",
      "\\n================================================================================\n",
      "BEST CONFIGURATIONS:\n",
      "================================================================================\n",
      "Best L-size (highmem): 3 VMs (efficiency: 0.991)\n",
      "Best S/M-size (standard): 12 VMs (efficiency: 1.000)\n",
      "\\nTotal deployment: 15 VMs (3 highmem + 12 standard)\n",
      "Average efficiency: 0.996\n"
     ]
    }
   ],
   "source": [
    "# Print summary comparison table\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ALGORITHM COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Separate results by size category\n",
    "l_results = df_results[df_results['size_category'] == 'L'] if 'size_category' in df_results.columns else pd.DataFrame()\n",
    "sm_results = df_results[df_results['size_category'] == 'S/M'] if 'size_category' in df_results.columns else df_results\n",
    "\n",
    "print(f\"\\\\n{'Size':<6} {'VM Count':<9} {'Algorithm':<25} {'Efficiency':<12} {'Max Runtime (h)':<15} {'Std Dev (h)':<12}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for _, row in df_results.iterrows():\n",
    "    size_cat = row.get('size_category', 'Mixed')\n",
    "    alg_name = row['algorithm'].split('(')[0].strip()\n",
    "    print(f\"{size_cat:<6} {row['num_vms']:<9} {alg_name:<25} \"\n",
    "          f\"{row['efficiency']:.3f}{'':8} {row['max_runtime']/3600:.1f}{'':12} \"\n",
    "          f\"{row['std_runtime']/3600:.1f}\")\n",
    "\n",
    "# Find best configurations for each size category\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(\"BEST CONFIGURATIONS:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if len(l_results) > 0:\n",
    "    best_l = l_results.loc[l_results['efficiency'].idxmax()]\n",
    "    print(f\"Best L-size (highmem): {best_l['num_vms']} VMs (efficiency: {best_l['efficiency']:.3f})\")\n",
    "\n",
    "if len(sm_results) > 0:\n",
    "    best_sm = sm_results.loc[sm_results['efficiency'].idxmax()]\n",
    "    print(f\"Best S/M-size (standard): {best_sm['num_vms']} VMs (efficiency: {best_sm['efficiency']:.3f})\")\n",
    "\n",
    "# Calculate total deployment\n",
    "if len(l_results) > 0 and len(sm_results) > 0:\n",
    "    total_vms = best_l['num_vms'] + best_sm['num_vms']\n",
    "    total_efficiency = (best_l['efficiency'] + best_sm['efficiency']) / 2\n",
    "    print(f\"\\\\nTotal deployment: {total_vms} VMs ({best_l['num_vms']} highmem + {best_sm['num_vms']} standard)\")\n",
    "    print(f\"Average efficiency: {total_efficiency:.3f}\")\n",
    "elif len(sm_results) > 0:\n",
    "    print(f\"\\\\nTotal deployment: {best_sm['num_vms']} standard VMs only\")\n",
    "    print(f\"Efficiency: {best_sm['efficiency']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Optimal Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating optimal allocations with size-based machine separation...\n",
      "\n",
      "L-size benchmarks: 3 highmem VMs\n",
      "Efficiency: 0.991\n",
      "Max VM runtime: 256.2 hours\n",
      "Using 15 benchmarks with real runtime data (filtered from 15 total)\n",
      "Target runtime per VM: 609506.3 seconds (169.3 hours)\n",
      "\n",
      "=== Final L-size Allocation - Highmem ===\n",
      "Total VMs created: 3\n",
      "Active VMs (with benchmarks): 2\n",
      "Empty VMs: 1\n",
      "Total runtime: 1828518.8 seconds (507.9 hours)\n",
      "Average runtime per active VM: 914259.4 seconds (254.0 hours)\n",
      "Runtime standard deviation: 8123.0 seconds (2.3 hours)\n",
      "Min runtime: 906136.4 seconds (251.7 hours)\n",
      "Max runtime: 922382.4 seconds (256.2 hours)\n",
      "Runtime ratio (max/min): 1.02\n",
      "Load balance efficiency: 0.991 (1.0 = perfect balance)\n",
      "\n",
      "S/M-size benchmarks: 12 standard VMs\n",
      "Efficiency: 1.000\n",
      "Max VM runtime: 31.7 hours\n",
      "Using 105 benchmarks with real runtime data (filtered from 105 total)\n",
      "Target runtime per VM: 66624.1 seconds (18.5 hours)\n",
      "\n",
      "=== Final S/M-size Allocation - Standard ===\n",
      "Total VMs created: 12\n",
      "Active VMs (with benchmarks): 7\n",
      "Empty VMs: 5\n",
      "Total runtime: 799489.3 seconds (222.1 hours)\n",
      "Average runtime per active VM: 114212.8 seconds (31.7 hours)\n",
      "Runtime standard deviation: 6.4 seconds (0.0 hours)\n",
      "Min runtime: 114205.9 seconds (31.7 hours)\n",
      "Max runtime: 114222.8 seconds (31.7 hours)\n",
      "Runtime ratio (max/min): 1.00\n",
      "Load balance efficiency: 1.000 (1.0 = perfect balance)\n",
      "\n",
      "============================================================\n",
      "FINAL ALLOCATION SUMMARY\n",
      "============================================================\n",
      "Total VMs: 15\n",
      "  - Highmem VMs (L-size): 3\n",
      "  - Standard VMs (S/M-size): 12\n",
      "Total allocated runtime: 730.0 hours\n",
      "Machine separation ensures optimal resource utilization\n"
     ]
    }
   ],
   "source": [
    "# Generate optimal allocations for both size categories\n",
    "print(f\"\\n\\nGenerating optimal allocations with size-based machine separation...\")\n",
    "\n",
    "if MAX_RUNTIME_PER_VM_SECONDS is not None:\n",
    "    print(f\"Runtime cap: {MAX_RUNTIME_PER_VM_SECONDS}s ({MAX_RUNTIME_PER_VM_SECONDS/3600:.1f}h) per VM\")\n",
    "\n",
    "optimal_l_vms = []\n",
    "optimal_sm_vms = []\n",
    "best_l_result = None\n",
    "best_sm_result = None\n",
    "\n",
    "# Generate L-size allocation (highmem machines)\n",
    "if len(l_results) > 0:\n",
    "    best_l_result = l_results.loc[l_results['efficiency'].idxmax()]\n",
    "    optimal_l_num_vms = best_l_result['num_vms']\n",
    "\n",
    "    print(f\"\\nL-size benchmarks: {optimal_l_num_vms} highmem VMs\")\n",
    "    print(f\"Efficiency: {best_l_result['efficiency']:.3f}\")\n",
    "    print(f\"Max VM runtime: {best_l_result['max_runtime']/3600:.1f} hours\")\n",
    "\n",
    "    optimal_l_vms = balanced_partition(l_size_benchmarks, optimal_l_num_vms, MAX_RUNTIME_PER_VM_SECONDS)\n",
    "    l_final_result = analyze_allocation(optimal_l_vms, f\"Final L-size Allocation - Highmem\")\n",
    "\n",
    "# Generate S/M-size allocation (standard machines)\n",
    "if len(sm_results) > 0:\n",
    "    best_sm_result = sm_results.loc[sm_results['efficiency'].idxmax()]\n",
    "    optimal_sm_num_vms = best_sm_result['num_vms']\n",
    "\n",
    "    print(f\"\\nS/M-size benchmarks: {optimal_sm_num_vms} standard VMs\")\n",
    "    print(f\"Efficiency: {best_sm_result['efficiency']:.3f}\")\n",
    "    print(f\"Max VM runtime: {best_sm_result['max_runtime']/3600:.1f} hours\")\n",
    "\n",
    "    optimal_sm_vms = balanced_partition(non_l_benchmarks, optimal_sm_num_vms, MAX_RUNTIME_PER_VM_SECONDS)\n",
    "    sm_final_result = analyze_allocation(optimal_sm_vms, f\"Final S/M-size Allocation - Standard\")\n",
    "\n",
    "# Combined summary\n",
    "total_vms = len(optimal_l_vms) + len(optimal_sm_vms)\n",
    "total_runtime = sum(vm.total_runtime for vm in optimal_l_vms + optimal_sm_vms)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL ALLOCATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total VMs: {total_vms}\")\n",
    "print(f\"  - Highmem VMs (L-size): {len(optimal_l_vms)}\")\n",
    "print(f\"  - Standard VMs (S/M-size): {len(optimal_sm_vms)}\")\n",
    "print(f\"Total allocated runtime: {total_runtime/3600:.1f} hours\")\n",
    "print(f\"Machine separation ensures optimal resource utilization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 2 highmem VMs and 7 standard VMs (skipping empty VMs)\n",
      "\n",
      "Exported highmem_vm_00.yaml: 6 L-size benchmarks, 256.2h runtime\n",
      "Exported highmem_vm_01.yaml: 9 L-size benchmarks, 251.7h runtime\n",
      "Exported standard_00.yaml: 15 S/M-size benchmarks, 31.7h runtime\n",
      "Exported standard_01.yaml: 15 S/M-size benchmarks, 31.7h runtime\n",
      "Exported standard_02.yaml: 15 S/M-size benchmarks, 31.7h runtime\n",
      "Exported standard_03.yaml: 14 S/M-size benchmarks, 31.7h runtime\n",
      "Exported standard_04.yaml: 18 S/M-size benchmarks, 31.7h runtime\n",
      "Exported standard_05.yaml: 14 S/M-size benchmarks, 31.7h runtime\n",
      "Exported standard_06.yaml: 14 S/M-size benchmarks, 31.7h runtime\n",
      "\n",
      "======================================================================\n",
      "Configuration files written to ../infrastructure/benchmarks/runtime_optimized/\n",
      "Total VMs exported: 9 (skipped 6 empty VMs)\n",
      "  - Highmem VMs: 2\n",
      "  - Standard VMs: 7\n",
      "Total benchmarks exported: 120\n",
      "Total runtime allocated: 730.0 hours\n",
      "\n",
      "MACHINE SEPARATION POLICY:\n",
      "  - L-size benchmarks → c4-highmem-8 (high memory for large problems)\n",
      "  - S/M-size benchmarks → c4-standard-2 (cost-effective for smaller problems)\n",
      "\n",
      "NOTE: Only benchmarks with real HiGHS runtime data were included.\n"
     ]
    }
   ],
   "source": [
    "# Export the allocation to YAML files for infrastructure\n",
    "# NOTE: This exports ONLY benchmarks with real runtime data, separated by size category\n",
    "# ONLY exports VMs that have benchmarks assigned (skips empty VMs)\n",
    "output_dir = Path('../infrastructure/benchmarks/runtime_optimized')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Clear existing files\n",
    "for file in output_dir.glob('*.yaml'):\n",
    "    file.unlink()\n",
    "\n",
    "exported_vms = 0\n",
    "total_benchmarks_exported = 0\n",
    "\n",
    "# Filter to only VMs with benchmarks\n",
    "active_l_vms = [vm for vm in optimal_l_vms if vm.benchmarks]\n",
    "active_sm_vms = [vm for vm in optimal_sm_vms if vm.benchmarks]\n",
    "\n",
    "print(f\"Exporting {len(active_l_vms)} highmem VMs and {len(active_sm_vms)} standard VMs (skipping empty VMs)\\n\")\n",
    "\n",
    "# Export L-size VMs (highmem machines)\n",
    "for vm_idx, vm in enumerate(active_l_vms):\n",
    "    # L-size benchmarks always get highmem machines\n",
    "    machine_type = 'c4-highmem-8'\n",
    "    years = [2025]  # Include highs-hipo for L benchmarks\n",
    "\n",
    "    # Create benchmark structure with runtime metadata\n",
    "    benchmarks_dict = {}\n",
    "    for benchmark in vm.benchmarks:\n",
    "        benchmark_name = benchmark['name']\n",
    "        if benchmark_name not in benchmarks_dict:\n",
    "            benchmarks_dict[benchmark_name] = {'Sizes': []}\n",
    "\n",
    "        size_entry = {\n",
    "            'Name': benchmark['size_name'],\n",
    "            'Size': benchmark['size_category'],\n",
    "            'URL': benchmark['url'],\n",
    "            '_runtime_s': round(benchmark['runtime'], 2)  # Add runtime for cross-checking\n",
    "        }\n",
    "        benchmarks_dict[benchmark_name]['Sizes'].append(size_entry)\n",
    "\n",
    "    # Create YAML content with total runtime metadata\n",
    "    yaml_content = {\n",
    "        'machine-type': machine_type,\n",
    "        'years': years,\n",
    "        '_total_runtime_s': round(vm.total_runtime, 2),  # Total runtime for this VM\n",
    "        '_total_runtime_h': round(vm.total_runtime / 3600, 2),  # In hours for readability\n",
    "        '_num_benchmarks': len(vm.benchmarks),\n",
    "        'benchmarks': benchmarks_dict\n",
    "    }\n",
    "\n",
    "    # Write to file\n",
    "    filename = f'highmem_vm_{vm_idx:02d}.yaml'\n",
    "    with open(output_dir / filename, 'w') as f:\n",
    "        yaml.safe_dump(yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "    print(f\"Exported {filename}: {len(vm.benchmarks)} L-size benchmarks, \"\n",
    "          f\"{vm.total_runtime/3600:.1f}h runtime\")\n",
    "\n",
    "    total_benchmarks_exported += len(vm.benchmarks)\n",
    "\n",
    "# Export S/M-size VMs (standard machines)\n",
    "for vm_idx, vm in enumerate(active_sm_vms):\n",
    "    # S/M-size benchmarks get standard machines\n",
    "    machine_type = 'c4-standard-2'\n",
    "    # years = [2020, 2022, 2023, 2024, 2025]\n",
    "    years = [2025]\n",
    "\n",
    "    # Create benchmark structure with runtime metadata\n",
    "    benchmarks_dict = {}\n",
    "    for benchmark in vm.benchmarks:\n",
    "        benchmark_name = benchmark['name']\n",
    "        if benchmark_name not in benchmarks_dict:\n",
    "            benchmarks_dict[benchmark_name] = {'Sizes': []}\n",
    "\n",
    "        size_entry = {\n",
    "            'Name': benchmark['size_name'],\n",
    "            'Size': benchmark['size_category'],\n",
    "            'URL': benchmark['url'],\n",
    "            '_runtime_s': round(benchmark['runtime'], 2)  # Add runtime for cross-checking\n",
    "        }\n",
    "        benchmarks_dict[benchmark_name]['Sizes'].append(size_entry)\n",
    "\n",
    "    # Create YAML content with total runtime metadata\n",
    "    yaml_content = {\n",
    "        'machine-type': machine_type,\n",
    "        'years': years,\n",
    "        '_total_runtime_s': round(vm.total_runtime, 2),  # Total runtime for this VM\n",
    "        '_total_runtime_h': round(vm.total_runtime / 3600, 2),  # In hours for readability\n",
    "        '_num_benchmarks': len(vm.benchmarks),\n",
    "        'benchmarks': benchmarks_dict\n",
    "    }\n",
    "\n",
    "    # Write to file\n",
    "    filename = f'standard_{vm_idx:02d}.yaml'\n",
    "    with open(output_dir / filename, 'w') as f:\n",
    "        yaml.safe_dump(yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "    print(f\"Exported {filename}: {len(vm.benchmarks)} S/M-size benchmarks, \"\n",
    "          f\"{vm.total_runtime/3600:.1f}h runtime\")\n",
    "\n",
    "    total_benchmarks_exported += len(vm.benchmarks)\n",
    "\n",
    "total_exported_vms = len(active_l_vms) + len(active_sm_vms)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Configuration files written to {output_dir}/\")\n",
    "print(f\"Total VMs exported: {total_exported_vms} (skipped {len(optimal_l_vms) + len(optimal_sm_vms) - total_exported_vms} empty VMs)\")\n",
    "print(f\"  - Highmem VMs: {len(active_l_vms)}\")\n",
    "print(f\"  - Standard VMs: {len(active_sm_vms)}\")\n",
    "print(f\"Total benchmarks exported: {total_benchmarks_exported}\")\n",
    "print(f\"Total runtime allocated: {sum(vm.total_runtime for vm in active_l_vms + active_sm_vms)/3600:.1f} hours\")\n",
    "print(f\"\\nMACHINE SEPARATION POLICY:\")\n",
    "print(f\"  - L-size benchmarks → c4-highmem-8 (high memory for large problems)\")\n",
    "print(f\"  - S/M-size benchmarks → c4-standard-2 (cost-effective for smaller problems)\")\n",
    "print(f\"\\nNOTE: Only benchmarks with real HiGHS runtime data were included.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
