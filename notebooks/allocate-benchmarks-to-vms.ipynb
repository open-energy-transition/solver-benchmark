{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime-Optimized VM Allocation for Benchmarks\n",
    "\n",
    "This notebook implements the balanced partition bin packing algorithm to allocate benchmarks to VMs based on actual solver runtime data from `results/benchmark_results.csv`, aiming to minimize total runtime variance across VMs.\n",
    "\n",
    "## Key Points\n",
    "\n",
    "- **Solver Coverage**: Uses actual runtime data for all solvers (glpk, highs, scip, cbc, gurobi) from benchmark results\n",
    "- **Algorithm**: Balanced partition algorithm for optimal load distribution\n",
    "- **Machine Separation**: L-size benchmarks on highmem VMs, S/M-size on standard VMs\n",
    "\n",
    "## Workflow\n",
    "1. Configure how long you want the benchmarks to run using `MAX_RUNTIME_PER_VM_SECONDS`, smaller values will result in more VMs being used. You can also set `MAX_NUM_VMS` to limit the number of VMs used. Allocation will error if the configuration is unfeasible.\n",
    "2. Filter out the benchmark results you want to re-run from the previous run in `results/benchmark_results.csv`.\n",
    "3. Run the playbook to generate allocation config in infrastructure/benchmarks/<run_id> directory.\n",
    "4. Launch the benchmarks using (Opentofu)[https://opentofu.org/]. Read `infrastructure/README.md` for details of the infrastructure orchestration setup.\n",
    "5. Monitor ongoing runs using `notebooks/run-and-observe-benchmarks.ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configure Benchmark Campaign config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Max runtime per VM: 48.0 hours (2.0 days)\n",
      "  Max number of VMs: 40\n",
      "  Year filter: 2025\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# CONFIGURATION\n",
    "# None for unlimited\n",
    "MAX_RUNTIME_PER_VM_SECONDS = 2 * 24 * 3600  # Max runtime per individual VM (hard constraint)\n",
    "MAX_NUM_VMS = 40  # Max number of VMs to create (hard constraint). If None, unlimited.\n",
    "\n",
    "YEAR_FILTER = 2025  # Only use benchmark results from this year\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(\n",
    "    f\"  Max runtime per VM: {MAX_RUNTIME_PER_VM_SECONDS / 3600:.1f} hours ({MAX_RUNTIME_PER_VM_SECONDS / (24 * 3600):.1f} days)\"\n",
    "    if MAX_RUNTIME_PER_VM_SECONDS\n",
    "    else \"  Max runtime per VM: None (unlimited)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Max number of VMs: {MAX_NUM_VMS}\"\n",
    "    if MAX_NUM_VMS is not None\n",
    "    else \"  Max number of VMs: None (unlimited)\"\n",
    ")\n",
    "print(f\"  Year filter: {YEAR_FILTER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and Process Run-time Data from Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total benchmark runs loaded: 240\n",
      "Solvers included: ['highs', 'scip']\n",
      "Total combined runtime: 1030947 seconds (286.4 hours)\n"
     ]
    }
   ],
   "source": [
    "# Load benchmark results for all solvers\n",
    "benchmark_data = pd.read_csv(\"../results/benchmark_results.csv\")\n",
    "\n",
    "# Convert Runtime column to numeric\n",
    "benchmark_data[\"Runtime (s)\"] = pd.to_numeric(benchmark_data[\"Runtime (s)\"], errors='coerce')\n",
    "\n",
    "# Filter to only runs from the configured year\n",
    "benchmark_data = benchmark_data[benchmark_data['Solver Release Year'] == YEAR_FILTER]\n",
    "\n",
    "print(f\"Total benchmark runs loaded: {len(benchmark_data)}\")\n",
    "print(f\"Solvers included: {sorted(benchmark_data['Solver'].unique().tolist())}\")\n",
    "print(f\"Total combined runtime: {benchmark_data['Runtime (s)'].sum():.0f} seconds ({benchmark_data['Runtime (s)'].sum() / 3600:.1f} hours)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Load Benchmark Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total benchmark-size combinations: 120\n",
      "  S: 18\n",
      "  M: 87\n",
      "  L: 15\n",
      "\n",
      "Total runtime (all solvers combined): 328.4 hours\n",
      "\n",
      "Sample benchmark-size with all solvers:\n",
      "  genx-4_three_zones_w_policies_slack-3-1h: solvers=['highs', 'scip'], total_runtime=7200.0s\n",
      "    - highs: 3600.0s ✗(TO)\n",
      "    - scip: 3600.0s ✗(TO)\n"
     ]
    }
   ],
   "source": [
    "# Load benchmark metadata to get size categories and URLs\n",
    "meta = yaml.safe_load(open(\"../results/metadata.yaml\"))\n",
    "\n",
    "# Create a lookup for metadata\n",
    "metadata_lookup = {}\n",
    "for name, benchmark in meta[\"benchmarks\"].items():\n",
    "    for size_info in benchmark[\"Sizes\"]:\n",
    "        instance_key = f\"{name}-{size_info['Name']}\"\n",
    "        metadata_lookup[instance_key] = size_info\n",
    "\n",
    "benchmarks_by_size = {\"S\": [], \"M\": [], \"L\": []}\n",
    "all_benchmark_instances = []\n",
    "\n",
    "# Group by (Benchmark, Size) and sum runtimes acyaml\" is not defineyaml\" is not defineross all solvers\n",
    "grouped_data = {}\n",
    "for _, row in benchmark_data.iterrows():\n",
    "    benchmark_name = row[\"Benchmark\"]\n",
    "    size_name = row[\"Size\"]\n",
    "    solver_name = row[\"Solver\"]\n",
    "    instance_key = f\"{benchmark_name}-{size_name}\"\n",
    "\n",
    "    # Get metadata for this benchmark\n",
    "    size_info = metadata_lookup.get(instance_key)\n",
    "    if size_info is None:\n",
    "        continue\n",
    "\n",
    "    # Get runtime: use actual runtime if available, otherwise use Timeout value\n",
    "    runtime = row[\"Runtime (s)\"]\n",
    "    if pd.isna(runtime):\n",
    "        # For failed/timed out runs, use the Timeout value as estimate\n",
    "        runtime = row[\"Timeout\"]\n",
    "\n",
    "    # Skip if still NaN\n",
    "    if pd.isna(runtime):\n",
    "        continue\n",
    "\n",
    "    runtime = float(runtime)\n",
    "\n",
    "    # Group by benchmark-size combination\n",
    "    if instance_key not in grouped_data:\n",
    "        grouped_data[instance_key] = {\n",
    "            \"name\": benchmark_name,\n",
    "            \"size_name\": size_name,\n",
    "            \"size_category\": size_info[\"Size\"],\n",
    "            \"instance_key\": instance_key,\n",
    "            \"solvers\": [],\n",
    "            \"solver_runtimes\": {},\n",
    "            \"solver_status\": {},  # Track success/failure status\n",
    "            \"runtime\": 0.0,  # Sum of all solver runtimes for this benchmark-size\n",
    "            \"num_variables\": size_info.get(\"Num. variables\", 0),\n",
    "            \"num_constraints\": size_info.get(\"Num. constraints\", 0),\n",
    "            \"url\": size_info[\"URL\"],\n",
    "        }\n",
    "\n",
    "    # Add this solver's runtime and status\n",
    "    if solver_name not in grouped_data[instance_key][\"solvers\"]:\n",
    "        grouped_data[instance_key][\"solvers\"].append(solver_name)\n",
    "    grouped_data[instance_key][\"solver_runtimes\"][solver_name] = runtime\n",
    "    grouped_data[instance_key][\"solver_status\"][solver_name] = row[\"Status\"]\n",
    "    grouped_data[instance_key][\"runtime\"] += runtime\n",
    "\n",
    "# Convert grouped data to list and categorize by size\n",
    "for instance_key, instance_data in grouped_data.items():\n",
    "    all_benchmark_instances.append(instance_data)\n",
    "    size_cat = instance_data[\"size_category\"]\n",
    "    benchmarks_by_size[size_cat].append(instance_data)\n",
    "\n",
    "print(f\"Total benchmark-size combinations: {len(all_benchmark_instances)}\")\n",
    "for size, instances in benchmarks_by_size.items():\n",
    "    print(f\"  {size}: {len(instances)}\")\n",
    "print(f\"\\nTotal runtime (all solvers combined): {sum(i['runtime'] for i in all_benchmark_instances) / 3600:.1f} hours\")\n",
    "print(f\"\\nSample benchmark-size with all solvers:\")\n",
    "sample = all_benchmark_instances[0]\n",
    "print(f\"  {sample['instance_key']}: solvers={sorted(sample['solvers'])}, total_runtime={sample['runtime']:.1f}s\")\n",
    "for solver in sorted(sample['solvers']):\n",
    "    status = sample['solver_status'][solver]\n",
    "    status_str = \"✓\" if status == \"ok\" else f\"✗({status})\"\n",
    "    print(f\"    - {solver}: {sample['solver_runtimes'][solver]:.1f}s {status_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Allocate Benchmarks to VMs\n",
    "\n",
    "Use Balanced Partitioning to bin-pack Benchmarks on VMs by Size Label (S,M.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMAllocation:\n",
    "    def __init__(self, vm_id: int):\n",
    "        self.vm_id = vm_id\n",
    "        self.benchmarks = []\n",
    "        self.total_runtime = 0.0\n",
    "\n",
    "    def add_benchmark(self, benchmark: dict):\n",
    "        \"\"\"Add benchmark with real runtime data only\"\"\"\n",
    "        if benchmark[\"runtime\"] is None:\n",
    "            raise ValueError(\n",
    "                f\"Benchmark {benchmark['instance_key']} has no runtime data!\"\n",
    "            )\n",
    "\n",
    "        self.benchmarks.append(benchmark)\n",
    "        self.total_runtime += benchmark[\"runtime\"]\n",
    "\n",
    "    def get_total_runtime(self):\n",
    "        return self.total_runtime\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        # For heap operations - compare by total runtime\n",
    "        return self.total_runtime < other.total_runtime\n",
    "\n",
    "def balanced_partition(\n",
    "    benchmarks: list[dict], num_vms: int, max_runtime_per_vm: float = None, max_num_vms: int = None\n",
    ") -> list[VMAllocation]:\n",
    "    \"\"\"\n",
    "    Balanced partition algorithm that tries to achieve equal total runtime per VM.\n",
    "    Uses ONLY benchmarks with real runtime data.\n",
    "\n",
    "    FAIL-FAST: Raises ValueError if constraints cannot be satisfied.\n",
    "\n",
    "    Args:\n",
    "        benchmarks: List of benchmark dictionaries with runtime data\n",
    "        num_vms: Initial number of VMs to create\n",
    "        max_runtime_per_vm: Maximum runtime allowed per VM (in seconds). If None, no limit.\n",
    "        max_num_vms: Maximum number of VMs allowed (hard constraint). If None, unlimited.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any constraint cannot be satisfied (fail-fast)\n",
    "    \"\"\"\n",
    "    # Filter to only benchmarks with real runtime data\n",
    "    runtime_benchmarks = [\n",
    "        b for b in benchmarks if b[\"runtime\"] is not None and not pd.isna(b[\"runtime\"])\n",
    "    ]\n",
    "    print(\n",
    "        f\"Using {len(runtime_benchmarks)} benchmarks with real runtime data (filtered from {len(benchmarks)} total)\"\n",
    "    )\n",
    "\n",
    "    if len(runtime_benchmarks) == 0:\n",
    "        return []\n",
    "\n",
    "    # CONSTRAINT CHECK 1: No single benchmark exceeds max_runtime_per_vm\n",
    "    if max_runtime_per_vm is not None:\n",
    "        oversized = [b for b in runtime_benchmarks if b[\"runtime\"] > max_runtime_per_vm]\n",
    "        if oversized:\n",
    "            benchmark_list = \"\\n    \".join(\n",
    "                f\"- {b['instance_key']}: {b['runtime'] / 3600:.1f}h\"\n",
    "                for b in oversized[:5]\n",
    "            )\n",
    "            if len(oversized) > 5:\n",
    "                benchmark_list += f\"\\n    ... and {len(oversized) - 5} more\"\n",
    "            raise ValueError(\n",
    "                f\"❌ CONSTRAINT VIOLATION: {len(oversized)} benchmark(s) exceed max runtime per VM ({max_runtime_per_vm / 3600:.1f}h):\\n    {benchmark_list}\\n\"\n",
    "                f\"   Solution: Increase MAX_RUNTIME_PER_VM_SECONDS or filter/exclude benchmarks\"\n",
    "            )\n",
    "\n",
    "    # Calculate total runtime and determine minimum VMs needed\n",
    "    total_runtime = sum(b[\"runtime\"] for b in runtime_benchmarks)\n",
    "\n",
    "    # CONSTRAINT CHECK 2: Minimum VMs needed for runtime cap\n",
    "    if max_runtime_per_vm is not None:\n",
    "        min_vms_needed = int(np.ceil(total_runtime / max_runtime_per_vm))\n",
    "        if min_vms_needed > num_vms:\n",
    "            print(\n",
    "                f\"⚠️  Initial {num_vms} VMs insufficient for runtime cap. Increasing to {min_vms_needed} VMs\"\n",
    "            )\n",
    "            num_vms = min_vms_needed\n",
    "\n",
    "    # CONSTRAINT CHECK 3: Requested VMs doesn't exceed max\n",
    "    if max_num_vms is not None and num_vms > max_num_vms:\n",
    "        raise ValueError(\n",
    "            f\"❌ CONSTRAINT VIOLATION: Cannot allocate {len(runtime_benchmarks)} benchmarks \"\n",
    "            f\"within {max_num_vms} VMs (would need at least {num_vms} VMs)\\n\"\n",
    "            f\"   Solution: Increase MAX_NUM_VMS or increase MAX_RUNTIME_PER_VM_SECONDS\"\n",
    "        )\n",
    "\n",
    "    target_runtime_per_vm = total_runtime / num_vms\n",
    "\n",
    "    print(\n",
    "        f\"Target runtime per VM: {target_runtime_per_vm:.1f} seconds ({target_runtime_per_vm / 3600:.1f} hours)\"\n",
    "    )\n",
    "    if max_runtime_per_vm is not None:\n",
    "        print(\n",
    "            f\"Max runtime per VM (hard cap): {max_runtime_per_vm:.1f} seconds ({max_runtime_per_vm / 3600:.1f} hours)\"\n",
    "        )\n",
    "    if max_num_vms is not None:\n",
    "        print(f\"Max VMs allowed: {max_num_vms}\")\n",
    "\n",
    "    # Create initial VMs\n",
    "    vms = [VMAllocation(i) for i in range(num_vms)]\n",
    "\n",
    "    # Sort benchmarks by runtime (descending) - largest first for better bin packing\n",
    "    sorted_benchmarks = sorted(\n",
    "        runtime_benchmarks, key=lambda x: x[\"runtime\"], reverse=True\n",
    "    )\n",
    "\n",
    "    # Assign benchmarks with balance consideration\n",
    "    for benchmark in sorted_benchmarks:\n",
    "        benchmark_runtime = benchmark[\"runtime\"]\n",
    "\n",
    "        # Find VM that would be closest to target after adding this benchmark\n",
    "        best_vm = None\n",
    "        best_score = float(\"inf\")\n",
    "\n",
    "        for vm in vms:\n",
    "            current_runtime = vm.total_runtime\n",
    "            after_runtime = current_runtime + benchmark_runtime\n",
    "\n",
    "            # HARD CAP: Skip if this would exceed max runtime\n",
    "            if max_runtime_per_vm is not None and after_runtime > max_runtime_per_vm:\n",
    "                continue  # This VM cannot take this benchmark\n",
    "\n",
    "            # Score based on deviation from target\n",
    "            score = abs(after_runtime - target_runtime_per_vm)\n",
    "\n",
    "            # Prefer VMs that are under-loaded\n",
    "            if current_runtime < target_runtime_per_vm:\n",
    "                score *= 0.8  # Bonus for under-loaded VMs\n",
    "\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_vm = vm\n",
    "\n",
    "        # If no VM can take this benchmark, FAIL FAST\n",
    "        if best_vm is None:\n",
    "            raise ValueError(\n",
    "                f\"❌ CONSTRAINT VIOLATION: Cannot allocate benchmark {benchmark['instance_key']} ({benchmark_runtime / 3600:.1f}h)\\n\"\n",
    "                f\"   - No VM has enough capacity (all at or near max {max_runtime_per_vm / 3600 if max_runtime_per_vm else 'unlimited':.1f}h)\\n\"\n",
    "                f\"   - Cannot create additional VM (would exceed MAX_NUM_VMS={max_num_vms})\\n\"\n",
    "                f\"   Solution: Increase MAX_RUNTIME_PER_VM_SECONDS or MAX_NUM_VMS\"\n",
    "            )\n",
    "\n",
    "        best_vm.add_benchmark(benchmark)\n",
    "\n",
    "    return vms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_allocation(vms: list[VMAllocation], algorithm_name: str):\n",
    "    \"\"\"\n",
    "    Analyze and print statistics for a VM allocation.\n",
    "    \"\"\"\n",
    "    runtimes = [vm.total_runtime for vm in vms]\n",
    "\n",
    "    # Filter out empty VMs (should not happen with real runtime data only)\n",
    "    active_vms = [vm for vm in vms if vm.total_runtime > 0]\n",
    "    active_runtimes = [vm.total_runtime for vm in active_vms]\n",
    "\n",
    "    print(f\"\\n=== {algorithm_name} ===\")\n",
    "    print(f\"Total VMs created: {len(vms)}\")\n",
    "    print(f\"Active VMs (with benchmarks): {len(active_vms)}\")\n",
    "    print(f\"Empty VMs: {len(vms) - len(active_vms)}\")\n",
    "\n",
    "    if len(active_vms) > 0:\n",
    "        print(\n",
    "            f\"Total runtime: {sum(active_runtimes):.1f} seconds ({sum(active_runtimes) / 3600:.1f} hours)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Average runtime per active VM: {np.mean(active_runtimes):.1f} seconds ({np.mean(active_runtimes) / 3600:.1f} hours)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Runtime standard deviation: {np.std(active_runtimes):.1f} seconds ({np.std(active_runtimes) / 3600:.1f} hours)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Min runtime: {min(active_runtimes):.1f} seconds ({min(active_runtimes) / 3600:.1f} hours)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Max runtime: {max(active_runtimes):.1f} seconds ({max(active_runtimes) / 3600:.1f} hours)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Runtime ratio (max/min): {max(active_runtimes) / min(active_runtimes):.2f}\"\n",
    "        )\n",
    "\n",
    "        # Efficiency (how balanced the allocation is)\n",
    "        efficiency = 1 - (np.std(active_runtimes) / np.mean(active_runtimes))\n",
    "        print(f\"Load balance efficiency: {efficiency:.3f} (1.0 = perfect balance)\")\n",
    "    else:\n",
    "        print(\"No active VMs found!\")\n",
    "        efficiency = 0\n",
    "\n",
    "    return {\n",
    "        \"algorithm\": algorithm_name,\n",
    "        \"total_runtime\": sum(active_runtimes) if active_vms else 0,\n",
    "        \"std_runtime\": np.std(active_runtimes) if active_vms else 0,\n",
    "        \"max_runtime\": max(active_runtimes) if active_vms else 0,\n",
    "        \"min_runtime\": min(active_runtimes) if active_vms else 0,\n",
    "        \"efficiency\": efficiency,\n",
    "        \"runtimes\": runtimes,\n",
    "        \"active_vms\": len(active_vms),\n",
    "        \"num_vms\": len(vms),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 120 benchmark-solver instances with runtime data\n",
      "Total combined runtime: 328.4 hours\n",
      "\n",
      "⚙️  Runtime cap enabled: 172800 seconds (48.0 hours) per VM\n",
      "⚙️  VM limit enabled: max 40 VMs\n",
      "\n",
      "Benchmark separation by size category:\n",
      "  L-size (highmem): 15 instances, 243.2 hours\n",
      "  S/M-size (standard): 105 instances, 85.2 hours\n",
      "\n",
      "==================================================\n",
      "TESTING L-SIZE BENCHMARKS (HIGHMEM MACHINES)\n",
      "==================================================\n",
      "Calculated VM options for L-size: [4, 5, 6, 7, 8]\n",
      "\n",
      "Testing 4 highmem VMs for L-size benchmarks:\n",
      "Using 15 benchmarks with real runtime data (filtered from 15 total)\n",
      "⚠️  Initial 4 VMs insufficient for runtime cap. Increasing to 6 VMs\n",
      "Target runtime per VM: 145904.2 seconds (40.5 hours)\n",
      "Max runtime per VM (hard cap): 172800.0 seconds (48.0 hours)\n",
      "Max VMs allowed: 40\n",
      "\n",
      "=== L-size Balanced Partition (4 VMs) ===\n",
      "Total VMs created: 6\n",
      "Active VMs (with benchmarks): 6\n",
      "Empty VMs: 0\n",
      "Total runtime: 875425.0 seconds (243.2 hours)\n",
      "Average runtime per active VM: 145904.2 seconds (40.5 hours)\n",
      "Runtime standard deviation: 11266.6 seconds (3.1 hours)\n",
      "Min runtime: 130763.8 seconds (36.3 hours)\n",
      "Max runtime: 168661.2 seconds (46.9 hours)\n",
      "Runtime ratio (max/min): 1.29\n",
      "Load balance efficiency: 0.923 (1.0 = perfect balance)\n",
      "\n",
      "Testing 5 highmem VMs for L-size benchmarks:\n",
      "Using 15 benchmarks with real runtime data (filtered from 15 total)\n",
      "⚠️  Initial 5 VMs insufficient for runtime cap. Increasing to 6 VMs\n",
      "Target runtime per VM: 145904.2 seconds (40.5 hours)\n",
      "Max runtime per VM (hard cap): 172800.0 seconds (48.0 hours)\n",
      "Max VMs allowed: 40\n",
      "\n",
      "=== L-size Balanced Partition (5 VMs) ===\n",
      "Total VMs created: 6\n",
      "Active VMs (with benchmarks): 6\n",
      "Empty VMs: 0\n",
      "Total runtime: 875425.0 seconds (243.2 hours)\n",
      "Average runtime per active VM: 145904.2 seconds (40.5 hours)\n",
      "Runtime standard deviation: 11266.6 seconds (3.1 hours)\n",
      "Min runtime: 130763.8 seconds (36.3 hours)\n",
      "Max runtime: 168661.2 seconds (46.9 hours)\n",
      "Runtime ratio (max/min): 1.29\n",
      "Load balance efficiency: 0.923 (1.0 = perfect balance)\n",
      "\n",
      "Testing 6 highmem VMs for L-size benchmarks:\n",
      "Using 15 benchmarks with real runtime data (filtered from 15 total)\n",
      "Target runtime per VM: 145904.2 seconds (40.5 hours)\n",
      "Max runtime per VM (hard cap): 172800.0 seconds (48.0 hours)\n",
      "Max VMs allowed: 40\n",
      "\n",
      "=== L-size Balanced Partition (6 VMs) ===\n",
      "Total VMs created: 6\n",
      "Active VMs (with benchmarks): 6\n",
      "Empty VMs: 0\n",
      "Total runtime: 875425.0 seconds (243.2 hours)\n",
      "Average runtime per active VM: 145904.2 seconds (40.5 hours)\n",
      "Runtime standard deviation: 11266.6 seconds (3.1 hours)\n",
      "Min runtime: 130763.8 seconds (36.3 hours)\n",
      "Max runtime: 168661.2 seconds (46.9 hours)\n",
      "Runtime ratio (max/min): 1.29\n",
      "Load balance efficiency: 0.923 (1.0 = perfect balance)\n",
      "\n",
      "Testing 7 highmem VMs for L-size benchmarks:\n",
      "Using 15 benchmarks with real runtime data (filtered from 15 total)\n",
      "Target runtime per VM: 125060.7 seconds (34.7 hours)\n",
      "Max runtime per VM (hard cap): 172800.0 seconds (48.0 hours)\n",
      "Max VMs allowed: 40\n",
      "\n",
      "=== L-size Balanced Partition (7 VMs) ===\n",
      "Total VMs created: 7\n",
      "Active VMs (with benchmarks): 6\n",
      "Empty VMs: 1\n",
      "Total runtime: 875425.0 seconds (243.2 hours)\n",
      "Average runtime per active VM: 145904.2 seconds (40.5 hours)\n",
      "Runtime standard deviation: 11266.6 seconds (3.1 hours)\n",
      "Min runtime: 130763.8 seconds (36.3 hours)\n",
      "Max runtime: 168661.2 seconds (46.9 hours)\n",
      "Runtime ratio (max/min): 1.29\n",
      "Load balance efficiency: 0.923 (1.0 = perfect balance)\n",
      "\n",
      "Testing 8 highmem VMs for L-size benchmarks:\n",
      "Using 15 benchmarks with real runtime data (filtered from 15 total)\n",
      "Target runtime per VM: 109428.1 seconds (30.4 hours)\n",
      "Max runtime per VM (hard cap): 172800.0 seconds (48.0 hours)\n",
      "Max VMs allowed: 40\n",
      "\n",
      "=== L-size Balanced Partition (8 VMs) ===\n",
      "Total VMs created: 8\n",
      "Active VMs (with benchmarks): 6\n",
      "Empty VMs: 2\n",
      "Total runtime: 875425.0 seconds (243.2 hours)\n",
      "Average runtime per active VM: 145904.2 seconds (40.5 hours)\n",
      "Runtime standard deviation: 4371.9 seconds (1.2 hours)\n",
      "Min runtime: 143747.1 seconds (39.9 hours)\n",
      "Max runtime: 155677.9 seconds (43.2 hours)\n",
      "Runtime ratio (max/min): 1.08\n",
      "Load balance efficiency: 0.970 (1.0 = perfect balance)\n",
      "\n",
      "==================================================\n",
      "TESTING S/M-SIZE BENCHMARKS (STANDARD MACHINES)\n",
      "==================================================\n",
      "Calculated VM options for S/M-size: [2, 4, 6, 8]\n",
      "\n",
      "Testing 2 standard VMs for S/M-size benchmarks:\n",
      "Using 105 benchmarks with real runtime data (filtered from 105 total)\n",
      "Target runtime per VM: 153360.8 seconds (42.6 hours)\n",
      "Max runtime per VM (hard cap): 172800.0 seconds (48.0 hours)\n",
      "Max VMs allowed: 40\n",
      "\n",
      "=== S/M-size Balanced Partition (2 VMs) ===\n",
      "Total VMs created: 2\n",
      "Active VMs (with benchmarks): 2\n",
      "Empty VMs: 0\n",
      "Total runtime: 306721.6 seconds (85.2 hours)\n",
      "Average runtime per active VM: 153360.8 seconds (42.6 hours)\n",
      "Runtime standard deviation: 19439.2 seconds (5.4 hours)\n",
      "Min runtime: 133921.6 seconds (37.2 hours)\n",
      "Max runtime: 172800.0 seconds (48.0 hours)\n",
      "Runtime ratio (max/min): 1.29\n",
      "Load balance efficiency: 0.873 (1.0 = perfect balance)\n",
      "\n",
      "Testing 4 standard VMs for S/M-size benchmarks:\n",
      "Using 105 benchmarks with real runtime data (filtered from 105 total)\n",
      "Target runtime per VM: 76680.4 seconds (21.3 hours)\n",
      "Max runtime per VM (hard cap): 172800.0 seconds (48.0 hours)\n",
      "Max VMs allowed: 40\n",
      "\n",
      "=== S/M-size Balanced Partition (4 VMs) ===\n",
      "Total VMs created: 4\n",
      "Active VMs (with benchmarks): 3\n",
      "Empty VMs: 1\n",
      "Total runtime: 306721.6 seconds (85.2 hours)\n",
      "Average runtime per active VM: 102240.5 seconds (28.4 hours)\n",
      "Runtime standard deviation: 44140.4 seconds (12.3 hours)\n",
      "Min runtime: 39818.5 seconds (11.1 hours)\n",
      "Max runtime: 133880.3 seconds (37.2 hours)\n",
      "Runtime ratio (max/min): 3.36\n",
      "Load balance efficiency: 0.568 (1.0 = perfect balance)\n",
      "\n",
      "Testing 6 standard VMs for S/M-size benchmarks:\n",
      "Using 105 benchmarks with real runtime data (filtered from 105 total)\n",
      "Target runtime per VM: 51120.3 seconds (14.2 hours)\n",
      "Max runtime per VM (hard cap): 172800.0 seconds (48.0 hours)\n",
      "Max VMs allowed: 40\n",
      "\n",
      "=== S/M-size Balanced Partition (6 VMs) ===\n",
      "Total VMs created: 6\n",
      "Active VMs (with benchmarks): 4\n",
      "Empty VMs: 2\n",
      "Total runtime: 306721.6 seconds (85.2 hours)\n",
      "Average runtime per active VM: 76680.4 seconds (21.3 hours)\n",
      "Runtime standard deviation: 17406.1 seconds (4.8 hours)\n",
      "Min runtime: 46533.0 seconds (12.9 hours)\n",
      "Max runtime: 86839.6 seconds (24.1 hours)\n",
      "Runtime ratio (max/min): 1.87\n",
      "Load balance efficiency: 0.773 (1.0 = perfect balance)\n",
      "\n",
      "Testing 8 standard VMs for S/M-size benchmarks:\n",
      "Using 105 benchmarks with real runtime data (filtered from 105 total)\n",
      "Target runtime per VM: 38340.2 seconds (10.7 hours)\n",
      "Max runtime per VM (hard cap): 172800.0 seconds (48.0 hours)\n",
      "Max VMs allowed: 40\n",
      "\n",
      "=== S/M-size Balanced Partition (8 VMs) ===\n",
      "Total VMs created: 8\n",
      "Active VMs (with benchmarks): 5\n",
      "Empty VMs: 3\n",
      "Total runtime: 306721.6 seconds (85.2 hours)\n",
      "Average runtime per active VM: 61344.3 seconds (17.0 hours)\n",
      "Runtime standard deviation: 7424.2 seconds (2.1 hours)\n",
      "Min runtime: 46533.0 seconds (12.9 hours)\n",
      "Max runtime: 65457.6 seconds (18.2 hours)\n",
      "Runtime ratio (max/min): 1.41\n",
      "Load balance efficiency: 0.879 (1.0 = perfect balance)\n"
     ]
    }
   ],
   "source": [
    "# Use all benchmark instances with their exact runtime values\n",
    "benchmarks_with_runtime = [\n",
    "    b for b in all_benchmark_instances if b[\"runtime\"] is not None\n",
    "]\n",
    "print(f\"Using {len(benchmarks_with_runtime)} benchmark-solver instances with runtime data\")\n",
    "print(\n",
    "    f\"Total combined runtime: {sum(b['runtime'] for b in benchmarks_with_runtime) / 3600:.1f} hours\"\n",
    ")\n",
    "\n",
    "if MAX_RUNTIME_PER_VM_SECONDS is not None:\n",
    "    print(\n",
    "        f\"\\n⚙️  Runtime cap enabled: {MAX_RUNTIME_PER_VM_SECONDS} seconds ({MAX_RUNTIME_PER_VM_SECONDS / 3600:.1f} hours) per VM\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n⚙️  No runtime cap configured (unlimited)\")\n",
    "\n",
    "if MAX_NUM_VMS is not None:\n",
    "    print(f\"⚙️  VM limit enabled: max {MAX_NUM_VMS} VMs\")\n",
    "else:\n",
    "    print(\"⚙️  No VM limit configured (unlimited)\")\n",
    "\n",
    "# Separate L-size benchmarks for highmem machines\n",
    "l_size_benchmarks = [b for b in benchmarks_with_runtime if b[\"size_category\"] == \"L\"]\n",
    "non_l_benchmarks = [b for b in benchmarks_with_runtime if b[\"size_category\"] != \"L\"]\n",
    "\n",
    "print(\"\\nBenchmark separation by size category:\")\n",
    "print(\n",
    "    f\"  L-size (highmem): {len(l_size_benchmarks)} instances, {sum(b['runtime'] for b in l_size_benchmarks) / 3600:.1f} hours\"\n",
    ")\n",
    "print(\n",
    "    f\"  S/M-size (standard): {len(non_l_benchmarks)} instances, {sum(b['runtime'] for b in non_l_benchmarks) / 3600:.1f} hours\"\n",
    ")\n",
    "\n",
    "# Calculate VM options dynamically based on data\n",
    "results = []\n",
    "\n",
    "# L-size benchmarks (fewer VMs since they need highmem)\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(\"TESTING L-SIZE BENCHMARKS (HIGHMEM MACHINES)\")\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "if len(l_size_benchmarks) > 0:\n",
    "    l_total_runtime = sum(b['runtime'] for b in l_size_benchmarks)\n",
    "    if MAX_RUNTIME_PER_VM_SECONDS:\n",
    "        l_min_vms = int(np.ceil(l_total_runtime / MAX_RUNTIME_PER_VM_SECONDS))\n",
    "    else:\n",
    "        l_min_vms = 1\n",
    "    # Test from min_vms up to min_vms+4 (in steps of 1)\n",
    "    l_vm_options = list(range(max(1, l_min_vms - 2), l_min_vms + 3))\n",
    "    print(f\"Calculated VM options for L-size: {l_vm_options}\")\n",
    "else:\n",
    "    l_vm_options = []\n",
    "\n",
    "for num_vms in l_vm_options:\n",
    "    if len(l_size_benchmarks) == 0:\n",
    "        print(\"No L-size benchmarks with runtime data\")\n",
    "        break\n",
    "\n",
    "    print(f\"\\nTesting {num_vms} highmem VMs for L-size benchmarks:\")\n",
    "\n",
    "    try:\n",
    "        bp_vms = balanced_partition(\n",
    "            l_size_benchmarks, num_vms, MAX_RUNTIME_PER_VM_SECONDS, MAX_NUM_VMS\n",
    "        )\n",
    "        bp_result = analyze_allocation(bp_vms, f\"L-size Balanced Partition ({num_vms} VMs)\")\n",
    "        bp_result[\"num_vms\"] = num_vms\n",
    "        bp_result[\"size_category\"] = \"L\"\n",
    "        results.append(bp_result)\n",
    "    except ValueError as e:\n",
    "        print(f\"❌ Configuration infeasible:\\n{e}\")\n",
    "        continue\n",
    "\n",
    "# S/M-size benchmarks (more VMs with standard machines)\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(\"TESTING S/M-SIZE BENCHMARKS (STANDARD MACHINES)\")\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "if len(non_l_benchmarks) > 0:\n",
    "    sm_total_runtime = sum(b['runtime'] for b in non_l_benchmarks)\n",
    "    if MAX_RUNTIME_PER_VM_SECONDS:\n",
    "        sm_min_vms = int(np.ceil(sm_total_runtime / MAX_RUNTIME_PER_VM_SECONDS))\n",
    "    else:\n",
    "        sm_min_vms = 1\n",
    "    # Test from min_vms up to min_vms+6 (in steps of 2)\n",
    "    sm_vm_options = list(range(max(1, sm_min_vms), sm_min_vms + 7, 2))\n",
    "    print(f\"Calculated VM options for S/M-size: {sm_vm_options}\")\n",
    "else:\n",
    "    sm_vm_options = []\n",
    "\n",
    "for num_vms in sm_vm_options:\n",
    "    if len(non_l_benchmarks) == 0:\n",
    "        print(\"No S/M-size benchmarks with runtime data\")\n",
    "        break\n",
    "\n",
    "    print(f\"\\nTesting {num_vms} standard VMs for S/M-size benchmarks:\")\n",
    "\n",
    "    try:\n",
    "        bp_vms = balanced_partition(\n",
    "            non_l_benchmarks, num_vms, MAX_RUNTIME_PER_VM_SECONDS, MAX_NUM_VMS\n",
    "        )\n",
    "        bp_result = analyze_allocation(\n",
    "            bp_vms, f\"S/M-size Balanced Partition ({num_vms} VMs)\"\n",
    "        )\n",
    "        bp_result[\"num_vms\"] = num_vms\n",
    "        bp_result[\"size_category\"] = \"S/M\"\n",
    "        results.append(bp_result)\n",
    "    except ValueError as e:\n",
    "        print(f\"❌ Configuration infeasible:\\n{e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Allocation Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BALANCED PARTITION ALGORITHM RESULTS\n",
      "================================================================================\n",
      "\n",
      "Size   VM Count  Efficiency   Max Runtime (h) Std Dev (h) \n",
      "------------------------------------------------------------\n",
      "L      4         0.923         46.9             3.1\n",
      "L      5         0.923         46.9             3.1\n",
      "L      6         0.923         46.9             3.1\n",
      "L      7         0.923         46.9             3.1\n",
      "L      8         0.970         43.2             1.2\n",
      "S/M    2         0.873         48.0             5.4\n",
      "S/M    4         0.568         37.2             12.3\n",
      "S/M    6         0.773         24.1             4.8\n",
      "S/M    8         0.879         18.2             2.1\n",
      "\n",
      "================================================================================\n",
      "BEST CONFIGURATIONS:\n",
      "================================================================================\n",
      "Best L-size (highmem): 8 VMs (efficiency: 0.970)\n",
      "Best S/M-size (standard): 8 VMs (efficiency: 0.879)\n",
      "\n",
      "Total deployment: 16 VMs (8 highmem + 8 standard)\n",
      "Average efficiency: 0.925\n"
     ]
    }
   ],
   "source": [
    "# Print summary comparison table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BALANCED PARTITION ALGORITHM RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Separate results by size category\n",
    "l_results = (\n",
    "    df_results[df_results[\"size_category\"] == \"L\"]\n",
    "    if \"size_category\" in df_results.columns and len(df_results) > 0\n",
    "    else pd.DataFrame()\n",
    ")\n",
    "sm_results = (\n",
    "    df_results[df_results[\"size_category\"] == \"S/M\"]\n",
    "    if \"size_category\" in df_results.columns and len(df_results) > 0\n",
    "    else pd.DataFrame()\n",
    ")\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    print(\n",
    "        f\"\\n{'Size':<6} {'VM Count':<9} {'Efficiency':<12} {'Max Runtime (h)':<15} {'Std Dev (h)':<12}\"\n",
    "    )\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for _, row in df_results.iterrows():\n",
    "        size_cat = row.get(\"size_category\", \"Mixed\")\n",
    "        print(\n",
    "            f\"{size_cat:<6} {int(row['num_vms']):<9} \"\n",
    "            f\"{row['efficiency']:.3f}{'':8} {row['max_runtime'] / 3600:.1f}{'':12} \"\n",
    "            f\"{row['std_runtime'] / 3600:.1f}\"\n",
    "        )\n",
    "\n",
    "# Find best configurations for each size category\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"BEST CONFIGURATIONS:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "if len(l_results) > 0:\n",
    "    best_l = l_results.loc[l_results[\"efficiency\"].idxmax()]\n",
    "    print(\n",
    "        f\"Best L-size (highmem): {int(best_l['num_vms'])} VMs (efficiency: {best_l['efficiency']:.3f})\"\n",
    "    )\n",
    "else:\n",
    "    best_l = None\n",
    "\n",
    "if len(sm_results) > 0:\n",
    "    best_sm = sm_results.loc[sm_results[\"efficiency\"].idxmax()]\n",
    "    print(\n",
    "        f\"Best S/M-size (standard): {int(best_sm['num_vms'])} VMs (efficiency: {best_sm['efficiency']:.3f})\"\n",
    "    )\n",
    "else:\n",
    "    best_sm = None\n",
    "\n",
    "# Calculate total deployment\n",
    "if best_l is not None and best_sm is not None:\n",
    "    total_vms = int(best_l[\"num_vms\"]) + int(best_sm[\"num_vms\"])\n",
    "    total_efficiency = (best_l[\"efficiency\"] + best_sm[\"efficiency\"]) / 2\n",
    "    print(\n",
    "        f\"\\nTotal deployment: {total_vms} VMs ({int(best_l['num_vms'])} highmem + {int(best_sm['num_vms'])} standard)\"\n",
    "    )\n",
    "    print(f\"Average efficiency: {total_efficiency:.3f}\")\n",
    "elif best_l is not None:\n",
    "    print(f\"\\nTotal deployment: {int(best_l['num_vms'])} highmem VMs only\")\n",
    "    print(f\"Efficiency: {best_l['efficiency']:.3f}\")\n",
    "elif best_sm is not None:\n",
    "    print(f\"\\nTotal deployment: {int(best_sm['num_vms'])} standard VMs only\")\n",
    "    print(f\"Efficiency: {best_sm['efficiency']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generating optimal allocations with size-based machine separation...\n",
      "Runtime cap: 172800s (48.0h) per VM\n",
      "VM limit: max 40 VMs total\n",
      "\n",
      "L-size benchmarks: 8 highmem VMs\n",
      "Efficiency: 0.970\n",
      "Max VM runtime: 43.2 hours\n",
      "Using 15 benchmarks with real runtime data (filtered from 15 total)\n",
      "Target runtime per VM: 109428.1 seconds (30.4 hours)\n",
      "Max runtime per VM (hard cap): 172800.0 seconds (48.0 hours)\n",
      "Max VMs allowed: 40\n",
      "\n",
      "=== Final L-size Allocation - Highmem ===\n",
      "Total VMs created: 8\n",
      "Active VMs (with benchmarks): 6\n",
      "Empty VMs: 2\n",
      "Total runtime: 875425.0 seconds (243.2 hours)\n",
      "Average runtime per active VM: 145904.2 seconds (40.5 hours)\n",
      "Runtime standard deviation: 4371.9 seconds (1.2 hours)\n",
      "Min runtime: 143747.1 seconds (39.9 hours)\n",
      "Max runtime: 155677.9 seconds (43.2 hours)\n",
      "Runtime ratio (max/min): 1.08\n",
      "Load balance efficiency: 0.970 (1.0 = perfect balance)\n",
      "\n",
      "S/M-size benchmarks: 8 standard VMs\n",
      "Efficiency: 0.879\n",
      "Max VM runtime: 18.2 hours\n",
      "Using 105 benchmarks with real runtime data (filtered from 105 total)\n",
      "Target runtime per VM: 38340.2 seconds (10.7 hours)\n",
      "Max runtime per VM (hard cap): 172800.0 seconds (48.0 hours)\n",
      "Max VMs allowed: 40\n",
      "\n",
      "=== Final S/M-size Allocation - Standard ===\n",
      "Total VMs created: 8\n",
      "Active VMs (with benchmarks): 5\n",
      "Empty VMs: 3\n",
      "Total runtime: 306721.6 seconds (85.2 hours)\n",
      "Average runtime per active VM: 61344.3 seconds (17.0 hours)\n",
      "Runtime standard deviation: 7424.2 seconds (2.1 hours)\n",
      "Min runtime: 46533.0 seconds (12.9 hours)\n",
      "Max runtime: 65457.6 seconds (18.2 hours)\n",
      "Runtime ratio (max/min): 1.41\n",
      "Load balance efficiency: 0.879 (1.0 = perfect balance)\n",
      "\n",
      "============================================================\n",
      "FINAL ALLOCATION SUMMARY\n",
      "============================================================\n",
      "Total VMs: 16\n",
      "  - Highmem VMs (L-size): 8\n",
      "  - Standard VMs (S/M-size): 8\n",
      "Total allocated runtime: 328.4 hours\n",
      "Machine separation ensures optimal resource utilization\n"
     ]
    }
   ],
   "source": [
    "# Generate optimal allocations for both size categories\n",
    "print(\"\\n\\nGenerating optimal allocations with size-based machine separation...\")\n",
    "\n",
    "if MAX_RUNTIME_PER_VM_SECONDS is not None:\n",
    "    print(\n",
    "        f\"Runtime cap: {MAX_RUNTIME_PER_VM_SECONDS}s ({MAX_RUNTIME_PER_VM_SECONDS / 3600:.1f}h) per VM\"\n",
    "    )\n",
    "\n",
    "if MAX_NUM_VMS is not None:\n",
    "    print(f\"VM limit: max {MAX_NUM_VMS} VMs total\")\n",
    "\n",
    "optimal_l_vms = []\n",
    "optimal_sm_vms = []\n",
    "\n",
    "# Generate L-size allocation (highmem machines)\n",
    "if best_l is not None:\n",
    "    optimal_l_num_vms = int(best_l[\"num_vms\"])\n",
    "\n",
    "    print(f\"\\nL-size benchmarks: {optimal_l_num_vms} highmem VMs\")\n",
    "    print(f\"Efficiency: {best_l['efficiency']:.3f}\")\n",
    "    print(f\"Max VM runtime: {best_l['max_runtime'] / 3600:.1f} hours\")\n",
    "\n",
    "    try:\n",
    "        optimal_l_vms = balanced_partition(\n",
    "            l_size_benchmarks, optimal_l_num_vms, MAX_RUNTIME_PER_VM_SECONDS, MAX_NUM_VMS\n",
    "        )\n",
    "        l_final_result = analyze_allocation(\n",
    "            optimal_l_vms, \"Final L-size Allocation - Highmem\"\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n❌ FATAL: L-size allocation failed:\\n{e}\")\n",
    "        raise\n",
    "\n",
    "# Generate S/M-size allocation (standard machines)\n",
    "if best_sm is not None:\n",
    "    optimal_sm_num_vms = int(best_sm[\"num_vms\"])\n",
    "\n",
    "    print(f\"\\nS/M-size benchmarks: {optimal_sm_num_vms} standard VMs\")\n",
    "    print(f\"Efficiency: {best_sm['efficiency']:.3f}\")\n",
    "    print(f\"Max VM runtime: {best_sm['max_runtime'] / 3600:.1f} hours\")\n",
    "\n",
    "    try:\n",
    "        optimal_sm_vms = balanced_partition(\n",
    "            non_l_benchmarks, optimal_sm_num_vms, MAX_RUNTIME_PER_VM_SECONDS, MAX_NUM_VMS\n",
    "        )\n",
    "        sm_final_result = analyze_allocation(\n",
    "            optimal_sm_vms, \"Final S/M-size Allocation - Standard\"\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n❌ FATAL: S/M-size allocation failed:\\n{e}\")\n",
    "        raise\n",
    "\n",
    "# Combined summary\n",
    "total_vms = len(optimal_l_vms) + len(optimal_sm_vms)\n",
    "total_runtime = sum(vm.total_runtime for vm in optimal_l_vms + optimal_sm_vms)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"FINAL ALLOCATION SUMMARY\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"Total VMs: {total_vms}\")\n",
    "print(f\"  - Highmem VMs (L-size): {len(optimal_l_vms)}\")\n",
    "print(f\"  - Standard VMs (S/M-size): {len(optimal_sm_vms)}\")\n",
    "print(f\"Total allocated runtime: {total_runtime / 3600:.1f} hours\")\n",
    "print(\"Machine separation ensures optimal resource utilization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Export Benchmark Campaign Configuration\n",
    "Writes opentofu variables file and Benchmark config YAMLs to the campaign's output directory.\n",
    "These are used by `infrastructure/main.tf` to provision VMs that launch the benchmark runner using `infrastructure/startup-script.sh`.\n",
    "The startup script runs on the VMs on boot and launches the runner on them.\n",
    "\n",
    "For more details refer to\n",
    "- `infrastructure/README.md`\n",
    "- `runner/README.md`\n",
    "\n",
    "Use `run-and-observe.ipynb` for more observing an on-going benchmark run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "now_utc = datetime.now(timezone.utc)\n",
    "formatted_time = now_utc.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# run_id is the identifier for a benchmark campaign\n",
    "run_id = f\"{formatted_time}_batch\"\n",
    "\n",
    "# will contain all the config for a run\n",
    "output_dir = Path(\"../infrastructure/benchmarks/\") / run_id\n",
    "output_dir.mkdir(exist_ok=False, parents=True)\n",
    "\n",
    "project_id = \"compute-app-427709\"\n",
    "zone       = \"europe-west4-a\" # This will be overriden if a value is specified in the input metadata file\n",
    "enable_gcs_upload = True\n",
    "auto_destroy_vm   = False\n",
    "benchmarks_dir = output_dir.resolve()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_run_tfvars(output_path: str, **vars):\n",
    "    import textwrap\n",
    "    \"\"\"Generate a config file from a simple inline template.\"\"\"\n",
    "\n",
    "    template = textwrap.dedent(\"\"\"\\\n",
    "        project_id = \"{project_id}\"\n",
    "        # This will be overriden if a value is specified in the input metadata file\n",
    "        zone = \"{zone}\"\n",
    "        # Optional\n",
    "        enable_gcs_upload = {enable_gcs_upload}\n",
    "        auto_destroy_vm = {auto_destroy_vm}\n",
    "        run_id = \"{run_id}\"\n",
    "        benchmarks_dir = \"{benchmarks_dir}\"\n",
    "        \"\"\")\n",
    "\n",
    "    # Convert booleans to lowercase true/false strings\n",
    "    for key, value in vars.items():\n",
    "        if isinstance(value, bool):\n",
    "            vars[key] = str(value).lower()\n",
    "\n",
    "    rendered = template.format(**vars)\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(rendered)\n",
    "\n",
    "    print(f\"Config written to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config written to ../infrastructure/benchmarks/20251107_080237_batch/run.tfvars\n",
      "Solvers available for year 2025: highs scip\n",
      "\n",
      "Exporting 6 highmem VMs and 5 standard VMs (skipping empty VMs)\n",
      "\n",
      "Exported highmem-vm-00.yaml: 2 L-size benchmark-size units, 40.0h runtime\n",
      "Exported highmem-vm-01.yaml: 2 L-size benchmark-size units, 40.0h runtime\n",
      "Exported highmem-vm-02.yaml: 2 L-size benchmark-size units, 40.0h runtime\n",
      "Exported highmem-vm-03.yaml: 2 L-size benchmark-size units, 40.0h runtime\n",
      "Exported highmem-vm-04.yaml: 3 L-size benchmark-size units, 43.2h runtime\n",
      "Exported highmem-vm-05.yaml: 4 L-size benchmark-size units, 39.9h runtime\n",
      "Exported standard-00.yaml: 10 S/M-size benchmark-size units, 18.2h runtime\n",
      "Exported standard-01.yaml: 10 S/M-size benchmark-size units, 18.1h runtime\n",
      "Exported standard-02.yaml: 10 S/M-size benchmark-size units, 18.2h runtime\n",
      "Exported standard-03.yaml: 11 S/M-size benchmark-size units, 17.8h runtime\n",
      "Exported standard-04.yaml: 64 S/M-size benchmark-size units, 12.9h runtime\n",
      "\n",
      "======================================================================\n",
      "Configuration files written to ../infrastructure/benchmarks/20251107_080237_batch/\n",
      "Total VMs exported: 11\n",
      "  - Highmem VMs: 6\n",
      "  - Standard VMs: 5\n",
      "Total benchmark-size combinations exported: 120\n",
      "Total runtime allocated: 328.4 hours\n",
      "\n",
      "MACHINE SEPARATION POLICY:\n",
      "  - L-size benchmarks → c4-highmem-8 (high memory for large problems)\n",
      "  - S/M-size benchmarks → c4-standard-2 (cost-effective for smaller problems)\n",
      "\n",
      "RUNTIME CAP: 48.0h (2.0 days) per VM\n",
      "\n",
      "ALLOCATION STRATEGY:\n",
      "  - Each benchmark-size combination runs all solvers together\n",
      "  - Runtime per unit = sum of individual solver runtimes\n",
      "  - For failed/timed-out solvers: uses Timeout value as runtime estimate\n",
      "  - _solver_status indicates 'ok' vs 'ER'/'TO'/'OOM' etc for each solver\n",
      "NOTE: Using exact runtime values from benchmark_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the allocation to YAML files for infrastructure\n",
    "generate_run_tfvars(\n",
    "    output_dir / \"run.tfvars\",\n",
    "    project_id=project_id,\n",
    "    zone=zone,\n",
    "    enable_gcs_upload=enable_gcs_upload,\n",
    "    auto_destroy_vm=auto_destroy_vm,\n",
    "    run_id=run_id,\n",
    "    benchmarks_dir=benchmarks_dir)\n",
    "\n",
    "# Get unique solvers from filtered benchmark data\n",
    "available_solvers = sorted(benchmark_data['Solver'].unique().tolist())\n",
    "solvers_str = \" \".join(available_solvers)\n",
    "print(f\"Solvers available for year {YEAR_FILTER}: {solvers_str}\\n\")\n",
    "\n",
    "# Filter to only VMs with benchmarks\n",
    "active_l_vms = [vm for vm in optimal_l_vms if vm.benchmarks]\n",
    "active_sm_vms = [vm for vm in optimal_sm_vms if vm.benchmarks]\n",
    "\n",
    "print(\n",
    "    f\"Exporting {len(active_l_vms)} highmem VMs and {len(active_sm_vms)} standard VMs (skipping empty VMs)\\n\"\n",
    ")\n",
    "\n",
    "# Export L-size VMs (highmem machines)\n",
    "for vm_idx, vm in enumerate(active_l_vms):\n",
    "    machine_type = \"c4-highmem-8\"\n",
    "    years = [YEAR_FILTER]\n",
    "\n",
    "    # Sort benchmarks by runtime (SMALLEST FIRST) so they run in order\n",
    "    sorted_benchmarks = sorted(vm.benchmarks, key=lambda b: b[\"runtime\"])\n",
    "\n",
    "    # Create benchmark structure with runtime metadata\n",
    "    benchmarks_dict = {}\n",
    "    benchmark_runtimes = {}\n",
    "\n",
    "    for benchmark in sorted_benchmarks:\n",
    "        benchmark_name = benchmark[\"name\"]\n",
    "        if benchmark_name not in benchmarks_dict:\n",
    "            benchmarks_dict[benchmark_name] = {\"Sizes\": []}\n",
    "            benchmark_runtimes[benchmark_name] = 0.0\n",
    "\n",
    "        # Include all solvers and their individual runtimes\n",
    "        size_entry = {\n",
    "            \"Name\": benchmark[\"size_name\"],\n",
    "            \"Size\": benchmark[\"size_category\"],\n",
    "            \"_solvers\": sorted(benchmark[\"solvers\"]),  # List of all solvers (metadata only)\n",
    "            \"URL\": benchmark[\"url\"],\n",
    "            \"_runtime_s\": round(benchmark[\"runtime\"], 2),  # Total runtime (sum of all solvers)\n",
    "            \"_solver_runtimes_s\": {solver: round(benchmark[\"solver_runtimes\"][solver], 2)\n",
    "                                   for solver in sorted(benchmark[\"solvers\"])},\n",
    "            \"_solver_status\": {solver: benchmark[\"solver_status\"][solver]\n",
    "                               for solver in sorted(benchmark[\"solvers\"])},\n",
    "        }\n",
    "        benchmarks_dict[benchmark_name][\"Sizes\"].append(size_entry)\n",
    "        benchmark_runtimes[benchmark_name] += benchmark[\"runtime\"]\n",
    "\n",
    "    # Add total runtime for each benchmark\n",
    "    for benchmark_name in benchmarks_dict:\n",
    "        benchmarks_dict[benchmark_name][\"_runtime_s\"] = round(\n",
    "            benchmark_runtimes[benchmark_name], 2\n",
    "        )\n",
    "\n",
    "    # Create YAML content with total runtime metadata\n",
    "    yaml_content = {\n",
    "        \"machine-type\": machine_type,\n",
    "        \"years\": years,\n",
    "        \"solver\": solvers_str,  # Space-separated list of solvers used for this year\n",
    "        \"_total_runtime_s\": round(vm.total_runtime, 2),\n",
    "        \"_total_runtime_h\": round(vm.total_runtime / 3600, 2),\n",
    "        \"_max_runtime_cap_h\": MAX_RUNTIME_PER_VM_SECONDS / 3600\n",
    "        if MAX_RUNTIME_PER_VM_SECONDS\n",
    "        else None,\n",
    "        \"_num_benchmarks\": len(vm.benchmarks),\n",
    "        \"_note\": \"Each benchmark-size runs all solvers together. For failed/timed-out runs, Timeout value is used as runtime estimate.\",\n",
    "        \"benchmarks\": benchmarks_dict,\n",
    "    }\n",
    "\n",
    "    # Write to file\n",
    "    filename = f\"highmem-vm-{vm_idx:02d}.yaml\"\n",
    "    with open(output_dir / filename, \"w\") as f:\n",
    "        yaml.safe_dump(yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "    print(\n",
    "        f\"Exported {filename}: {len(vm.benchmarks)} L-size benchmark-size units, \"\n",
    "        f\"{vm.total_runtime / 3600:.1f}h runtime\"\n",
    "    )\n",
    "\n",
    "# Export S/M-size VMs (standard machines)\n",
    "for vm_idx, vm in enumerate(active_sm_vms):\n",
    "    machine_type = \"c4-standard-2\"\n",
    "    years = [YEAR_FILTER]\n",
    "\n",
    "    # Sort benchmarks by runtime (SMALLEST FIRST) so they run in order\n",
    "    sorted_benchmarks = sorted(vm.benchmarks, key=lambda b: b[\"runtime\"])\n",
    "\n",
    "    # Create benchmark structure with runtime metadata\n",
    "    benchmarks_dict = {}\n",
    "    benchmark_runtimes = {}\n",
    "\n",
    "    for benchmark in sorted_benchmarks:\n",
    "        benchmark_name = benchmark[\"name\"]\n",
    "        if benchmark_name not in benchmarks_dict:\n",
    "            benchmarks_dict[benchmark_name] = {\"Sizes\": []}\n",
    "            benchmark_runtimes[benchmark_name] = 0.0\n",
    "\n",
    "        # Include all solvers and their individual runtimes\n",
    "        size_entry = {\n",
    "            \"Name\": benchmark[\"size_name\"],\n",
    "            \"Size\": benchmark[\"size_category\"],\n",
    "            \"_solvers\": sorted(benchmark[\"solvers\"]),  # List of all solvers (metadata only)\n",
    "            \"URL\": benchmark[\"url\"],\n",
    "            \"_runtime_s\": round(benchmark[\"runtime\"], 2),  # Total runtime (sum of all solvers)\n",
    "            \"_solver_runtimes_s\": {solver: round(benchmark[\"solver_runtimes\"][solver], 2)\n",
    "                                   for solver in sorted(benchmark[\"solvers\"])},\n",
    "            \"_solver_status\": {solver: benchmark[\"solver_status\"][solver]\n",
    "                               for solver in sorted(benchmark[\"solvers\"])},\n",
    "        }\n",
    "        benchmarks_dict[benchmark_name][\"Sizes\"].append(size_entry)\n",
    "        benchmark_runtimes[benchmark_name] += benchmark[\"runtime\"]\n",
    "\n",
    "    # Add total runtime for each benchmark\n",
    "    for benchmark_name in benchmarks_dict:\n",
    "        benchmarks_dict[benchmark_name][\"_runtime_s\"] = round(\n",
    "            benchmark_runtimes[benchmark_name], 2\n",
    "        )\n",
    "\n",
    "    # Create YAML content with total runtime metadata\n",
    "    yaml_content = {\n",
    "        \"machine-type\": machine_type,\n",
    "        \"years\": years,\n",
    "        \"solver\": solvers_str,  # Space-separated list of solvers used for this year\n",
    "        \"_total_runtime_s\": round(vm.total_runtime, 2),\n",
    "        \"_total_runtime_h\": round(vm.total_runtime / 3600, 2),\n",
    "        \"_max_runtime_cap_h\": MAX_RUNTIME_PER_VM_SECONDS / 3600\n",
    "        if MAX_RUNTIME_PER_VM_SECONDS\n",
    "        else None,\n",
    "        \"_num_benchmarks\": len(vm.benchmarks),\n",
    "        \"_note\": \"Each benchmark-size runs all solvers together. For failed/timed-out runs, Timeout value is used as runtime estimate.\",\n",
    "        \"benchmarks\": benchmarks_dict,\n",
    "    }\n",
    "\n",
    "    # Write to file\n",
    "    filename = f\"standard-{vm_idx:02d}.yaml\"\n",
    "    with open(output_dir / filename, \"w\") as f:\n",
    "        yaml.safe_dump(yaml_content, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "    print(\n",
    "        f\"Exported {filename}: {len(vm.benchmarks)} S/M-size benchmark-size units, \"\n",
    "        f\"{vm.total_runtime / 3600:.1f}h runtime\"\n",
    "    )\n",
    "\n",
    "total_exported_vms = len(active_l_vms) + len(active_sm_vms)\n",
    "total_benchmarks = sum(len(vm.benchmarks) for vm in active_l_vms + active_sm_vms)\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"Configuration files written to {output_dir}/\")\n",
    "print(f\"Total VMs exported: {total_exported_vms}\")\n",
    "print(f\"  - Highmem VMs: {len(active_l_vms)}\")\n",
    "print(f\"  - Standard VMs: {len(active_sm_vms)}\")\n",
    "print(f\"Total benchmark-size combinations exported: {total_benchmarks}\")\n",
    "print(\n",
    "    f\"Total runtime allocated: {sum(vm.total_runtime for vm in active_l_vms + active_sm_vms) / 3600:.1f} hours\"\n",
    ")\n",
    "print(\"\\nMACHINE SEPARATION POLICY:\")\n",
    "print(\"  - L-size benchmarks → c4-highmem-8 (high memory for large problems)\")\n",
    "print(\"  - S/M-size benchmarks → c4-standard-2 (cost-effective for smaller problems)\")\n",
    "print(\n",
    "    f\"\\nRUNTIME CAP: {MAX_RUNTIME_PER_VM_SECONDS / 3600:.1f}h ({MAX_RUNTIME_PER_VM_SECONDS / (24 * 3600):.1f} days) per VM\"\n",
    "    if MAX_RUNTIME_PER_VM_SECONDS\n",
    "    else \"\\nNo runtime cap configured\"\n",
    ")\n",
    "print(\"\\nALLOCATION STRATEGY:\")\n",
    "print(\"  - Each benchmark-size combination runs all solvers together\")\n",
    "print(\"  - Runtime per unit = sum of individual solver runtimes\")\n",
    "print(\"  - For failed/timed-out solvers: uses Timeout value as runtime estimate\")\n",
    "print(\"  - _solver_status indicates 'ok' vs 'ER'/'TO'/'OOM' etc for each solver\")\n",
    "print(\"NOTE: Using exact runtime values from benchmark_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-tests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
