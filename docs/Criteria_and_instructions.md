## Goals for the benchmark set

We encourage submission of benchmarks that help the project meet the following overall targets:

1. A set of benchmarks that are diverse in terms of modelling frameworks that generated them, problem structure, and model features. For instance, we would like models that consider innovative technologies (e.g., electrolyzers, CO2 capture) or policy-driven constraints (e.g., on CO2 emissions). By "features" we mean the different kinds of energy planning problems that can be modeleld by the framework (e.g., capacity expansion, power system operations, resource adequacy).

1. Benchmarks using model features that are implemented using MILP constraints, especially features other than unit commitment.

1. Benchmarks that help open-source solver developers improve their solvers: benchmarks that can be solved rapidly (< 5 minutes) by Gurobi but are slow (~1 hour or higher) or fail when solved by an open-source solver.

## Criteria for the selection of benchmarks

The Solver Benchmark project is open and encourages the community to submit benchmark problems. Please ensure that submissions adhere to the following criteria:

1. Benchmarks must be in the `.lp` or `.mps` file formats, that are suitable for providing to the solver directly as input (i.e., no further pre-processing must be necessary). An advantage of using these formats is that they preserve [confidentiality of the model's input data](https://www.gams.com/48/docs/S_CONVERT.html?search=confidential) as they contain only mathemetical equations and it is near impossible to reconstruct the underlying energy specification and technological data.

1. Benchmarks must be Linear Programming (LP) or Mixed Integer Linear Programming (MILP) problems. We do not currently accept other kinds of problems such as non-linear, or multi-objective problems.

1. Benchmarks must be problems generated by bottom-up energy system models (see *Target modelling frameworks* below).

1. Benchmarks must be solvable in one of the following time limits, depending on the size category:
    - Small: under 10 minutes HiGHS solving time
    - Medium: under 1 hour HiGHS solving time
    - Large / Real: under 10 hours Gurobi solving time

    where HiGHS runtimes are measured with the latest solver versions on a machine with [TBD] 2 vCPUs and 8 GB memory (e.g. an `e2-standard-2` VM on Google Cloud) and Gurobi solving time is on a [TBD -- reasonable machine?].

Whenever possible, we prefer benchmarks that can be generated in multiple "sizes" by varying the time scale (single-stage / multi-stage planning horizons), temporal resolution (hourly, daily, etc), or spatial resolution (number of regions / nodes).

## Instructions for submitting benchmarks

The prefered and recommended approach for submission is to open a pull request to this repository that adds to the `benchmarks/<framework>/` folder:
- Metadata (name, description, etc; see below) added to a YAML file `benchmarks/<framework>/metadata.yaml`, create this if it doesn't exist already
- A configuration file that is used as an input to the modelling framework
- A dockerfile that specifies the modelling framework version (preferably a commit hash), pinned versions of all dependencies, and a script to run the modelling framework and obtain the LP/MPS file given to the solver.
    - For example, see the benchmarks in the `benchmarks/pypsa/` folder.
- For non fully open-source modelling frameworks, where LP/MPS files cannot be reproduced automatically as above, we will accept LP/MPS files hosted on a public immutable file storage service such as Zenodo. In such cases, the metadata file containing a URL to download the benchmark (prefereably via a permalink) is sufficient.

### Benchmark metadata

Please include along with each benchmark submission, the following metadata. Further fields that are relevant to your modelling framework and application domain are welcome.

||||||||
| -- | -- | -- | -- | -- | -- | -- |
| **Problem name** |
| **Short description** | (no more than 140 characters) |
| **Model name** |
| **Version** |
| **Technique** | LP | MILP |
| **Kind of problem** | Infrastructure (capacity expansion) | Operational (dispatch only) | Other (please indicate) |
| **Sectors** | Sector-coupled (power + heating, industry, transport) | Power sector |
| **Time horizon** | Single-period | Multi-period (indicate n. of periods) |
| **Temporal resolution** | Hourly | 3 hourly | Daily | Yearly |
| **Spatial resolution** | Single node / 2 nodes (indicate countries/regions) | Multi-nodal (10 $\div$ 20) (indicate countries/regions) |
| **MILP features** | None | Unit commitment | Transmission expansion | Other (please indicate) |

For example, here is an entry in the `benchmarks/pypsa/metadata.yaml` file:

```yaml
pypsa-eur-sec-2-lv1-3h:
  Short description: Sector-coupled PyPSA-Eur run for Italy considering 2050 as single planning horizon (LP, lot of variables, strongly intermeshed constraints)
  Model name: PyPSA-Eur
  Version: 0.12.0 (commit b328169)
  Technique: LP
  Kind of problem: Infrastructure
  Sectors: Sector-coupled (power + heating, biomass, industry, transport)
  Time horizon: Single period (1 year)
  MILP features: None
  Sizes:
  - URL: https://todo.todo/todo.lp
    Temporal resolution: 3 hourly
    Spatial resolution: 2 nodes
```

## Target modelling frameworks

Based on the classification in *M. G. Prina et al., ["Classification and challenges of bottom-up energy system models - A review"](https://www.sciencedirect.com/science/article/pii/S1364032120302082), Renewable and Sustainable Energy Reviews, vol. 129, 2020, 109917* and the [results of the survey](https://zenodo.org/records/13354034) recently conducted by OET, the list of models targeted for the solver benchmark website considers either short- or long-term bottom-up models including but not limited to [^1]:
- PyPSA
- Switch
- oemof
- energyRt
- PLEXOS
- Dolphyin
- GenX
- IDEEA
- Pandapower
- Calliope
- Genesys
- DESSTinEE
- GAMAMOD
- Ficus
- REMod
- REMix
- TIMES
- OSeMOSYS
- TEMOA
