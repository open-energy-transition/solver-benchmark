# Open Energy Benchmark

This repository contains code for benchmarking optimization solvers on problems from the energy planning domain, and an interactive website for analyzing the results. The live website can be viewed at:

https://openenergybenchmark.org/

## Benchmark Problems

All our benchmark problems are open and available as LP/MPS files that can be downloaded in one click from our website's [Benchmark Set](https://openenergybenchmark.org/dashboard/benchmark-set) page. Some problems have been generated by us using open source energy model frameworks, and for these we have configuration files and instructions for reproducing the problems.

## Project Structure

Understanding the project layout to help you navigate and contribute:

```bash
solver-benchmark/
├── runner/                   # Benchmark execution scripts
│   ├── benchmark_all.sh      # Main entry point for running benchmarks
│   ├── run_benchmarks.py     # Python script that orchestrates benchmark runs
│   ├── run_solver.py         # Individual solver runner
│   ├── envs/                 # Conda environment definitions for each solver year
│   └── benchmarks/           # Downloaded benchmark problem files
├── benchmarks/               # Benchmark problem definitions and metadata
│   ├── pypsa/               # PyPSA-generated energy models
│   ├── jump_highs_platform/ # JuMP/HiGHS benchmark metadata
│   └── *_metadata.yaml      # Problem definitions and details
├── website-nextjs/          # Next.js website for viewing results
├── infrastructure/          # GCP VM deployment scripts (for running benchmarks at scale)
├── results/                 # Output directory for benchmark results
│   ├── benchmark_results.csv      # Main results file
│   └── metadata.yaml              # Merged metadata of all problems on the website
│
└── README.md
```

### Contributing Benchmark Problems

We welcome contributions of new benchmark problems from the community! See [this page](https://openenergybenchmark.org/key-insights#what-benchmark-problems-do-we-have-and-what-are-missing) for details about our current benchmark set, and what gaps we have that we would love your help to fill.

To contribute, if you are familiar with git and GitHub, we request that you:

1. Generate an MPS file (preferred; alternatively, LP files are also acceptable) for each optimization problem using your energy model framework. (The steps for how to do this depend on the framework, but reach out if you need help -- we are happy to support you through this.)

1. Write up the details and classification of each problem you contribute in a YAML file, using this [template](benchmarks/_template_metadata.yaml) as a guide.

1. Upload the MPS file to any file sharing service of your choice.

1. Open a pull request (PR) which adds the metadata file to a new directory `benchmarks/<model-framework-or-source>/`. We will review the contribution and work with you to add suitable problems to our platform.

Don't worry if you're not familiar with git or GitHub! Please write to us, or open an issue, and we can support you through the above steps. We thank you in advance for your contributions.

### Generating Benchmark Problems

1. The PyPSA benchmarks in `benchmarks/pypsa/` can be generated by using the Dockerfile present in that directory. Please see the [instructions](benchmarks/pypsa/README.md) for more details.

1. The JuMP-HiGHS benchmarks in `benchmarks/jump_highs_platform/` contain only the metadata for the benchmarks that are present in https://github.com/jump-dev/open-energy-modeling-benchmarks/tree/main/instances. These are fetched automatically by the benchmark runner from GitHub.

1. The metadata of all benchmarks under `benchmarks/` are collected by the following script to generate a unified `results/metadata.yaml` file, when run as follows:
   ```shell
   python benchmarks/merge_metadata.py
   ```

The unified `results/metadata.yaml` contains all details of each benchmark problem, including the download link, and is used by the benchmark runner (below).

## Running Benchmarks

### Pre-requisites

#### System Requirements

**This project requires Linux.** The benchmark runner uses `systemd-run` to enforce memory limits on solvers, which is not available on macOS or Windows.

Supported Linux distributions:
- Ubuntu 20.04 LTS or later
- Debian 11 or later
- Other systemd-based Linux distributions

#### Required Software

Ensure you have the following installed:

- **Python 3.12+**
- **Conda** (if not already present, [install Miniconda](https://docs.conda.io/projects/miniconda/latest/miniconda-install-html.html))
- **Git** for cloning the repository
- **systemd** (usually pre-installed on modern Linux distributions)

#### Running supported solvers on benchmarks config

The `benchmark_all.sh` script takes a YAML benchmark config file  as argument and runs all the solvers in series for each benchmark problem.

> see `Generating Benchmark Problems` section above to understand how to generate the benchmark problems and the schema of these configs. *TODO* describe the schema - its required fields

The benchmark runner script creates conda environments containing the solvers and other necessary pre-requisites, so a virtual environment is not necessary just for running the benchmark runner.
It can only run existing solvers tabulated in the next section. To add a new solver you must *TODO*

```shell
./runner/benchmark_all.sh ./results/metadata.yaml
```

The script will save the measured runtime and memory consumption into a CSV file in `results/` that the website will then read and display.
The script has options, e.g. to run only particular years, that you can see with the `-h` flag:
```bash
$ ./runner/benchmark_all.sh -h
```
```
Runs the solvers from the specified years (default all) on the benchmarks in the given file
Options:
    -a    Append to the results CSV file instead of overwriting. Default: overwrite
    -y    A space separated string of years to run. Default: 2020 2021 2022 2023 2024 2025
    -r    Reference benchmark interval in seconds. Default: 0 (disabled)
    -u    Unique run ID to identify this benchmark run. Default: auto-generated
    -s    Space separated list of solvers to run. Default: year-specific default
```

The `benchmark_all.sh` script activates the appropriate conda environment and then calls `python runner/run_benchmarks.py`.
This script can also be called directly, if required, but you must be in a conda environment that contains the solvers you want to benchmark.
For example:
```shell
python ./runner/run_benchmarks.py ./results/metadata.yaml 2024
```
Call `python runner/run_benchmarks.py -h` to see more options.

### Solver Versions

We support the following versions of solvers: (We use the last released solver version in each calendar year.)

| Solver | 2020 | 2021 | 2022 | 2023 | 2024 | 2025 |
| ------ | ---- | ---- | ---- | ---- | ---- | ---- |
| HiGHS | | [Not on PyPI](https://github.com/open-energy-transition/solver-benchmark/blob/aa32f81d523295d308733841428b4199eaf2f1ff/runner/envs/benchmark-2021.yaml#L16) | 1.5.0 | 1.6.0 | 1.9.0 | 1.10.0 |
| SCIP | [Error](https://github.com/open-energy-transition/solver-benchmark/blob/aa32f81d523295d308733841428b4199eaf2f1ff/runner/envs/benchmark-2020.yaml#L13) | [Error](https://github.com/open-energy-transition/solver-benchmark/blob/aa32f81d523295d308733841428b4199eaf2f1ff/runner/envs/benchmark-2021.yaml#L12) | 8.0.3 | 8.1.0 | [Error](https://github.com/open-energy-transition/solver-benchmark/blob/main/runner/envs/benchmark-2024.yaml) | 9.2.2 |
| CBC | [Bug](https://github.com/coin-or/Cbc/issues/708) | | [Bug](https://github.com/coin-or/Cbc/issues/708) | 2.10.11 | 2.10.12 | |
| GLPK | 5.0.0 |  |  |  |  |  |
| Gurobi | [Incompatible](https://github.com/open-energy-transition/solver-benchmark/blob/aa32f81d523295d308733841428b4199eaf2f1ff/runner/envs/benchmark-2020.yaml#L16) | [Incompatible](https://github.com/open-energy-transition/solver-benchmark/blob/aa32f81d523295d308733841428b4199eaf2f1ff/runner/envs/benchmark-2021.yaml#L14) | 10.0.0 | 11.0.0 | 12.0.0 | |


When determining which is the most recent version released in a particular year, we use the following resources:
- https://github.com/ERGO-Code/HiGHS/releases
- https://github.com/coin-or/Cbc/releases
- https://github.com/scipopt/scip/releases and https://pypi.org/project/PySCIPOpt/#history
- https://support.gurobi.com/hc/en-us/articles/360048138771-Gurobi-release-and-support-history

## Running the Website

### NextJS Production Website

The website code is under `website-nextjs/`. To run the website locally, you need a recent version of `node` and `npm` installed. Then, run the following commands:

```sh
cd website-nextjs/
npm install
npm run build && npm run dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the website.

### Running the Streamlit Website

## Development

We use the [ruff](https://docs.astral.sh/ruff) code linter and formatter, and GitHub Actions runs various pre-commit checks to ensure code and files are clean.

You can install a git pre-commit that will ensure that your changes are formatted
and no lint issues are detected before creating new commits:
```bash
pip install pre-commit
pre-commit install
```
If you want to skip these pre-commit steps for a particular commit, you can run:
```bash
git commit --no-verify
```
