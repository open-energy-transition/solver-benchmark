### Contributing Benchmark Problems

We welcome contributions of new benchmark problems from the community! See [this page](https://openenergybenchmark.org/key-insights#what-benchmark-problems-do-we-have-and-what-are-missing) for details about our current benchmark set, and what gaps we have that we would love your help to fill.

To contribute, if you are familiar with git and GitHub, we request that you:

1. Generate an MPS file (preferred; alternatively, LP files are also acceptable) for each optimization problem using your energy model framework. (The steps for how to do this depend on the framework, but reach out if you need help -- we are happy to support you through this.)

1. Write up the details and classification of each problem you contribute in a YAML file, using this [template](benchmarks/_template_metadata.yaml) as a guide.

1. Upload the MPS file to any file sharing service of your choice.

1. Open a pull request (PR) which adds the metadata file to a new directory `benchmarks/<model-framework-or-source>/`. We will review the contribution and work with you to add suitable problems to our platform.

Don't worry if you're not familiar with git or GitHub! Please write to us, or open an issue, and we can support you through the above steps. We thank you in advance for your contributions.

### Generating Benchmark Problems

1. The PyPSA benchmarks in `benchmarks/pypsa/` can be generated by using the Dockerfile present in that directory. Please see the [instructions](benchmarks/pypsa/README.md) for more details.

1. The JuMP-HiGHS benchmarks in `benchmarks/jump_highs_platform/` contain only the metadata for the benchmarks that are present in https://github.com/jump-dev/open-energy-modeling-benchmarks/tree/main/instances. These are fetched automatically by the benchmark runner from GitHub.

1. The metadata of all benchmarks under `benchmarks/` are collected by the following script to generate a unified `results/metadata.yaml` file, when run as follows:
   ```shell
   python benchmarks/merge_metadata.py
   ```

The unified `results/metadata.yaml` contains all details of each benchmark problem, including the download link, and is used by the benchmark runner (below).
